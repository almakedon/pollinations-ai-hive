{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "\n",
        "<img src=\"https://pollinations.ai/ipfs/QmUKydjhLKP26P8qT49cvsDYXjmYAShbo2Z95JxatvzwkK\" width=\"300\" height=\"300\" />\n",
        "\n",
        "Input: \"*Self-pollination in the style of Ernst Haeckel*\"\n",
        "\n",
        "Based on a notebook by [Katherine Crowson](https://twitter.com/RiversHaveWings).\n",
        "\n",
        "Modified by [jbuster](https://twitter.com/jbusted1). and [thomash](https://twitter.com/pollinations_ai). \n",
        "\n",
        "---\n",
        "\n",
        "It is possible to provide multiple text prompts with weights by providing them like this: \n",
        "**```oil painting: 0.5|salvador dali: 0.3|edward much:0.6|robot friend: 1.0|text:-0.5```** *In this case a negative weight next to `text` makes the model avoid text.*  \n",
        "\n",
        "\n",
        "[UPD 13.12.2021] Re-order library imports to fix bug that was crashing a lot of runs\n",
        "\n",
        "[UPD 10.12.2021] Image prompting and extra parameters\n",
        "\n",
        "[UPD 3.11.2021] Added Gumbel sampling method suggested by Daniel Russ on Discord\n",
        "\n",
        "[UPD 2.11.2021] Added ruDALLE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "pZmCmyKM9fmv",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Text Prompt\n",
        "text_input = 'the ghost of a geisha under a weeping willow, matte painting'  #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "# Width in pixels of image to be generated. If you put this too high the GPU may run out of memory. Rather use super-resolution to achieve high resolutions.\n",
        "width = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Height in pixels of image to be generated. (see width)\n",
        "height = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Apply a (neural) super-resolution step (2 x image width x image height)\n",
        "super_resolution = False #@param {type: \"boolean\"}\n",
        "\n",
        "# Image prompt. Leave blank if you want to use only text\n",
        "image_file = '' #@param {type: \"string\"}\n",
        "\n",
        "# Step size (how fast to try and optimize the image. Between 0 and 100)\n",
        "step_size = 30 #@param {type: \"number\"}\n",
        "\n",
        "# Iterations (how many frames to optimize the image. Determines the length of the output video)\n",
        "iterations = 600 #@param {type: \"number\"}\n",
        "\n",
        "\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "social = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EM5FfHRxm8av"
      },
      "outputs": [],
      "source": [
        "# Model to use. ruDALLE was recently finetuned by a russian company called Sber and could give better outputs.\n",
        "imagemodel = \"imagenet\" \n",
        " #@param ['imagenet', 'ruDALLE']\n",
        "\n",
        "\n",
        "# check if christmas and add christmas to prompt\n",
        "from datetime import datetime\n",
        "\n",
        "d = datetime.now()\n",
        "\n",
        "if social and (d.strftime(\"%m\") == \"12\") and  ((d.strftime(\"%d\") == \"24\") or  (d.strftime(\"%d\") == \"25\") or  (d.strftime(\"%d\") == \"26\")):\n",
        "    text_input = text_input + \"|christmas:3|text:-0.5\"\n",
        "\n",
        "image_prompt = image_file\n",
        "\n",
        "if len(image_prompt) == 0:\n",
        "    image_prompt = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "VIf9Z1CTsC7R",
        "outputId": "b8fb6343-8fbb-470f-c424-950975c1c4cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "#@title Upscale images/video frames\n",
        "\n",
        "black_and_white = False\n",
        "\n",
        "loaded_upscale_model = False\n",
        "\n",
        "def upscale(filepath):\n",
        "  if not super_resolution:\n",
        "    return\n",
        "  global loaded_upscale_model\n",
        "  if not loaded_upscale_model:\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd /content/Real-ESRGAN\n",
        "    !git checkout 3338b31f486586bd7f6b20cc2a9fadd5ed192a00\n",
        "    # Set up the environment\n",
        "    !pip install git+https://github.com/xinntao/BasicSR.git\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    %cd -\n",
        "    loaded_upscale_model = True \n",
        "  \n",
        "  %cd /content/Real-ESRGAN\n",
        "  !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth --input $filepath --netscale 4 --outscale 2 --half --output $output_path\n",
        "  filepath_out = filepath.replace(\".jpg\",\"_out.jpg\")\n",
        "  !mv -v $filepath_out $filepath\n",
        "  %cd -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cf54A275YLpt",
        "outputId": "18253346-ea2e-4354-a1b1-d07d7e1e5e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 10 12:55:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wSfISAhyPmyp",
        "outputId": "eacc2f5a-8455-48f9-c771-7ab90e0ca76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 195 (delta 12), reused 22 (delta 9), pack-reused 168\u001b[K\n",
            "Receiving objects: 100% (195/195), 8.91 MiB | 24.71 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1335, done.\u001b[K\n",
            "remote: Counting objects: 100% (525/525), done.\u001b[K\n",
            "remote: Compressing objects: 100% (493/493), done.\u001b[K\n",
            "remote: Total 1335 (delta 58), reused 479 (delta 30), pack-reused 810\u001b[K\n",
            "Receiving objects: 100% (1335/1335), 412.35 MiB | 35.33 MiB/s, done.\n",
            "Resolving deltas: 100% (267/267), done.\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 52.8 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 56.9 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 58.8 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 54.0 MB/s \n",
            "\u001b[?25h  Downloading torchtext-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1 torchtext-0.9.1\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 67.0 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.8 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.1-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 62.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 54.3 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.11)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, future\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=e66cbdf277547f1a5f959f75cd54b99f1cb0bdd43d68a62b43c5c4e11ab788f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=46e78a089b53f380a948c72e7b0082e42f0cbaabd3ef17f42b978d957d0da02b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built antlr4-python3-runtime future\n",
            "Installing collected packages: setuptools, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, future, antlr4-python3-runtime, pytorch-lightning, omegaconf, ftfy\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.1.0 ftfy-6.1.1 future-0.18.2 multidict-6.0.2 omegaconf-2.1.1 pyDeprecate-0.3.1 pytorch-lightning-1.5.10 setuptools-59.5.0 torchmetrics-0.7.1 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.6.3-py2.py3-none-any.whl (474 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 38.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 102 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 153 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 194 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 204 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 245 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 256 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 307 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 348 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 358 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 389 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 399 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 409 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 440 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 450 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 474 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.7)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.3\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3 freeglut3-dev\n",
            "  keyboard-configuration libargon2-0 libcap2 libcryptsetup12\n",
            "  libdevmapper1.02.1 libfontenc1 libidn11 libip4tc0 libjansson4\n",
            "  libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxnvctrl0 libxtst6\n",
            "  nsight-compute-2020.2.1 nsight-compute-2022.1.0 nsight-systems-2020.3.2\n",
            "  nsight-systems-2020.3.4 nsight-systems-2021.5.2 nvidia-dkms-510\n",
            "  nvidia-kernel-common-510 nvidia-kernel-source-510 nvidia-modprobe\n",
            "  nvidia-settings openjdk-11-jre policykit-1 policykit-1-gnome python3-xkit\n",
            "  screen-resolution-extra systemd systemd-sysv udev x11-xkb-utils\n",
            "  xserver-common xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libc-ares2\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,274 kB of archives.\n",
            "After this operation, 4,912 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-ares2 amd64 1.14.0-1ubuntu0.1 [37.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aria2 amd64 1.33.1-1 [1,236 kB]\n",
            "Fetched 1,274 kB in 0s (11.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.14.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.14.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.33.1-1_amd64.deb ...\n",
            "Unpacking aria2 (1.33.1-1) ...\n",
            "Setting up libc-ares2:amd64 (1.14.0-1ubuntu0.1) ...\n",
            "Setting up aria2 (1.33.1-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p $output_path\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "#%cd taming-transformers\n",
        "#!git checkout 2908a53b88478e5812d619b6ac003dbb29b069a0\n",
        "#%cd -\n",
        "!pip install torch==1.8.1 torchtext\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops\n",
        "!sudo apt install -y aria2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FhhdWrSxQhwg",
        "outputId": "1864c5c8-518b-4915-a0b0-f2e7d2f378e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "02/10 12:57:42 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://heibox.uni-heidelberg.de/seafhttp/files/17fa92e5-92f4-4ceb-a79f-d2f2c463109b/model.yaml\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/vqgan_imagenet_f16_16384.ckpt.yaml\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "9d8478|\u001b[1;32mOK\u001b[0m  |   112KiB/s|/content/vqgan_imagenet_f16_16384.ckpt.yaml\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://heibox.uni-heidelberg.de/seafhttp/files/a7651150-5169-4d3b-9dbb-27c18c2f1a33/last.ckpt\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://heibox.uni-heidelberg.de/seafhttp/files/ee8c4916-b930-41c8-9d48-156189a673e4/last.ckpt\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://heibox.uni-heidelberg.de/seafhttp/files/e3a40899-a9f4-4059-9590-adc8c92c5d6a/last.ckpt\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://heibox.uni-heidelberg.de/seafhttp/files/57f43d60-334d-4316-9aef-06961e5454ce/last.ckpt\n",
            "\n",
            "02/10 12:57:43 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://heibox.uni-heidelberg.de/seafhttp/files/1b70ee1b-7bb9-4a45-9421-ed7c7e9f51aa/last.ckpt\n",
            " *** Download Progress Summary as of Thu Feb 10 12:58:43 2022 *** \n",
            "=\n",
            "[#2df201 906MiB/0.9GiB(96%) CN:3 DL:14MiB ETA:1s]\n",
            "FILE: /content/vqgan_imagenet_f16_16384.ckpt\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "02/10 12:58:45 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/vqgan_imagenet_f16_16384.ckpt\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "2df201|\u001b[1;32mOK\u001b[0m  |    15MiB/s|/content/vqgan_imagenet_f16_16384.ckpt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "model_mapping = {\n",
        "    \"ruDALLE\": { \n",
        "        \"name\": \"vqgan_openimages_f16_8192.ckpt\",\n",
        "        \"config\": \"https://pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt.yaml\",\n",
        "        \"checkpoint\": \"https://pollinations.ai/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt\",\n",
        "    },\n",
        "    \"imagenet\": {\n",
        "        \"name\": \"vqgan_imagenet_f16_16384.ckpt\",\n",
        "        \"config\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n",
        "        \"checkpoint\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1',        \n",
        "    } \n",
        "}\n",
        "\n",
        "selected_vqgan = model_mapping[imagemodel]\n",
        "vqgan_name = selected_vqgan[\"name\"]\n",
        "\n",
        "config = selected_vqgan[\"config\"]\n",
        "checkpoint = selected_vqgan[\"checkpoint\"]\n",
        "\n",
        "if not Path(vqgan_name).exists():\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{config}' -o {vqgan_name}.yaml\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{checkpoint}'  -o {vqgan_name}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "outputs": [],
      "source": [
        "# import argparse\n",
        "# import math\n",
        "# from pathlib import Path\n",
        "# import sys\n",
        "\n",
        "# sys.path.append('./taming-transformers')\n",
        "\n",
        "# import tensorflow\n",
        "# from IPython import display\n",
        "# from omegaconf import OmegaConf\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# from torch import nn, optim\n",
        "# from torch.nn import functional as F\n",
        "# from torchvision import transforms\n",
        "# from torchvision.transforms import functional as TF\n",
        "# from taming.models import cond_transformer, vqgan\n",
        "# from tqdm.notebook import tqdm\n",
        "# import numpy as np\n",
        "\n",
        "# from CLIP import clip\n",
        "\n",
        "# import kornia.augmentation as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JvnTBhPGT1gn"
      },
      "outputs": [],
      "source": [
        "# def noise_gen(shape):\n",
        "#     n, c, h, w = shape\n",
        "#     noise = torch.zeros([n, c, 1, 1])\n",
        "#     for i in reversed(range(5)):\n",
        "#         h_cur, w_cur = h // 2**i, w // 2**i\n",
        "#         noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "#         noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "#     return noise\n",
        "\n",
        "\n",
        "# def sinc(x):\n",
        "#     return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "# def lanczos(x, a):\n",
        "#     cond = torch.logical_and(-a < x, x < a)\n",
        "#     out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "#     return out / out.sum()\n",
        "\n",
        "\n",
        "# def ramp(ratio, width):\n",
        "#     n = math.ceil(width / ratio + 1)\n",
        "#     out = torch.empty([n])\n",
        "#     cur = 0\n",
        "#     for i in range(out.shape[0]):\n",
        "#         out[i] = cur\n",
        "#         cur += ratio\n",
        "#     return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "# def resample(input, size, align_corners=True):\n",
        "#     n, c, h, w = input.shape\n",
        "#     dh, dw = size\n",
        "\n",
        "#     input = input.view([n * c, 1, h, w])\n",
        "\n",
        "#     if dh < h:\n",
        "#         kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "#         pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "#         input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "#         input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "#     if dw < w:\n",
        "#         kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "#         pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "#         input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "#         input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "#     input = input.view([n, c, h, w])\n",
        "#     return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# # def replace_grad(fake, real):\n",
        "# #     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "# class ReplaceGrad(torch.autograd.Function):\n",
        "#     @staticmethod\n",
        "#     def forward(ctx, x_forward, x_backward):\n",
        "#         ctx.shape = x_backward.shape\n",
        "#         return x_forward\n",
        "\n",
        "#     @staticmethod\n",
        "#     def backward(ctx, grad_in):\n",
        "#         return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "# class ClampWithGrad(torch.autograd.Function):\n",
        "#     @staticmethod\n",
        "#     def forward(ctx, input, min, max):\n",
        "#         ctx.min = min\n",
        "#         ctx.max = max\n",
        "#         ctx.save_for_backward(input)\n",
        "#         return input.clamp(min, max)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def backward(ctx, grad_in):\n",
        "#         input, = ctx.saved_tensors\n",
        "#         return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "# replace_grad = ReplaceGrad.apply\n",
        "\n",
        "# clamp_with_grad = ClampWithGrad.apply\n",
        "# # clamp_with_grad = torch.clamp\n",
        "\n",
        "# def vector_quantize(x, codebook):\n",
        "#     d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "#     indices = d.argmin(-1)\n",
        "#     x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "#     return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "# class Prompt(nn.Module):\n",
        "#     def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "#         super().__init__()\n",
        "#         self.register_buffer('embed', embed)\n",
        "#         self.register_buffer('weight', torch.as_tensor(weight))\n",
        "#         self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "#     def forward(self, input):\n",
        "        \n",
        "#         input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "#         embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\n",
        "\n",
        "#         dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "#         dists = dists * self.weight.sign()\n",
        "#         return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "# def parse_prompt(prompt):\n",
        "#     vals = prompt.rsplit(':', 2)\n",
        "#     vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "#     return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "# def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "#     input_normed = F.normalize(input, dim=-1)\n",
        "#     target_normed = F.normalize(target, dim=-1)\n",
        "#     logits = input_normed @ target_normed.T * logit_scale\n",
        "#     if labels is None:\n",
        "#         labels = torch.arange(len(input), device=logits.device)\n",
        "#     return F.cross_entropy(logits, labels)\n",
        "\n",
        "# class MakeCutouts(nn.Module):\n",
        "#     def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "#         super().__init__()\n",
        "#         self.cut_size = cut_size\n",
        "#         self.cutn = cutn\n",
        "#         self.cut_pow = cut_pow\n",
        "\n",
        "#         self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "#         self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "#     def set_cut_pow(self, cut_pow):\n",
        "#       self.cut_pow = cut_pow\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         sideY, sideX = input.shape[2:4]\n",
        "#         max_size = min(sideX, sideY)\n",
        "#         min_size = min(sideX, sideY, self.cut_size)\n",
        "#         cutouts = []\n",
        "#         cutouts_full = []\n",
        "        \n",
        "#         min_size_width = min(sideX, sideY)\n",
        "#         lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "#         for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "#           size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "\n",
        "#           offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "#           offsety = torch.randint(0, sideY - size + 1, ())\n",
        "#           cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "#           cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        \n",
        "#         cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "#         if args.use_augs:\n",
        "#           cutouts = augs(cutouts)\n",
        "\n",
        "#         if args.noise_fac:\n",
        "#           facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "#           cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        \n",
        "\n",
        "#         return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "# def load_vqgan_model(config_path, checkpoint_path):\n",
        "#     config = OmegaConf.load(config_path)\n",
        "#     if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "#         model = vqgan.VQModel(**config.model.params)\n",
        "#         model.eval().requires_grad_(False)\n",
        "#         model.init_from_ckpt(checkpoint_path)\n",
        "#     elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "#         parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "#         parent_model.eval().requires_grad_(False)\n",
        "#         parent_model.init_from_ckpt(checkpoint_path)\n",
        "#         model = parent_model.first_stage_model\n",
        "#     elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "#         model = vqgan.GumbelVQ(**config.model.params)\n",
        "#         model.eval().requires_grad_(False)\n",
        "#         model.init_from_ckpt(checkpoint_path)\n",
        "#     else:\n",
        "#         raise ValueError(f'unknown model type: {config.model.target}')\n",
        "#     del model.loss\n",
        "#     return model\n",
        "\n",
        "# def resize_image(image, out_size):\n",
        "#     ratio = image.size[0] / image.size[1]\n",
        "#     area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "#     size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "#     return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "# class TVLoss(nn.Module):\n",
        "#     def forward(self, input):\n",
        "#         input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "#         x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "#         y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "#         diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "#         return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "# class GaussianBlur2d(nn.Module):\n",
        "#     def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "#         super().__init__()\n",
        "#         self.mode = mode\n",
        "#         self.value = value\n",
        "#         if not window:\n",
        "#             window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "#         if sigma:\n",
        "#             kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "#             kernel /= kernel.sum()\n",
        "#         else:\n",
        "#             kernel = torch.ones([1])\n",
        "#         self.register_buffer('kernel', kernel)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         n, c, h, w = input.shape\n",
        "#         input = input.view([n * c, 1, h, w])\n",
        "#         start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "#         end_pad = self.kernel.shape[0] // 2\n",
        "#         input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "#         input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "#         input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "#         return input.view([n, c, h, w])\n",
        "\n",
        "# class EMATensor(nn.Module):\n",
        "#     \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "#     def __init__(self, tensor, decay):\n",
        "#         super().__init__()\n",
        "#         self.tensor = nn.Parameter(tensor)\n",
        "#         self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "#         self.register_buffer('average', torch.zeros_like(tensor))\n",
        "#         self.decay = decay\n",
        "#         self.register_buffer('accum', torch.tensor(1.))\n",
        "#         self.update()\n",
        "    \n",
        "#     @torch.no_grad()\n",
        "#     def update(self):\n",
        "#         if not self.training:\n",
        "#             raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "#         self.accum *= self.decay\n",
        "#         self.biased.mul_(self.decay)\n",
        "#         self.biased.add_((1 - self.decay) * self.tensor)\n",
        "#         self.average.copy_(self.biased)\n",
        "#         self.average.div_(1 - self.accum)\n",
        "\n",
        "#     def forward(self):\n",
        "#         if self.training:\n",
        "#             return self.tensor\n",
        "#         return self.average\n",
        "\n",
        "# %mkdir /content/vids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tLw9p5Rzacso"
      },
      "outputs": [],
      "source": [
        "# # scale step size\n",
        "# step_size = step_size / 100\n",
        "# step_size = step_size*step_size\n",
        "\n",
        "# if vqgan_name == 'vqgan_openimages_f16_8192.ckpt':\n",
        "#   step_size=step_size / 20\n",
        "  \n",
        "# args = argparse.Namespace(\n",
        "    \n",
        "#     prompts=[t.strip() for t in text_input.split(\"|\")],\n",
        "#     size=[width, height], \n",
        "#     init_image=image_prompt,\n",
        "#     init_weight= 0.5,\n",
        "\n",
        "#     # clip model settings\n",
        "#     clip_model='ViT-B/32',\n",
        "#     vqgan_config=f'{vqgan_name}.yaml',         \n",
        "#     vqgan_checkpoint=vqgan_name,\n",
        "#     step_size=step_size,\n",
        "    \n",
        "#     # cutouts / crops\n",
        "#     cutn=32,\n",
        "#     cut_pow=1,\n",
        "#     cut_size=224,\n",
        "\n",
        "#     # display\n",
        "#     display_freq=5,\n",
        "#     seed=None,\n",
        "#     use_augs = True,\n",
        "#     noise_fac= 0.1,\n",
        "\n",
        "#     record_generation=True,\n",
        "\n",
        "#     # noise and other constraints\n",
        "#     use_noise = None,\n",
        "#     constraint_regions = False,#\n",
        "    \n",
        "    \n",
        "#     # add noise to embedding\n",
        "#     noise_prompt_weights = None,\n",
        "#     noise_prompt_seeds = [14575],#\n",
        "\n",
        "#     # mse settings\n",
        "#     mse_withzeros = True,\n",
        "#     mse_decay_rate = 50,\n",
        "#     mse_epoches = 10,\n",
        "\n",
        "#     # end itteration\n",
        "#     max_itter = iterations,\n",
        "# )\n",
        "\n",
        "# mse_decay = 0\n",
        "# if args.init_weight:\n",
        "#   mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "# # <AUGMENTATIONS>\n",
        "# augs = nn.Sequential(\n",
        "    \n",
        "#     K.RandomHorizontalFlip(p=0.5),\n",
        "#     K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "#     K.RandomPerspective(0.2,p=0.4, ),\n",
        "#     K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "#     )\n",
        "\n",
        "# noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "# image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "# image.save('init3.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgTa_JWi7Sn"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g7EDme5RYCrt"
      },
      "outputs": [],
      "source": [
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# print('Using device:', device)\n",
        "# print('using prompts: ', args.prompts)\n",
        "\n",
        "# tv_loss = TVLoss() \n",
        "\n",
        "# model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "# perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "# mse_weight = args.init_weight\n",
        "\n",
        "# cut_size = args.cut_size\n",
        "# # e_dim = model.quantize.e_dim\n",
        "\n",
        "# if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "#     e_dim = 256\n",
        "#     n_toks = model.quantize.n_embed\n",
        "#     z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "#     z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "# else:\n",
        "#     e_dim = model.quantize.e_dim\n",
        "#     n_toks = model.quantize.n_e\n",
        "#     z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "#     z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "# make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "# f = 2**(model.decoder.num_resolutions - 1)\n",
        "# toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "\n",
        "# if args.seed is not None:\n",
        "#     torch.manual_seed(args.seed)\n",
        "\n",
        "# if args.init_image:\n",
        "#     pil_image = Image.open(args.init_image).convert('RGB')\n",
        "#     pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "#     pil_image = TF.to_tensor(pil_image)\n",
        "#     if args.use_noise:\n",
        "#       pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "#     z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "# else:\n",
        "    \n",
        "#     one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "#     if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "#         z = one_hot @ model.quantize.embed.weight\n",
        "#     else:\n",
        "#         z = one_hot @ model.quantize.embedding.weight\n",
        "#     z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "# if args.mse_withzeros and not args.init_image:\n",
        "#   z_orig = torch.zeros_like(z)\n",
        "# else:\n",
        "#   z_orig = z.clone()\n",
        "\n",
        "# z.requires_grad = True\n",
        "\n",
        "# opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "# normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "#                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "# pMs = []\n",
        "\n",
        "# if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "#   for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "#     gen = torch.Generator().manual_seed(seed)\n",
        "#     embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "#     pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "# for prompt in args.prompts:\n",
        "#     txt, weight, stop = parse_prompt(prompt)\n",
        "#     embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "#     pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "# def synth_gumbel(z, quantize=True, saturate=True):\n",
        "#     logits = model.quantize.proj(z)\n",
        "#     if quantize:\n",
        "#         one_hot = F.gumbel_softmax(logits, tau=1, hard=True, dim=1)\n",
        "#     else:\n",
        "#         one_hot = F.one_hot(logits.argmax(1), logits.shape[1]).movedim(3, 1).to(logits.dtype)\n",
        "#     z_q = torch.einsum('nchw,cd->ndhw', one_hot, model.quantize.embed.weight)\n",
        "#     return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "# def synth(z, quantize=True, saturate=True):\n",
        "#     out = None\n",
        "#     if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "#        out = synth_gumbel(z, quantize, saturate)\n",
        "#     else:\n",
        "#       if args.constraint_regions:\n",
        "#         z = replace_grad(z, z * z_mask)\n",
        "\n",
        "#       if quantize:\n",
        "#         z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "#       else:\n",
        "#         z_q = z.model\n",
        "\n",
        "#       out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "#     if saturate and not image_prompt:\n",
        "#       progress = i / args.max_itter\n",
        "#       saturation = max(0,min(1,(progress - 0.25) * 2))\n",
        "#       out = transforms.functional.adjust_saturation(out, saturation)\n",
        "    \n",
        "#     return out\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def checkin(i, losses):\n",
        "#     losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "#     tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "#     out = synth(z, True)# False)\n",
        "\n",
        "#     TF.to_pil_image(out[0].cpu()).save(f'progress.png')   \n",
        "#     #display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "# def ascend_txt(i):\n",
        "#     global mse_weight\n",
        "\n",
        "#     out = synth(z)\n",
        "#     if args.record_generation:\n",
        "#       with torch.no_grad():\n",
        "#         global vid_index\n",
        "#         out_a = synth(z, True)#, False)\n",
        "#         if vid_index % 5 == 0:\n",
        "#           filename = f'{output_path}/progress_{vid_index:05}.jpg'\n",
        "#           TF.to_pil_image(out_a[0].cpu()).save(filename)\n",
        "#           upscale(filename)\n",
        "#         vid_index += 1\n",
        "\n",
        "#     cutouts = make_cutouts(out)\n",
        "#     cutouts = resample(cutouts, (perceptor.visual.input_resolution, perceptor.visual.input_resolution))\n",
        "\n",
        "\n",
        "#     iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "#     result = []\n",
        "\n",
        "#     if args.init_weight:\n",
        "        \n",
        "#         global z_orig\n",
        "        \n",
        "#         result.append(F.mse_loss(z, z_orig) * mse_weight / 2)\n",
        "#         # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#           if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "#             if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "#               mse_weight = mse_weight - mse_decay\n",
        "#               print(f\"updated mse weight: {mse_weight}\")\n",
        "#             else:\n",
        "#               mse_weight = 0\n",
        "#               print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "#     for prompt in pMs:\n",
        "#         result.append(prompt(iii))\n",
        "\n",
        "#     return result\n",
        "\n",
        "# vid_index = 0\n",
        "# def train(i):\n",
        "    \n",
        "#     opt.zero_grad()\n",
        "#     lossAll = ascend_txt(i)\n",
        "\n",
        "#     if i % args.display_freq == 0:\n",
        "#         checkin(i, lossAll)\n",
        "    \n",
        "#     loss = sum(lossAll)\n",
        "\n",
        "#     loss.backward()\n",
        "#     opt.step()\n",
        "\n",
        "# i = 0\n",
        "# try:\n",
        "#     with tqdm() as pbar:\n",
        "#         while True and i != args.max_itter:\n",
        "\n",
        "#             train(i)\n",
        "\n",
        "#             if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "              \n",
        "#               opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "#             i += 1\n",
        "#             pbar.update()\n",
        "\n",
        "# except KeyboardInterrupt:\n",
        "#     pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# create video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DT3hKb5gJUPq"
      },
      "outputs": [],
      "source": [
        "# out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "# !mkdir -p /tmp/ffmpeg\n",
        "# !cp $output_path/*.jpg /tmp/ffmpeg\n",
        "# last_frame=!ls -t /tmp/ffmpeg/*.jpg | head -1\n",
        "# last_frame = last_frame[0]\n",
        "\n",
        "# # Copy last frame to start and duplicate at end so it sticks around longer\n",
        "# end_still_seconds = 4\n",
        "# !cp -v $last_frame /tmp/ffmpeg/0000.jpg\n",
        "# for i in range(end_still_seconds * 10):\n",
        "#   pad_file = f\"/tmp/ffmpeg/zzzz_pad_{i:05}.jpg\"\n",
        "#   !cp -v $last_frame $pad_file\n",
        "\n",
        "# !ffmpeg  -r 10 -i /tmp/ffmpeg/%*.jpg -y -c:v libx264 /tmp/vid_no_audio.mp4\n",
        "# !ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n",
        "\n",
        "# print(\"Written\", out_file)\n",
        "# !sleep 2\n",
        "# !rm -r /tmp/ffmpeg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text-To-Image - CLIP-Guided VQGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}