{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/notebooks/1%20Text-To-Image/2%20CLIP-Guided%20VQGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "\n",
        "<img src=\"https://cloudflare-ipfs.com/ipfs/QmUKydjhLKP26P8qT49cvsDYXjmYAShbo2Z95JxatvzwkK\" width=\"300\" height=\"300\" />\n",
        "\n",
        "Input: \"*Self-pollination in the style of Ernst Haeckel*\"\n",
        "\n",
        "\n",
        "This model has defined the looks of text-to-image of summer 2021 and has been dominating generative media for quite a while. It works especially well with a style or artist in the text prompt. Another interesting feature is the ability to provide an image from which it starts.\n",
        "\n",
        "---\n",
        "\n",
        "It is possible to provide multiple text prompts with weights by providing them like this: \n",
        "**```oil painting: 0.5|salvador dali: 0.3|edward much:0.6|robot friend: 1.0|text:-0.5```** *In this case a negative weight next to `text` makes the model avoid text.* \n",
        "\n",
        "Credits: [Katherine Crowson](https://twitter.com/RiversHaveWings), [jbuster](https://twitter.com/jbusted1), [thomash](https://twitter.com/pollinations_ai)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWawsyA5GZAg"
      },
      "source": [
        "\n",
        "[UPD 13.12.2021] Re-order library imports to fix bug that was crashing a lot of runs\n",
        "\n",
        "[UPD 10.12.2021] Image prompting and extra parameters\n",
        "\n",
        "[UPD 3.11.2021] Added Gumbel sampling method suggested by Daniel Russ on Discord\n",
        "\n",
        "[UPD 2.11.2021] Added ruDALLE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pZmCmyKM9fmv",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Text Prompt\n",
        "text_input = 'Self-pollination in the style of Ernst Haeckel'  #@param {type: \"string\"}\n",
        "\n",
        "# Width in pixels of image to be generated. If you put this too high the GPU may run out of memory. Rather use super-resolution to achieve high resolutions. (advanced)\n",
        "width = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Height in pixels of image to be generated. (see width) (advanced)\n",
        "height = 512 #@param {type: \"number\"}\n",
        "\n",
        "# Apply a (neural) super-resolution step (2 x image width x image height)\n",
        "super_resolution = False #@param {type: \"boolean\"}\n",
        "\n",
        "# Image prompt. Leave blank if you want to use only text\n",
        "image_file = '' #@param {type: \"string\"}\n",
        "\n",
        "# Step size - how fast to try and optimize the image. Between 0 and 100 (advanced)\n",
        "step_size = 30 #@param {type: \"number\"}\n",
        "\n",
        "# Iterations - how many frames to optimize the image. Determines the length of the output video (advanced)\n",
        "iterations = 50 #@param {type: \"number\"}\n",
        "\n",
        "\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "social = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if image_file == '':\n",
        "    !pip install git+https://github.com/openai/CLIP.git\n",
        "    import clip\n",
        "    import torch\n",
        "\n",
        "    # Load the open CLIP model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Create a folder for the precomputed features\n",
        "    !mkdir unsplash-dataset\n",
        "\n",
        "    # Download from Github Releases\n",
        "    if not Path('unsplash-dataset/photo_ids.csv').exists():\n",
        "        !wget https://github.com/haltakov/natural-language-image-search/releases/download/1.0.0/photo_ids.csv -O unsplash-dataset/photo_ids.csv\n",
        "\n",
        "    if not Path('unsplash-dataset/features.npy').exists():\n",
        "        !wget https://github.com/haltakov/natural-language-image-search/releases/download/1.0.0/features.npy -O unsplash-dataset/features.npy\n",
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Load the photo IDs\n",
        "    photo_ids = pd.read_csv(\"unsplash-dataset/photo_ids.csv\")\n",
        "    photo_ids = list(photo_ids['photo_id'])\n",
        "\n",
        "    # Load the features vectors\n",
        "    photo_features = np.load(\"unsplash-dataset/features.npy\")\n",
        "\n",
        "    # Convert features to Tensors: Float32 on CPU and Float16 on GPU\n",
        "    if device == \"cpu\":\n",
        "        photo_features = torch.from_numpy(photo_features).float().to(device)\n",
        "    else:\n",
        "        photo_features = torch.from_numpy(photo_features).to(device)\n",
        "\n",
        "    # Print some statistics\n",
        "    print(f\"Photos loaded: {len(photo_ids)}\")\n",
        "\n",
        "\n",
        "    def encode_search_query(search_query):\n",
        "        with torch.no_grad():\n",
        "            # Encode and normalize the search query using CLIP\n",
        "            text_encoded = clip_model.encode_text(clip.tokenize(search_query).to(device))\n",
        "            text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Retrieve the feature vector\n",
        "        return text_encoded\n",
        "\n",
        "\n",
        "    def find_best_matches(text_features, photo_features, photo_ids, results_count=3):\n",
        "        # Compute the similarity between the search query and each photo using the Cosine similarity\n",
        "        similarities = (photo_features @ text_features.T).squeeze(1)\n",
        "\n",
        "        # Sort the photos by their similarity score\n",
        "        best_photo_idx = (-similarities).argsort()\n",
        "\n",
        "        # Return the photo IDs of the best matches\n",
        "        return [photo_ids[i] for i in best_photo_idx[:results_count]]\n",
        "\n",
        "\n",
        "    !apt-get install imagemagick\n",
        "    def search_unslash(search_query, photo_features, photo_ids, results_count=10):\n",
        "        # Encode the search query\n",
        "        text_features = encode_search_query(search_query)\n",
        "\n",
        "        # Find the best matches\n",
        "        best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, results_count)\n",
        "\n",
        "        # Selecgt random photo\n",
        "        best_photo_id = best_photo_ids[np.random.randint(len(best_photo_ids))]\n",
        "        print(best_photo_id)\n",
        "        !wget https://unsplash.com/photos/{best_photo_id}/download?w=512 -O /content/photo.jpg\n",
        "        !convert -blur 0x4 photo.jpg photo.jpg\n",
        "        return \"/content/photo.jpg\"\n",
        "\n",
        "    image_file = search_unslash(text_input, photo_features, photo_ids, 8)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ1JypO0WBWQ"
      },
      "outputs": [],
      "source": [
        "# Model to use. ruDALLE was recently finetuned by a russian company called Sber and could give better outputs.\n",
        "imagemodel = \"imagenet\" \n",
        " #@param ['imagenet', 'ruDALLE']\n",
        "\n",
        "\n",
        "# check if christmas and add christmas to prompt\n",
        "from datetime import datetime\n",
        "\n",
        "d = datetime.now()\n",
        "\n",
        "if social and (d.strftime(\"%m\") == \"12\") and  ((d.strftime(\"%d\") == \"24\") or  (d.strftime(\"%d\") == \"25\") or  (d.strftime(\"%d\") == \"26\")):\n",
        "    text_input = text_input + \"|christmas:3|text:-0.5\"\n",
        "\n",
        "image_prompt = image_file\n",
        "\n",
        "if len(image_prompt) == 0:\n",
        "    image_prompt = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VIf9Z1CTsC7R"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "#@title Upscale images/video frames\n",
        "\n",
        "black_and_white = False\n",
        "\n",
        "loaded_upscale_model = False\n",
        "\n",
        "def upscale(filepath):\n",
        "  if not super_resolution:\n",
        "    return\n",
        "  global loaded_upscale_model\n",
        "  if not loaded_upscale_model:\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd /content/Real-ESRGAN\n",
        "    !git checkout 3338b31f486586bd7f6b20cc2a9fadd5ed192a00\n",
        "    # Set up the environment\n",
        "    !pip install git+https://github.com/xinntao/BasicSR.git\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    %cd -\n",
        "    loaded_upscale_model = True \n",
        "  \n",
        "  %cd /content/Real-ESRGAN\n",
        "  !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth --input $filepath --netscale 4 --outscale 2 --half --output $output_path\n",
        "  filepath_out = filepath.replace(\".jpg\",\"_out.jpg\")\n",
        "  !mv -v $filepath_out $filepath\n",
        "  %cd -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf54A275YLpt"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSfISAhyPmyp"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $output_path\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "#%cd taming-transformers\n",
        "#!git checkout 2908a53b88478e5812d619b6ac003dbb29b069a0\n",
        "#%cd -\n",
        "#!pip install torchvision@https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl torch torchaudio torchtext\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhhdWrSxQhwg"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "!sudo apt install -y aria2\n",
        "\n",
        "model_mapping = {\n",
        "    \"ruDALLE\": { \n",
        "        \"name\": \"vqgan_openimages_f16_8192.ckpt\",\n",
        "        \"config\": \"https://cloudflare-ipfs.com/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt.yaml\",\n",
        "        \"checkpoint\": \"https://cloudflare-ipfs.com/ipfs/QmXey26KJ1S5fc5gtrXbGqdpgc3xvoQiApVYCxzE5uB9D4/vqgan.gumbelf8-sber.model.ckpt\",\n",
        "    },\n",
        "    \"imagenet\": {\n",
        "        \"name\": \"vqgan_imagenet_f16_16384.ckpt\",\n",
        "        \"config\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1',\n",
        "        \"checkpoint\": 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1',        \n",
        "    } \n",
        "}\n",
        "\n",
        "selected_vqgan = model_mapping[imagemodel]\n",
        "vqgan_name = selected_vqgan[\"name\"]\n",
        "\n",
        "config = selected_vqgan[\"config\"]\n",
        "checkpoint = selected_vqgan[\"checkpoint\"]\n",
        "\n",
        "if not Path(vqgan_name+\".yaml\").exists():\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{config}' -o {vqgan_name}.yaml\n",
        "    !aria2c -x 5 --auto-file-renaming=false '{checkpoint}'  -o {vqgan_name}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "import tensorflow\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from taming.models import cond_transformer, vqgan\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import kornia.augmentation as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvnTBhPGT1gn"
      },
      "outputs": [],
      "source": [
        "def noise_gen(shape):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    for i in reversed(range(5)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    try:\n",
        "        return vals[0], float(vals[1]), float(vals[2])\n",
        "    except:\n",
        "        return prompt, 1, float('-inf')\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        if args.use_augs:\n",
        "          cutouts = augs(cutouts)\n",
        "\n",
        "        if args.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        \n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "\n",
        "%mkdir /content/vids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLw9p5Rzacso"
      },
      "outputs": [],
      "source": [
        "# scale step size\n",
        "step_size = step_size / 100\n",
        "step_size = step_size*step_size\n",
        "\n",
        "if vqgan_name == 'vqgan_openimages_f16_8192.ckpt':\n",
        "  step_size=step_size / 20\n",
        "  \n",
        "args = argparse.Namespace(\n",
        "    \n",
        "    prompts=[t.strip() for t in text_input.split(\"|\")],\n",
        "    size=[width, height], \n",
        "    init_image=image_prompt,\n",
        "    init_weight= 0.5,\n",
        "\n",
        "    # clip model settings\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=f'{vqgan_name}.yaml',         \n",
        "    vqgan_checkpoint=vqgan_name,\n",
        "    step_size=step_size,\n",
        "    \n",
        "    # cutouts / crops\n",
        "    cutn=32,\n",
        "    cut_pow=1,\n",
        "    cut_size=224,\n",
        "\n",
        "    # display\n",
        "    display_freq=5,\n",
        "    seed=None,\n",
        "    use_augs = True,\n",
        "    noise_fac= 0.1,\n",
        "\n",
        "    record_generation=True,\n",
        "\n",
        "    # noise and other constraints\n",
        "    use_noise = None,\n",
        "    constraint_regions = False,#\n",
        "    \n",
        "    \n",
        "    # add noise to embedding\n",
        "    noise_prompt_weights = None,\n",
        "    noise_prompt_seeds = [14575],#\n",
        "\n",
        "    # mse settings\n",
        "    mse_withzeros = True,\n",
        "    mse_decay_rate = 50,\n",
        "    mse_epoches = 10,\n",
        "\n",
        "    # end itteration\n",
        "    max_itter = iterations,\n",
        ")\n",
        "\n",
        "mse_decay = 0\n",
        "if args.init_weight:\n",
        "  mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "# <AUGMENTATIONS>\n",
        "augs = nn.Sequential(\n",
        "    \n",
        "    K.RandomHorizontalFlip(p=0.5),\n",
        "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "    K.RandomPerspective(0.2,p=0.4, ),\n",
        "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "    )\n",
        "\n",
        "noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "image.save('init3.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgTa_JWi7Sn"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7EDme5RYCrt"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Using device:', device)\n",
        "print('using prompts: ', args.prompts)\n",
        "\n",
        "tv_loss = TVLoss() \n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "mse_weight = args.init_weight\n",
        "\n",
        "cut_size = args.cut_size\n",
        "# e_dim = model.quantize.e_dim\n",
        "\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "    e_dim = 256\n",
        "    n_toks = model.quantize.n_embed\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "else:\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "\n",
        "if args.seed is not None:\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "if args.init_image:\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\n",
        "    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "    pil_image = TF.to_tensor(pil_image)\n",
        "    if args.use_noise:\n",
        "      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "else:\n",
        "    \n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z = one_hot @ model.quantize.embed.weight\n",
        "    else:\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "if args.mse_withzeros and not args.init_image:\n",
        "  z_orig = torch.zeros_like(z)\n",
        "else:\n",
        "  z_orig = z.clone()\n",
        "\n",
        "z.requires_grad = True\n",
        "\n",
        "opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "pMs = []\n",
        "\n",
        "if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "def synth_gumbel(z, quantize=True, saturate=True):\n",
        "    logits = model.quantize.proj(z)\n",
        "    if quantize:\n",
        "        one_hot = F.gumbel_softmax(logits, tau=1, hard=True, dim=1)\n",
        "    else:\n",
        "        one_hot = F.one_hot(logits.argmax(1), logits.shape[1]).movedim(3, 1).to(logits.dtype)\n",
        "    z_q = torch.einsum('nchw,cd->ndhw', one_hot, model.quantize.embed.weight)\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "def synth(z, quantize=True, saturate=True):\n",
        "    out = None\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "       out = synth_gumbel(z, quantize, saturate)\n",
        "    else:\n",
        "      if args.constraint_regions:\n",
        "        z = replace_grad(z, z * z_mask)\n",
        "\n",
        "      if quantize:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "      else:\n",
        "        z_q = z.model\n",
        "\n",
        "      out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "    if saturate and not image_prompt:\n",
        "      progress = i / args.max_itter\n",
        "      saturation = max(0,min(1,(progress - 0.25) * 2))\n",
        "      out = transforms.functional.adjust_saturation(out, saturation)\n",
        "    \n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z, True)# False)\n",
        "\n",
        "    TF.to_pil_image(out[0].cpu()).save(f'progress.png')   \n",
        "    #display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "def ascend_txt(i):\n",
        "    global mse_weight\n",
        "\n",
        "    out = synth(z)\n",
        "    if args.record_generation:\n",
        "      with torch.no_grad():\n",
        "        global vid_index\n",
        "        out_a = synth(z, True)#, False)\n",
        "        if vid_index % 5 == 0:\n",
        "          filename = f'{output_path}/progress_{vid_index:05}.jpg'\n",
        "          TF.to_pil_image(out_a[0].cpu()).save(filename)\n",
        "          upscale(filename)\n",
        "        vid_index += 1\n",
        "\n",
        "    cutouts = make_cutouts(out)\n",
        "    cutouts = resample(cutouts, (perceptor.visual.input_resolution, perceptor.visual.input_resolution))\n",
        "\n",
        "\n",
        "    iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        \n",
        "        global z_orig\n",
        "        \n",
        "        result.append(F.mse_loss(z, z_orig) * mse_weight / 2)\n",
        "        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "              mse_weight = mse_weight - mse_decay\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "            else:\n",
        "              mse_weight = 0\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "\n",
        "    return result\n",
        "\n",
        "vid_index = 0\n",
        "def train(i):\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt(i)\n",
        "\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    \n",
        "    loss = sum(lossAll)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    for i in tqdm(range(args.max_itter)):\n",
        "\n",
        "        train(i)\n",
        "\n",
        "        if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "          \n",
        "          opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# create video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT3hKb5gJUPq",
        "outputId": "f46af883-7177-42f9-a13d-ee22d4bfde67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00005.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00006.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00007.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00008.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00009.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00010.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00011.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00012.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00013.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00014.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00015.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00016.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00017.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00018.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00019.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00020.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00021.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00022.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00023.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00024.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00025.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00026.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00027.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00028.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00029.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00030.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00031.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00032.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00033.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00034.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00035.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00036.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00037.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00038.jpg'\n",
            "'/tmp/ffmpeg/progress_00555.jpg' -> '/tmp/ffmpeg/zzzz_pad_00039.jpg'\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "\u001b[0;35m[image2 @ 0x564ac3686000] \u001b[0m\u001b[0;33mPattern type 'glob_sequence' is deprecated: use pattern_type 'glob' instead\n",
            "\u001b[0mInput #0, image2, from '/tmp/ffmpeg/%*.jpg':\n",
            "  Duration: 00:00:06.44, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 640x480 [SAR 1:1 DAR 4:3], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mprofile High, level 2.2\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/tmp/vid_no_audio.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 640x480 [SAR 1:1 DAR 4:3], q=-1--1, 10 fps, 10240 tbn, 10 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  161 fps= 32 q=-1.0 Lsize=    3574kB time=00:00:15.80 bitrate=1852.9kbits/s speed=3.14x    \n",
            "video:3571kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.076380%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mframe I:4     Avg QP:18.73  size: 38319\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mframe P:42    Avg QP:22.18  size: 25205\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mframe B:115   Avg QP:24.53  size: 21253\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mconsecutive B-frames:  4.3%  0.0%  3.7% 91.9%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mmb I  I16..4:  5.3% 93.8%  1.0%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mmb P  I16..4:  2.8% 60.1%  1.1%  P16..4:  5.0%  4.6%  2.5%  0.0%  0.0%    skip:23.9%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mmb B  I16..4:  1.6% 18.1%  1.7%  B16..8: 16.6% 14.1%  6.5%  direct:11.3%  skip:30.1%  L0:35.8% L1:29.2% BI:35.0%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0m8x8 transform intra:89.6% inter:92.1%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mcoded y,uvDC,uvAC intra: 84.7% 45.7% 22.5% inter: 48.3% 42.2% 7.0%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi16 v,h,dc,p:  4% 69%  3% 25%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 45% 16%  2%  2%  3%  4%  3%  5%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 20% 12%  7%  8%  9%  9%  9% 10%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mi8c dc,h,v,p: 70% 15% 10%  5%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mWeighted P-Frames: Y:19.0% UV:14.3%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mref P L0: 35.2% 17.1% 25.9% 18.6%  3.1%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mref B L0: 79.9% 14.6%  5.5%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mref B L1: 92.7%  7.3%\n",
            "\u001b[1;36m[libx264 @ 0x564ac368be00] \u001b[0mkb/s:1816.65\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/vid_no_audio.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "  Duration: 00:00:16.10, start: 0.000000, bitrate: 1818 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuvj420p(pc), 640x480 [SAR 1:1 DAR 4:3], 1816 kb/s, 10 fps, 10 tbr, 10240 tbn, 20 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Input #1, lavfi, from 'anullsrc':\n",
            "  Duration: N/A, start: 0.000000, bitrate: 705 kb/s\n",
            "    Stream #1:0: Audio: pcm_u8, 44100 Hz, stereo, u8, 705 kb/s\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (copy)\n",
            "  Stream #1:0 -> #0:1 (pcm_u8 (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp4, to '/content/output/video.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuvj420p(pc), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 1816 kb/s, 10 fps, 10 tbr, 10240 tbn, 10240 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 aac\n",
            "frame=  161 fps=0.0 q=-1.0 Lsize=    3583kB time=00:00:15.83 bitrate=1853.7kbits/s speed= 153x    \n",
            "video:3571kB audio:4kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.231886%\n",
            "\u001b[1;36m[aac @ 0x563950b49700] \u001b[0mQavg: 65536.000\n",
            "Written /content/output/video.mp4\n"
          ]
        }
      ],
      "source": [
        "out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "!mkdir -p /tmp/ffmpeg\n",
        "!cp $output_path/*.jpg /tmp/ffmpeg\n",
        "last_frame=!ls -w1 -t /tmp/ffmpeg/*.jpg | head -1\n",
        "last_frame = last_frame[0]\n",
        "\n",
        "# Copy last frame to start and duplicate at end so it sticks around longer\n",
        "end_still_seconds = 4\n",
        "for i in range(end_still_seconds * 10):\n",
        "  pad_file = f\"/tmp/ffmpeg/zzzz_pad_{i:05}.jpg\"\n",
        "  !cp -v $last_frame $pad_file\n",
        "\n",
        "!ffmpeg  -r 3 -i /tmp/ffmpeg/%*.jpg -y -c:v libx264 -r 15 /tmp/vid_no_audio.mp4\n",
        "!ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n",
        "\n",
        "print(\"Written\", out_file)\n",
        "!sleep 2\n",
        "!rm -r /tmp/ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzHhh98RGZAp"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "if not os.path.exists(out_file):\n",
        "  raise Exception(\"Expected output file does not exist.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Text-To-Image - CLIP-Guided VQGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
