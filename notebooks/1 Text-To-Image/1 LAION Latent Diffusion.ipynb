{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/notebooks/1%20Text-To-Image/1%20LAION%20Latent%20Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "<img src=\"https://cloudflare-ipfs.com/ipfs/QmZwyfox3JsR34F3e82pPbrbBhKPFrUEsXuBFuKJfiiHxp?filename=abstract-geometric-chihuahua-.png\" width=\"300\" height=\"300\" />\n",
        "\n",
        "Example: *Abstract geometric Chihuahua*\n",
        "\n",
        "Latent Diffusion model Text-to-image synthesis with 1.45B parameter model. This great model rivals OpenAI's DALL-E and can generate beautiful artistic styles as well as faces and text.\n",
        "\n",
        "**Should work with the free mode of Google Colab now**\n",
        "\n",
        "---\n",
        "Credits: model released by [CompVis](https://github.com/CompVis/latent-diffusion) and trained on the [LAION-400M dataset](https://laion.ai/laion-400-open-dataset/)\n",
        "Colab assembled by [@multimodalart](https://twitter.com/multimodalart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Eqj8CN15S9"
      },
      "outputs": [],
      "source": [
        "Prompt = \"Abstract geometric Chihuahua\" #@param{type:\"string\"}\n",
        "\n",
        "Steps = 100 #@param {type:\"integer\"}\n",
        "\n",
        "# Apparently this parameter can be 0 or 1. Did not completely understand what it does yet.\n",
        "ETA = 1 #@param{type:\"integer\"}\n",
        "\n",
        "Iterations =  1\n",
        "##@param{type:\"integer\"}\n",
        "\n",
        "# How many images to generate in parallel.\n",
        "Samples_in_parallel=4 #@param{type:\"integer\"}\n",
        "\n",
        "# As a rule of thumb, higher values of scale produce better samples at the cost of a reduced output diversity.\n",
        "Diversity_scale=10.0 #@param {type:\"number\"}\n",
        "\n",
        "PLMS_sampling=True #@param {type:\"boolean\"}\n",
        "\n",
        "output_path = \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJk0aESOX7tU"
      },
      "outputs": [],
      "source": [
        "# Rename to hide the poorly working interpolation function\n",
        "Prompts = Prompt\n",
        "\n",
        "# Up to 512x512 should be possible without running out of memory\n",
        "\n",
        "Width=256\n",
        "\n",
        "# Up to 512x512 should be possible without running out of memory\n",
        "Height=256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEVSOJ4f0B21"
      },
      "source": [
        "# Setup stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHgUAp48qwoG"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "!sudo apt install -y aria2\n",
        "!git clone https://github.com/crowsonkb/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "!pip install transformers\n",
        "!pip install open_clip_torch\n",
        "!pip install autokeras\n",
        "!pip install tensorflow\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')\n",
        "from taming.models import vqgan \n",
        "\n",
        "import os\n",
        "os.makedirs(output_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNqCqQDoyZmq"
      },
      "source": [
        "Now, download the checkpoint (~5.7 GB). This will usually take 3-6 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNHvQBhzyXCI"
      },
      "outputs": [],
      "source": [
        "#@title Download model\n",
        "%cd latent-diffusion/ \n",
        "\n",
        "\n",
        "!mkdir -p /content/models/\n",
        "if not os.path.exists(\"/content/models/ldm-model.ckpt\"):\n",
        "  !wget -O /content/models/ldm-model.ckpt https://ipfs.pollinations.ai/ipfs/QmUKTHwT9nWtPp1bes3VGjQRk8gizKvTo6s4cNgsoM1Jpk/model.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxmCePqt1mt"
      },
      "source": [
        "Let's also check what type of GPU we've got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbL2zJ7Pt7Jl"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      },
      "source": [
        "Load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnGwQRhtyBhb"
      },
      "outputs": [],
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnyd-XUKbfE"
      },
      "outputs": [],
      "source": [
        "#@title Import stuff\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import transformers\n",
        "import gc\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from open_clip import tokenizer\n",
        "import open_clip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY_7vvnPThzS"
      },
      "outputs": [],
      "source": [
        "# pomp@title Load necessary functions\n",
        "\n",
        "#NSFW CLIP Filter\n",
        "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\") \n",
        "model = load_model_from_config(config, \"/content/models/ldm-model.ckpt\") \n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NZ6RjwjrdOG"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/steps\n",
        "!mkdir -p /content/steps\n",
        "\n",
        "frame_id = 0\n",
        "def save_img_callback(pred_x0, i):\n",
        "  global frame_id\n",
        "  # print(pred_x0)\n",
        "  x_samples_ddim = model.decode_first_stage(pred_x0)\n",
        "  imgs = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "  print(\"images shape\", imgs.shape)\n",
        "  grid = imgs\n",
        "  #grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "\n",
        "  rows = len(imgs)\n",
        "  \n",
        "  # check if rows is quadratic and if yes take the square root\n",
        "  height = int(rows**0.5)\n",
        "\n",
        "  grid = make_grid(imgs, nrow=height)\n",
        "\n",
        "  # to image\n",
        "  grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "\n",
        "  step_out = os.path.join(\"/content/steps\", f'aaa_{frame_id:04}.png')\n",
        "  Image.fromarray(grid.astype(np.uint8)).save(step_out)\n",
        "\n",
        "  if frame_id % 10 == 0:\n",
        "    progress_out = os.path.join(output_path, \"aaa_progress.png\") \n",
        "    Image.fromarray(grid.astype(np.uint8)).save(progress_out)\n",
        "  frame_id += 1\n",
        "\n",
        "\n",
        "\n",
        "def run(opt):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if opt.plms:\n",
        "        opt.ddim_eta = 0\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "    \n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "\n",
        "    all_samples=list()\n",
        "    samples_ddim, x_samples_ddim = None, None\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            with model.ema_scope():\n",
        "                uc = None\n",
        "                if opt.scale > 0:\n",
        "                    uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "                for prompt in opt.prompts:\n",
        "                    print(prompt)\n",
        "                    for n in range(opt.n_iter):\n",
        "                        c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
        "                        shape = [4, opt.H//8, opt.W//8]\n",
        "                        samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                        conditioning=c,\n",
        "                                                        batch_size=opt.n_samples,\n",
        "                                                        shape=shape,\n",
        "                                                        verbose=False,\n",
        "                                                        img_callback=save_img_callback,\n",
        "                                                        unconditional_guidance_scale=opt.scale,\n",
        "                                                        unconditional_conditioning=uc,\n",
        "                                                        eta=opt.ddim_eta,\n",
        "                                                        x_T=samples_ddim)\n",
        "\n",
        "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        # for x_sample in x_samples_ddim:\n",
        "                        #     x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        #     Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(output_path, f\"aaa_{base_count:04}-{n}.png\"))\n",
        "                        #     base_count += 1\n",
        "                        all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "        # additionally, save as grid\n",
        "        grid = torch.stack(all_samples, 0)\n",
        "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "\n",
        "        rows = opt.n_samples\n",
        "        \n",
        "        # check if rows is quadratic and if yes take the square root\n",
        "        height = int(rows**0.5)\n",
        "\n",
        "        grid = make_grid(grid, nrow=height)\n",
        "\n",
        "        # to image\n",
        "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "        \n",
        "        Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'zzz_{prompt.replace(\" \", \"-\")}.png'))\n",
        "        display(Image.fromarray(grid.astype(np.uint8)))\n",
        "        #print(f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-J8eQDa0Kam"
      },
      "source": [
        "# Do the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-F5eNturTj7"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "import argparse\n",
        "\n",
        "\n",
        "!rm {output_path}/aaa_*.png\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompts = Prompts.split(\"->\"), \n",
        "    outdir=output_path,\n",
        "    ddim_steps = Steps,\n",
        "    ddim_eta = ETA,\n",
        "    n_iter = Iterations,\n",
        "    W=Width,\n",
        "    H=Height,\n",
        "    n_samples=Samples_in_parallel,\n",
        "    scale=Diversity_scale,\n",
        "    plms=PLMS_sampling\n",
        ")\n",
        "run(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVlBfYbpfug6"
      },
      "outputs": [],
      "source": [
        "last_frame=!ls -w1 -t /content/steps/*.png | head -1\n",
        "last_frame = last_frame[0]\n",
        "!cp -v $last_frame /content/steps/aaa_0000.png\n",
        "!cp -v $last_frame /content/steps/aaa_0001.png\n",
        "\n",
        "encoding_options = \"-c:v libx264 -crf 20 -preset slow -vf format=yuv420p -c:a aac -movflags +faststart\"\n",
        "\n",
        "!ffmpeg -y -r 10 -i /content/steps/aaa_%04d.png {encoding_options} {output_path}/zzz_output.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mJ8s2B_CxDj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xEVSOJ4f0B21"
      ],
      "machine_shape": "hm",
      "name": "1 Latent Diffusion LAION-400M model text-to-image",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}