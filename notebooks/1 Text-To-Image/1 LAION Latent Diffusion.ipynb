{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/notebooks/1%20Text-To-Image/1%20LAION%20Latent%20Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "![A painting of a creature that is half octopus and half human](https://i.imgur.com/BxjhCmK.png)\n",
        "\n",
        "Example: *A painting of a creature that is half octopus and half human*\n",
        "\n",
        "Latent Diffusion model Text-to-image synthesis with 1.45B parameter model. This great model rivals OpenAI's DALL-E and can generate beautiful artistic styles as well as faces and text.\n",
        "\n",
        "**Could work with the free mode of Google Colab now**\n",
        "\n",
        "---\n",
        "Credits: model released by [CompVis](https://github.com/CompVis/latent-diffusion) and trained on the [LAION-400M dataset](https://laion.ai/laion-400-open-dataset/)\n",
        "Colab assembled by [@multimodalart](https://twitter.com/multimodalart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Eqj8CN15S9"
      },
      "outputs": [],
      "source": [
        "Prompt = \"A painting of a squirrel eating a hotdog\" #@param{type:\"string\"}\n",
        "\n",
        "Steps = 100 #@param {type:\"integer\"}\n",
        "\n",
        "# Apparently this parameter can be 0 or 1. Did not completely understand what it does yet.\n",
        "ETA = 1 #@param{type:\"integer\"}\n",
        "\n",
        "Iterations = 2 #@param{type:\"integer\"}\n",
        "\n",
        "# How many images to generate in parallel. Two many could lead to memory issues\n",
        "Samples_in_parallel=1 #@param{type:\"integer\"}\n",
        "\n",
        "# As a rule of thumb, higher values of scale produce better samples at the cost of a reduced output diversity.\n",
        "Diversity_scale=10.0 #@param {type:\"number\"}\n",
        "\n",
        "PLMS_sampling=True #@param {type:\"boolean\"}\n",
        "\n",
        "output_path = \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Up to 512x512 should be possible without running out of memory\n",
        "\n",
        "Width=256\n",
        "\n",
        "# Up to 512x512 should be possible without running out of memory\n",
        "Height=256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEVSOJ4f0B21"
      },
      "source": [
        "# Setup stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHgUAp48qwoG",
        "outputId": "c1caccc7-d5a3-4080-d847-8fde77d206d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'latent-diffusion'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (231/231), done.\u001b[K\n",
            "remote: Compressing objects: 100% (155/155), done.\u001b[K\n",
            "remote: Total 231 (delta 80), reused 185 (delta 50), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (231/231), 24.10 MiB | 22.10 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1335, done.\u001b[K\n",
            "remote: Counting objects: 100% (525/525), done.\u001b[K\n",
            "remote: Compressing objects: 100% (493/493), done.\u001b[K\n",
            "remote: Total 1335 (delta 58), reused 481 (delta 30), pack-reused 810\u001b[K\n",
            "Receiving objects: 100% (1335/1335), 412.35 MiB | 28.36 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n",
            "Obtaining file:///content/taming-transformers\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from taming-transformers==0.0.1) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from taming-transformers==0.0.1) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from taming-transformers==0.0.1) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->taming-transformers==0.0.1) (3.10.0.2)\n",
            "Installing collected packages: taming-transformers\n",
            "  Running setup.py develop for taming-transformers\n",
            "Successfully installed taming-transformers-0.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 13.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.4.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "#@title Installation\n",
        "!sudo apt install -y aria2\n",
        "!git clone https://github.com/crowsonkb/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "!pip install transformers\n",
        "!pip install open_clip_torch\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')\n",
        "from taming.models import vqgan \n",
        "\n",
        "import os\n",
        "os.makedirs(output_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNqCqQDoyZmq"
      },
      "source": [
        "Now, download the checkpoint (~5.7 GB). This will usually take 3-6 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNHvQBhzyXCI",
        "outputId": "f241b8af-6dfa-4465-d56a-449adf77f19c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/latent-diffusion\n",
            "--2022-04-05 12:25:56--  https://nftstorage.link/ipfs/QmUKTHwT9nWtPp1bes3VGjQRk8gizKvTo6s4cNgsoM1Jpk/model.ckpt\n",
            "Resolving nftstorage.link (nftstorage.link)... 104.18.7.107, 104.18.6.107, 2606:4700::6812:66b, ...\n",
            "Connecting to nftstorage.link (nftstorage.link)|104.18.7.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bafybeicy26q4fimolgbw6gxrsfoh3p6hzgtab6sylb7a6frevyrbfpjtgu.ipfs.nftstorage.link/model.ckpt [following]\n",
            "--2022-04-05 12:25:56--  https://bafybeicy26q4fimolgbw6gxrsfoh3p6hzgtab6sylb7a6frevyrbfpjtgu.ipfs.nftstorage.link/model.ckpt\n",
            "Resolving bafybeicy26q4fimolgbw6gxrsfoh3p6hzgtab6sylb7a6frevyrbfpjtgu.ipfs.nftstorage.link (bafybeicy26q4fimolgbw6gxrsfoh3p6hzgtab6sylb7a6frevyrbfpjtgu.ipfs.nftstorage.link)... 104.18.6.107, 104.18.7.107, 2606:4700::6812:66b, ...\n",
            "Connecting to bafybeicy26q4fimolgbw6gxrsfoh3p6hzgtab6sylb7a6frevyrbfpjtgu.ipfs.nftstorage.link (bafybeicy26q4fimolgbw6gxrsfoh3p6hzgtab6sylb7a6frevyrbfpjtgu.ipfs.nftstorage.link)|104.18.6.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6152314307 (5.7G) [application/zip]\n",
            "Saving to: ‘models/ldm/text2img-large/model.ckpt’\n",
            "\n",
            "/ldm/text2img-large  16%[==>                 ] 951.00M  4.33MB/s    eta 28m 41s"
          ]
        }
      ],
      "source": [
        "#@title Download model\n",
        "%cd latent-diffusion/ \n",
        "\n",
        "\n",
        "!mkdir -p /content/models/\n",
        "if not os.path.exists(\"/content/models/ldm-model.ckpt\"):\n",
        "  !wget -O /content/models/ldm-model.ckpt https://public-ipfs-gateway.pollinations.ai/ipfs/QmUKTHwT9nWtPp1bes3VGjQRk8gizKvTo6s4cNgsoM1Jpk/model.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxmCePqt1mt"
      },
      "source": [
        "Let's also check what type of GPU we've got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbL2zJ7Pt7Jl"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      },
      "source": [
        "Load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fnGwQRhtyBhb"
      },
      "outputs": [],
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BPnyd-XUKbfE"
      },
      "outputs": [],
      "source": [
        "#@title Import stuff\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm, trange\n",
        "tqdm_auto_model = __import__(\"tqdm.auto\", fromlist=[None]) \n",
        "sys.modules['tqdm'] = tqdm_auto_model\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import transformers\n",
        "import gc\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from open_clip import tokenizer\n",
        "import open_clip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY_7vvnPThzS"
      },
      "outputs": [],
      "source": [
        "#@title Load necessary functions\n",
        "#NSFW CLIP Filter\n",
        "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
        "text = tokenizer.tokenize([\"NSFW\", \"adult content\", \"porn\", \"naked people\",\"genitalia\",\"penis\",\"vagina\"])\n",
        "with torch.no_grad():\n",
        "  text_features = clip_model.encode_text(text)\n",
        "\n",
        "#The higher this number the less the nsfw filter is sensitive\n",
        "nsfw_scale = 18\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\")  # TODO: Optionally download from same location as ckpt and chnage this logic\n",
        "model = load_model_from_config(config, \"/content/models/ldm-model.ckpt\") \n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "def run(opt):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if opt.plms:\n",
        "        opt.ddim_eta = 0\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "    \n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "    outpath = opt.outdir\n",
        "\n",
        "    prompt = opt.prompt\n",
        "\n",
        "\n",
        "    sample_path = os.path.join(outpath, \"samples\")\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "    base_count = len(os.listdir(sample_path))\n",
        "\n",
        "    all_samples=list()\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            with model.ema_scope():\n",
        "                uc = None\n",
        "                if opt.scale > 0:\n",
        "                    uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                    c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
        "                    shape = [4, opt.H//8, opt.W//8]\n",
        "                    samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                    conditioning=c,\n",
        "                                                    batch_size=opt.n_samples,\n",
        "                                                    shape=shape,\n",
        "                                                    verbose=False,\n",
        "                                                    unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,\n",
        "                                                    eta=opt.ddim_eta)\n",
        "\n",
        "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                    x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
        "\n",
        "                    for x_sample in x_samples_ddim:\n",
        "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
        "                        base_count += 1\n",
        "                    all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "    # additionally, save as grid\n",
        "    grid = torch.stack(all_samples, 0)\n",
        "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "    grid = make_grid(grid, nrow=opt.n_samples)\n",
        "\n",
        "    # to image\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    \n",
        "    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.png'))\n",
        "    display(Image.fromarray(grid.astype(np.uint8)))\n",
        "    #print(f\"Your samples are ready and waiting four you here: \\n{outpath} \\nEnjoy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-J8eQDa0Kam"
      },
      "source": [
        "# Do the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmafGmcyT1mZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-F5eNturTj7"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "import argparse\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompt = Prompt, \n",
        "    outdir=output_path,\n",
        "    ddim_steps = Steps,\n",
        "    ddim_eta = ETA,\n",
        "    n_iter = Iterations,\n",
        "    W=Width,\n",
        "    H=Height,\n",
        "    n_samples=Samples_in_parallel,\n",
        "    scale=Diversity_scale,\n",
        "    plms=PLMS_sampling\n",
        ")\n",
        "run(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xEVSOJ4f0B21"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "1 Latent Diffusion LAION-400M model text-to-image",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
