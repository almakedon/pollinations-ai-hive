{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZz-fFxNBCug"
      },
      "source": [
        "* Work in progress. Could still fail without obvious reasons*\n",
        "\n",
        "Generates images from text prompts with CLIP guided diffusion.\n",
        "\n",
        "Based on my previous jax port of Katherine Crowson's CLIP guided diffusion notebook.\n",
        " - [nshepperd's JAX CLIP Guided Diffusion 512x512.ipynb](https://colab.research.google.com/drive/1ZZi1djM8lU4sorkve3bD6EBHiHs6uNAi)\n",
        " - [CLIP Guided Diffusion HQ 512x512.ipynb](https://colab.research.google.com/drive/1V66mUeJbXrTuQITvJunvnWVn96FEbSI3)\n",
        "\n",
        "Added multi-perceptor and pytree ~trickery~ while eliminating the complicated OpenAI gaussian_diffusion classes. Supports both 256x256 and 512x512 OpenAI models (just change the `'image_size': 256` under Model Settings).\n",
        " - Added small secondary model for clip guidance.\n",
        " - Added anti-jpeg model for clearer samples.\n",
        " - Added secondary anti-jpeg classifier.\n",
        " - Added Katherine Crowso's v diffusion models (<https://github.com/crowsonkb/v-diffusion-jax>).\n",
        " - Added pixel art model.\n",
        " ---\n",
        "QOL tweaks by Prof. R.J#1965\n",
        " - Forms, ease of use and cleanup.\n",
        " - Added optional video creation from every iteration of generation.\n",
        " - Enabled nsheppard's implementation of InfoLOOB.\n",
        "\n",
        "Small changes/additions by crung#9999:\n",
        " - Added variable output display rate.\n",
        " - Re-wrote settings forms for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D55n0Z3kMeqJ"
      },
      "outputs": [],
      "source": [
        "#@title # Run Settings { display-mode: \"form\" }\n",
        "\n",
        "#@markdown all_title - Your text prompt.\n",
        "all_title = \"An authentic shaman making sushi by Salvador Dali\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#@markdown How many iterations the image will generate for.\n",
        "steps = 250 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown Select the diffusion model: *Resizing the smaller models is very vram intensive, but you can get away with using lower cuts*\n",
        "#@markdown ##### *Danbooru is especially vram intensive, resSize dimensions are automatically halved for danbooru.*\n",
        "choose_diffusion_model = \"OpenAI | x512\" #@param [\"OpenAI | x512\", \"PixelArtv4 | x256\", \"WikiArt | x256\", \"Danbooru | x128\"]\n",
        "\n",
        "\n",
        "#@markdown n_batches (default: 2) - Controls how many consecutive batches of images are generated.\n",
        "n_batches =  1#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown batch_size (default: 1) - Controls the amount of images generated in parallel.  \n",
        "batch_size = 1 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Image dimensions  \n",
        "#@markdown Larger resolutions cost more VRAM and time. Set to \"Custom\" to enter a custom resolution below.\n",
        "resSize = \"Square | 512x512\" #@param [\"Custom\", \"Square | 512x512\", \"LargeSquare | 768x768\",\"SmallLandscape | 896x512\", \"LargeLandscape | 1280x704\"]\n",
        "\n",
        "\n",
        "#@markdown Custom resolution - each length (in pixels) must be divisible by 64. A resSize other than \"Custom\" overrides these values.\n",
        "imageWidth = 0 #@param {type:\"integer\"}\n",
        "imageHeight = 0 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### cutn settings \n",
        "#@markdown (default: 32) - number of random CLIP cutouts to take from the image.\n",
        "cutn = 32 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown cut_batches (default: 2) - Runs multiple batches of cutouts. Thus, the effective value of cutn is cutn * cut_batches.\n",
        "cut_batches =  2 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Advanced: Affects the size of cutouts. Larger cut_pow -> smaller cutouts (down to the min of 224x244)\n",
        "cut_pow = 1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Init image  \n",
        "#@markdown Diffusion will start with a mixture of this image with noise.\n",
        "image_file = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown starting noise should be between 0 and 1. When using init image, generally 0.5-0.8 is good. Lower starting noise makes the result look more like the init.\n",
        "starting_noise = 1.0 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown MSE loss between the output and the init makes the result look more like the init, (should be between 0 and 100). \n",
        "init_weight_mse = 0 #@param {type:\"integer\"}   \n",
        "\n",
        "output_path = \"/content/output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_SbWTz0jd67"
      },
      "outputs": [],
      "source": [
        "init_image = image_file\n",
        "\n",
        "# scale to maximum 10000\n",
        "init_weight_mse = init_weight_mse * init_weight_mse\n",
        "\n",
        "has_init_image = True if len(image_file) > 0 else False\n",
        "\n",
        "starting_noise = float(starting_noise)\n",
        "\n",
        "googleDrive = False #@param {type:\"boolean\"}\n",
        "modelsOnDrive = False #@param {type:\"boolean\"}\n",
        "#@markdown Images will be stored in the subfolder \"nshepv2\", followed by the diffusion model used and finally into a subfolder with the current date (YYYY-MM).\n",
        "\n",
        "outputFolder = output_path\n",
        "outputFolderStatic = outputFolder\n",
        "\n",
        "!rm -r /tmp/ffmpeg\n",
        "!rm -r /content/frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zKX4uWFBks2"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson; nshepperd\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKZYWJt087dj"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYiq47LEeboR"
      },
      "source": [
        "# Set things up & Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ1HpnuQHOpU"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi | grep A100 && pip install https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.71+cuda111-cp37-none-manylinux2010_x86_64.whl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZS4uQYE9BXf"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install tensorflow==1.15.2\n",
        "!pip install dm-haiku cbor2 ftfy einops\n",
        "!git clone https://github.com/kingoflolz/CLIP_JAX\n",
        "!git clone https://github.com/nshepperd/jax-guided-diffusion -b v2\n",
        "!git clone https://github.com/crowsonkb/v-diffusion-jax "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erek6UjoqR7O"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./CLIP_JAX')\n",
        "sys.path.append('./jax-guided-diffusion')\n",
        "sys.path.append('./v-diffusion-jax')\n",
        "\n",
        "import math\n",
        "import io\n",
        "import time\n",
        "import os\n",
        "import functools\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "import weakref\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.scipy as jsp\n",
        "import jaxtorch\n",
        "from jaxtorch import PRNG, Context, Module, nn, init\n",
        "from tqdm import tqdm\n",
        "\n",
        "import clip_jax\n",
        "import diffusion as v_diffusion\n",
        "\n",
        "from lib.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from lib import util\n",
        "\n",
        "from IPython import display\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.transforms import functional as TF\n",
        "import torch.utils.data\n",
        "import torch\n",
        "\n",
        "import shutil\n",
        "import imageio\n",
        "from subprocess import Popen, PIPE\n",
        "import os.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTWBAsCCqtIz"
      },
      "outputs": [],
      "source": [
        "devices = jax.devices()\n",
        "n_devices = len(devices)\n",
        "print('Using device:', devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GLI2tEr0AUQ"
      },
      "outputs": [],
      "source": [
        "# Drive location for caching model parameters\n",
        "if modelsOnDrive:\n",
        "    model_location = '/content/drive/MyDrive/models'\n",
        "else:\n",
        "    model_location = '/content/models'\n",
        "os.makedirs(model_location, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVrjUvv0N-9K"
      },
      "outputs": [],
      "source": [
        "# Implement lazy loading and caching of model parameters for all the different models.\n",
        "\n",
        "class WeakKey(object):\n",
        "  \"\"\"Weak pointer equality based keys for hashable dicts. Does not keep x alive.\"\"\"\n",
        "  def __init__(self, x):\n",
        "    self.id = id(x)\n",
        "    self.weak = weakref.ref(x)\n",
        "  def __hash__(self):\n",
        "    return hash(self.id)\n",
        "  def __eq__(self, other):\n",
        "    a = self.weak()\n",
        "    b = other.weak()\n",
        "    return self.id == other.id and (a is b)\n",
        "\n",
        "class WeakCache(object):\n",
        "  \"\"\"A cache using weak references so values are cached only as long as they are referenced from elsewhere.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.cache = {}\n",
        "\n",
        "  def lookup(self, f, x):\n",
        "    \"\"\"Look up the cached value of f(x).\"\"\"\n",
        "    key = WeakKey(x)\n",
        "    if key in self.cache:\n",
        "      val = self.cache[key]()\n",
        "      if val is not None:\n",
        "        return val\n",
        "    val = f(x)\n",
        "    self.cache[key] = weakref.ref(val)\n",
        "    return val\n",
        "\n",
        "gpu_cache = WeakCache()\n",
        "\n",
        "def to_gpu(params):\n",
        "  \"\"\"Convert a pytree of params to jax, using cached arrays if they are still alive.\"\"\"\n",
        "  return jax.tree_util.tree_map(lambda x: gpu_cache.lookup(jnp.array,x) if type(x) is np.ndarray else x, params)\n",
        "\n",
        "# @jax.tree_util.register_pytree_node_class\n",
        "class LazyParams(object):\n",
        "  \"\"\"Lazily download parameters and load onto gpu. Parameters are kept in cpu memory and only loaded to gpu as long as needed.\"\"\"\n",
        "  def __init__(self, load):\n",
        "    self.load = load\n",
        "    self.params = None\n",
        "  @staticmethod\n",
        "  def pt(url, key=None):\n",
        "    def load():\n",
        "      params = jaxtorch.pt.load(fetch_model(url))\n",
        "      if key is not None:\n",
        "        return params[key]\n",
        "      else:\n",
        "        return params\n",
        "    return LazyParams(load)\n",
        "  def __call__(self):\n",
        "    if self.params is None:\n",
        "      self.params = jax.tree_util.tree_map(np.array, self.load())\n",
        "    return to_gpu(self.params)\n",
        "\n",
        "  def tree_flatten(self):\n",
        "      return [self()], []\n",
        "  def tree_unflatten(static, dynamic):\n",
        "      return dynamic[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz_wPYY3qbax"
      },
      "outputs": [],
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    local_path = os.path.join(model_location, basename)\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    else:\n",
        "        !curl '{url_or_path}' -o '{local_path}'\n",
        "        return local_path\n",
        "\n",
        "\n",
        "def grey(image):\n",
        "    [*_, c, h, w] = image.shape\n",
        "    return jnp.broadcast_to(image.mean(axis=-3, keepdims=True), image.shape)\n",
        "\n",
        "def cutout_image(image, offsetx, offsety, size, output_size=224):\n",
        "    \"\"\"Computes (square) cutouts of an image given x and y offsets and size.\"\"\"\n",
        "    (c, h, w) = image.shape\n",
        "\n",
        "    scale = jnp.stack([output_size / size, output_size / size])\n",
        "    translation = jnp.stack([-offsety * output_size / size, -offsetx * output_size / size])\n",
        "    return jax.image.scale_and_translate(image,\n",
        "                                         shape=(c, output_size, output_size),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def cutouts_images(image, offsetx, offsety, size, output_size=224):\n",
        "    f = partial(cutout_image, output_size=output_size)         # [c h w] [] [] [] -> [c h w]\n",
        "    f = jax.vmap(f, in_axes=(0, None, None, None), out_axes=0) # [n c h w] [] [] [] -> [n c h w]\n",
        "    f = jax.vmap(f, in_axes=(None, 0, 0, 0), out_axes=0)       # [n c h w] [k] [k] [k] -> [k n c h w]\n",
        "    return f(image, offsetx, offsety, size)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1., p_grey=0.2, p_mixgrey=0.0):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.p_grey = p_grey\n",
        "        self.p_mixgrey = p_mixgrey\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [b, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[self.cutn//2])**self.cut_pow\n",
        "        sizes = (min_size + cut_us * (max_size - min_size + 1)).astype(jnp.int32).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.uniform(rng.split(), [self.cutn//2], minval=0, maxval=w - sizes)\n",
        "        offsets_y = jax.random.uniform(rng.split(), [self.cutn//2], minval=0, maxval=h - sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "\n",
        "        B1 = 40\n",
        "        B2 = 40\n",
        "        lcut_us = jax.random.uniform(rng.split(), shape=[self.cutn//2])\n",
        "        border = B1 + lcut_us * B2\n",
        "        lsizes = (max(h,w) + border).astype(jnp.int32)\n",
        "        loffsets_x = jax.random.uniform(rng.split(), [self.cutn//2], minval=w/2-lsizes/2-border, maxval=w/2-lsizes/2+border)\n",
        "        loffsets_y = jax.random.uniform(rng.split(), [self.cutn//2], minval=h/2-lsizes/2-border, maxval=h/2-lsizes/2+border)\n",
        "        lcutouts = cutouts_images(input, loffsets_x, loffsets_y, lsizes)\n",
        "\n",
        "        cutouts = jnp.concatenate([cutouts, lcutouts], axis=0)\n",
        "\n",
        "        greyed = grey(cutouts)\n",
        "\n",
        "        grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        grey_rs = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        cutouts = jnp.where(grey_us < self.p_mixgrey, grey_rs * greyed + (1 - grey_rs) * cutouts, cutouts)\n",
        "\n",
        "        grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        cutouts = jnp.where(grey_us < self.p_grey, greyed, cutouts)\n",
        "        return cutouts\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.p_grey, self.cut_pow, self.p_mixgrey], (self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        (p_grey, cut_pow, p_mixgrey) = dynamic\n",
        "        return MakeCutouts(cut_size, cutn, cut_pow, p_grey, p_mixgrey)\n",
        "\n",
        "def Normalize(mean, std):\n",
        "    mean = jnp.array(mean).reshape(3,1,1)\n",
        "    std = jnp.array(std).reshape(3,1,1)\n",
        "    def forward(image):\n",
        "        return (image - mean) / std\n",
        "    return forward\n",
        "\n",
        "def norm1(x):\n",
        "    \"\"\"Normalize to the unit sphere.\"\"\"\n",
        "    return x / x.square().sum(axis=-1, keepdims=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = norm1(x)\n",
        "    y = norm1(y)\n",
        "    return (x - y).square().sum(axis=-1).sqrt().div(2).arcsin().square().mul(2)\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    x_diff = input[..., :, 1:] - input[..., :, :-1]\n",
        "    y_diff = input[..., 1:, :] - input[..., :-1, :]\n",
        "    return x_diff.square().mean([1,2,3]) + y_diff.square().mean([1,2,3])\n",
        "\n",
        "def downscale2d(image, f):\n",
        "  [c, n, h, w] = image.shape\n",
        "  return jax.image.resize(image, [c, n, h//f, w//f], method='cubic')\n",
        "\n",
        "def upscale2d(image, f):\n",
        "  [c, n, h, w] = image.shape\n",
        "  return jax.image.resize(image, [c, n, h*f, w*f], method='cubic')\n",
        "\n",
        "def gaussian_blur(image, sigma, radius):\n",
        "    if len(image.shape) == 4:\n",
        "      [n, c, h, w] = image.shape\n",
        "      return gaussian_blur(image.reshape([n*c,h,w]), sigma, radius).reshape(image.shape)\n",
        "    # image : [c, h, w]\n",
        "    kernel_size = radius * 2 + 1\n",
        "    kernel_1d = jsp.stats.norm.pdf(jnp.linspace(-radius / sigma, radius / sigma, kernel_size))\n",
        "    kernel = (kernel_1d[:, None] @ kernel_1d[None, :])[None]\n",
        "    kernel = kernel / jnp.sum(kernel)\n",
        "    return jsp.signal.convolve(image, kernel, 'same')\n",
        "\n",
        "@dataclass\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class DiffusionOutput:\n",
        "    v: torch.Tensor\n",
        "    pred: torch.Tensor\n",
        "    eps: torch.Tensor\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return [self.v, self.pred, self.eps], []\n",
        "\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        return cls(*dynamic)\n",
        "  \n",
        "# Noise schedule\n",
        "\n",
        "def alpha_sigma_to_t(alpha, sigma):\n",
        "    return jnp.arctan2(sigma, alpha) * 2 / math.pi\n",
        "\n",
        "def cosine_t_to_ddpm(t):\n",
        "    alpha, sigma = get_cosine_alphas_sigmas(t)\n",
        "    log_snr = jnp.log(alpha**2 / sigma**2)\n",
        "    return ((jnp.log1p(jnp.exp(-log_snr)) - 1e-4) / 10).clamp(0,1).sqrt()\n",
        "\n",
        "def get_ddpm_alphas_sigmas(t):\n",
        "    log_snrs = -jnp.expm1(1e-4 + 10 * t**2).log()\n",
        "    alphas_squared = jax.nn.sigmoid(log_snrs)\n",
        "    sigmas_squared = jax.nn.sigmoid(-log_snrs)\n",
        "    return alphas_squared.sqrt(), sigmas_squared.sqrt()\n",
        "\n",
        "def get_cosine_alphas_sigmas(t):\n",
        "    return jnp.cos(t * math.pi/2), jnp.sin(t * math.pi/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsLm7ElGc6nQ"
      },
      "outputs": [],
      "source": [
        "# Define combinators.\n",
        "\n",
        "# These (ab)use the jax pytree registration system to define parameterised\n",
        "# objects for doing various things, which are compatible with jax.jit.\n",
        "\n",
        "# For jit compatibility an object needs to act as a pytree, which means implementing two methods:\n",
        "#  - tree_flatten(self): returns two lists of the object's fields: \n",
        "#       1. 'dynamic' parameters: things which can be jax tensors, or other pytrees\n",
        "#       2. 'static' parameters: arbitrary python objects, will trigger recompilation when changed\n",
        "#  - tree_unflatten(static, dynamic): reconstitutes the object from its parts\n",
        "\n",
        "# With these tricks, you can simply define your cond_fn as an object, as is done\n",
        "# below, and pass it into the jitted sample step as a regular argument. JAX will\n",
        "# handle recompiling the jitted code whenever a control-flow affecting parameter\n",
        "# is changed (such as cut_batches).\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CosineModel(object):\n",
        "    def __init__(self, model, params, **kwargs):\n",
        "      if isinstance(params, LazyParams):\n",
        "        params = params()\n",
        "      self.model = model\n",
        "      self.params = params\n",
        "      self.kwargs = kwargs\n",
        "    @jax.jit\n",
        "    def __call__(self, x, t, key):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
        "        cx = Context(self.params, key).eval_mode_()\n",
        "        return self.model(cx, x, cosine_t.broadcast_to([n]), **self.kwargs)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params, self.kwargs], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params, kwargs] = dynamic\n",
        "        [model] = static\n",
        "        return CosineModel(model, params, **kwargs)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class OpenaiModel(object):\n",
        "    def __init__(self, model, params):\n",
        "      if isinstance(params, LazyParams):\n",
        "        params = params()\n",
        "      self.model = model\n",
        "      self.params = params\n",
        "    @jax.jit\n",
        "    def __call__(self, x, t, key):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cx = Context(self.params, key).eval_mode_()\n",
        "        openai_t = (t * 1000).broadcast_to([n])\n",
        "        eps = self.model(cx, x, openai_t)[:, :3, :, :]\n",
        "        pred = (x - eps * sigma) / alpha\n",
        "        v    = (eps - x * sigma) / alpha\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params] = dynamic\n",
        "        [model] = static\n",
        "        return OpenaiModel(model, params)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class Perceptor(object):\n",
        "    # Wraps a CLIP instance and its parameters.\n",
        "    def __init__(self, image_fn, text_fn, clip_params):\n",
        "        self.image_fn = image_fn\n",
        "        self.text_fn = text_fn\n",
        "        self.clip_params = clip_params\n",
        "    @jax.jit\n",
        "    def embed_cutouts(self, cutouts):\n",
        "        return norm1(self.image_fn(self.clip_params, cutouts))\n",
        "    def embed_text(self, text):\n",
        "        tokens = clip_jax.tokenize([text])\n",
        "        text_embed = self.text_fn(self.clip_params, tokens)\n",
        "        return norm1(text_embed.reshape(512))\n",
        "    def embed_texts(self, texts):\n",
        "        return jnp.stack([self.embed_text(t) for t in texts])\n",
        "    def tree_flatten(self):\n",
        "        return [self.clip_params], [self.image_fn, self.text_fn]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [clip_params] = dynamic\n",
        "        [image_fn, text_fn] = static\n",
        "        return Perceptor(image_fn, text_fn, clip_params)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class LerpModels(object):\n",
        "    \"\"\"Linear combination of diffusion models.\"\"\"\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "    def __call__(self, x, t, key):\n",
        "        outputs = [m(x,t,key) for (m,w) in self.models]\n",
        "        v = sum(out.v * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        pred = sum(out.pred * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        eps = sum(out.eps * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.models], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return LerpModels(*dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class KatModel(object):\n",
        "    def __init__(self, model, params, **kwargs):\n",
        "      if isinstance(params, LazyParams):\n",
        "        params = params()\n",
        "      self.model = model\n",
        "      self.params = params\n",
        "      self.kwargs = kwargs\n",
        "    @jax.jit\n",
        "    def __call__(self, x, t, key):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
        "        v = self.model.apply(self.params, key, x, cosine_t.broadcast_to([n]), self.kwargs)\n",
        "        pred = x * alpha - v * sigma\n",
        "        eps = x * sigma + v * alpha\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params, self.kwargs], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params, kwargs] = dynamic\n",
        "        [model] = static\n",
        "        return KatModel(model, params, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnpzHXfMFpCI"
      },
      "outputs": [],
      "source": [
        "# Common nn modules.\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, cx, input):\n",
        "        return jnp.concatenate([self.main(cx, input), self.skip(cx, input)], axis=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = init.normal(out_features // 2, in_features, stddev=std)\n",
        "\n",
        "    def forward(self, cx, input):\n",
        "        f = 2 * math.pi * input @ cx[self.weight].transpose()\n",
        "        return jnp.concatenate([f.cos(), f.sin()], axis=-1)\n",
        "\n",
        "\n",
        "class AvgPool2d(nn.Module):\n",
        "    def forward(self, cx, x):\n",
        "        [n, c, h, w] = x.shape\n",
        "        x = x.reshape([n, c, h//2, 2, w//2, 2])\n",
        "        x = x.mean((3,5))\n",
        "        return x\n",
        "\n",
        "\n",
        "def expand_to_planes(input, shape):\n",
        "    return input[..., None, None].broadcast_to(list(input.shape) + [shape[2], shape[3]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlN36wo_TaFC"
      },
      "outputs": [],
      "source": [
        "# Secondary Model \n",
        "class ConvBlock(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "class SecondaryDiffusionImageNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, c),\n",
        "            ConvBlock(c, c),\n",
        "            SkipBlock([\n",
        "                AvgPool2d(),\n",
        "                # nn.image.Downsample2d('linear'),\n",
        "                ConvBlock(c, c * 2),\n",
        "                ConvBlock(c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    AvgPool2d(),\n",
        "                    # nn.image.Downsample2d('linear'),\n",
        "                    ConvBlock(c * 2, c * 4),\n",
        "                    ConvBlock(c * 4, c * 4),\n",
        "                    SkipBlock([\n",
        "                        AvgPool2d(),\n",
        "                        # nn.image.Downsample2d('linear'),\n",
        "                        ConvBlock(c * 4, c * 8),\n",
        "                        ConvBlock(c * 8, c * 4),\n",
        "                        nn.image.Upsample2d('linear'),\n",
        "                    ]),\n",
        "                    ConvBlock(c * 8, c * 4),\n",
        "                    ConvBlock(c * 4, c * 2),\n",
        "                    nn.image.Upsample2d('linear'),\n",
        "                ]),\n",
        "                ConvBlock(c * 4, c * 2),\n",
        "                ConvBlock(c * 2, c),\n",
        "                nn.image.Upsample2d('linear'),\n",
        "            ]),\n",
        "            ConvBlock(c * 2, c),\n",
        "            nn.Conv2d(c, 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, t[:, None]), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(t)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "class SecondaryDiffusionImageNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.down = AvgPool2d()\n",
        "        self.up = nn.image.Upsample2d('linear')\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, cs[0]),\n",
        "            ConvBlock(cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                self.down,\n",
        "                ConvBlock(cs[0], cs[1]),\n",
        "                ConvBlock(cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    self.down,\n",
        "                    ConvBlock(cs[1], cs[2]),\n",
        "                    ConvBlock(cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        self.down,\n",
        "                        ConvBlock(cs[2], cs[3]),\n",
        "                        ConvBlock(cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            self.down,\n",
        "                            ConvBlock(cs[3], cs[4]),\n",
        "                            ConvBlock(cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                self.down,\n",
        "                                ConvBlock(cs[4], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[4]),\n",
        "                                self.up,\n",
        "                            ]),\n",
        "                            ConvBlock(cs[4] * 2, cs[4]),\n",
        "                            ConvBlock(cs[4], cs[3]),\n",
        "                            self.up,\n",
        "                        ]),\n",
        "                        ConvBlock(cs[3] * 2, cs[3]),\n",
        "                        ConvBlock(cs[3], cs[2]),\n",
        "                        self.up,\n",
        "                    ]),\n",
        "                    ConvBlock(cs[2] * 2, cs[2]),\n",
        "                    ConvBlock(cs[2], cs[1]),\n",
        "                    self.up,\n",
        "                ]),\n",
        "                ConvBlock(cs[1] * 2, cs[1]),\n",
        "                ConvBlock(cs[1], cs[0]),\n",
        "                self.up,\n",
        "            ]),\n",
        "            ConvBlock(cs[0] * 2, cs[0]),\n",
        "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, t[:, None]), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(t)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "secondary1_model = SecondaryDiffusionImageNet()\n",
        "secondary1_params = secondary1_model.init_weights(jax.random.PRNGKey(0))\n",
        "secondary1_params = LazyParams.pt('https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy/secondary_model_imagenet.pth')\n",
        "\n",
        "secondary2_model = SecondaryDiffusionImageNet2()\n",
        "secondary2_params = secondary2_model.init_weights(jax.random.PRNGKey(0))\n",
        "secondary2_params = LazyParams.pt('https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy/secondary_model_imagenet_2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ThgwVPtXIH5"
      },
      "outputs": [],
      "source": [
        "# Anti-JPEG model\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, cx, input):\n",
        "        return self.main(cx, input) + self.skip(cx, input)\n",
        "\n",
        "\n",
        "class ResConvBlock(ResidualBlock):\n",
        "    def __init__(self, c_in, c_mid, c_out, dropout=True):\n",
        "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "        super().__init__([\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "        ], skip)\n",
        "        \n",
        "\n",
        "CHANNELS=64\n",
        "class JPEGModel(nn.Module):\n",
        "    def __init__(self, c=CHANNELS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16, std=1.0)\n",
        "        self.class_embed = nn.Embedding(3, 16)\n",
        "\n",
        "        self.arch = '11(22(22(2)22)22)11'\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3 + 16 + 16, c, 1),\n",
        "            ResConvBlock(c, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "            SkipBlock([\n",
        "                nn.image.Downsample2d(),\n",
        "                ResConvBlock(c,     c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    nn.image.Downsample2d(),\n",
        "                    ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                    ResConvBlock(c * 2, 2 * 2, c * 2),\n",
        "                    SkipBlock([\n",
        "                        nn.image.Downsample2d(),\n",
        "                        ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                        nn.image.Upsample2d(),\n",
        "                    ]),\n",
        "                    ResConvBlock(c * 4, c * 2, c * 2),\n",
        "                    ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                    nn.image.Upsample2d(),\n",
        "                ]),\n",
        "                ResConvBlock(c * 4, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c),\n",
        "                nn.image.Upsample2d(),\n",
        "            ]),\n",
        "            ResConvBlock(c * 2, c, c),\n",
        "            ResConvBlock(c, c, 3, dropout=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, ts, cond):\n",
        "        [n, c, h, w] = input.shape\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, ts[:, None]), input.shape)\n",
        "        class_embed = expand_to_planes(self.class_embed(cx, cond), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed, class_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(ts)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "jpeg_model = JPEGModel()\n",
        "jpeg_params = jpeg_model.init_weights(jax.random.PRNGKey(0))\n",
        "jpeg_params = LazyParams.pt('https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy/jpeg-db-oi-614.pt', key='params_ema')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IQNdD-UXjw_"
      },
      "outputs": [],
      "source": [
        "# Secondary Anti-JPEG Classifier\n",
        "\n",
        "CHANNELS=64\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, c=CHANNELS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16, std=1.0)\n",
        "\n",
        "        self.arch = '11-22-22-22'\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3 + 16, c, 1),\n",
        "            ResConvBlock(c, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "            nn.image.Downsample2d(),\n",
        "            ResConvBlock(c,     c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            nn.image.Downsample2d(),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, 2 * 2, c * 2),\n",
        "            nn.image.Downsample2d(),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, 1, dropout=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, ts):\n",
        "        [n, c, h, w] = input.shape\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, ts[:, None]), input.shape)\n",
        "        return self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "\n",
        "    def score(self, cx, reals, ts, cond, flood_level, blur_size):\n",
        "        cond = cond[:, None, None, None]\n",
        "        logits = self.forward(cx, reals, ts)\n",
        "        logits = gaussian_blur(logits, blur_size, 6)\n",
        "        loss = -jax.nn.log_sigmoid(jnp.where(cond==0, logits, -logits))\n",
        "        loss = loss.clamp(minval=flood_level, maxval=None)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def classifier_probs(classifier_params, x, ts):\n",
        "  n = x.shape[0]\n",
        "  cx = Context(classifier_params, jax.random.PRNGKey(0)).eval_mode_()\n",
        "  probs = jax.nn.sigmoid(classifier_model(cx, x, ts.broadcast_to([n])))\n",
        "  return probs\n",
        "\n",
        "classifier_model = Classifier()\n",
        "classifier_params = classifier_model.init_weights(jax.random.PRNGKey(0))\n",
        "classifier_params = LazyParams.pt('https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy/jpeg-classifier-72.pt', 'params_ema')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHqocqbM_NzV"
      },
      "outputs": [],
      "source": [
        "# Pixel art model\n",
        "\n",
        "CHANNELS=192\n",
        "class PixelArtV4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = CHANNELS  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16, std=1.0)\n",
        "\n",
        "        self.arch = '122222'\n",
        "\n",
        "        muls = [1, 2, 2, 2, 2, 2]\n",
        "        cs = [CHANNELS * m for m in muls]\n",
        "\n",
        "        def downsample(c1, c2):\n",
        "            return nn.Sequential(nn.image.Downsample2d(), nn.Conv2d(c1, c2, 1) if c1!=c2 else nn.Identity())\n",
        "\n",
        "        def upsample(c1, c2):\n",
        "            return nn.Sequential(nn.Conv2d(c1, c2, 1) if c1!=c2 else nn.Identity(), nn.image.Upsample2d())\n",
        "\n",
        "        class ResConvBlock(ResidualBlock):\n",
        "            def __init__(self, c_in, c_mid, c_out, dropout=True):\n",
        "                skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "                super().__init__([\n",
        "                    nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "                    nn.Dropout2d(p=0.1) if dropout else nn.Identity(),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "                    nn.Dropout2d(p=0.1) if dropout else nn.Identity(),\n",
        "                    nn.ReLU() if dropout else nn.Identity(),\n",
        "                ], skip)\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ResConvBlock(3 + 16, cs[0], cs[0]),\n",
        "            ResConvBlock(cs[0], cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                downsample(cs[0], cs[1]), # 2x2\n",
        "                ResConvBlock(cs[1], cs[1], cs[1]),\n",
        "                ResConvBlock(cs[1], cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    downsample(cs[1], cs[2]),  # 4x4\n",
        "                    ResConvBlock(cs[2], cs[2], cs[2]),\n",
        "                    ResConvBlock(cs[2], cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        downsample(cs[2], cs[3]),  # 8x8\n",
        "                        ResConvBlock(cs[3], cs[3], cs[3]),\n",
        "                        ResConvBlock(cs[3], cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            downsample(cs[3], cs[4]),  # 16x16\n",
        "                            ResConvBlock(cs[4], cs[4], cs[4]),\n",
        "                            ResConvBlock(cs[4], cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                downsample(cs[4], cs[5]),  # 32x32\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                upsample(cs[5],cs[4]),\n",
        "                            ]),\n",
        "                            ResConvBlock(cs[4]*2, cs[4], cs[4]),\n",
        "                            ResConvBlock(cs[4], cs[4], cs[4]),\n",
        "                            upsample(cs[4],cs[3]),\n",
        "                        ]),\n",
        "                        ResConvBlock(cs[3]*2, cs[3], cs[3]),\n",
        "                        ResConvBlock(cs[3], cs[3], cs[3]),\n",
        "                        upsample(cs[3],cs[2]),\n",
        "                    ]),\n",
        "                    ResConvBlock(cs[2]*2, cs[2], cs[2]),\n",
        "                    ResConvBlock(cs[2], cs[2], cs[2]),\n",
        "                    upsample(cs[2],cs[1]),\n",
        "                ]),\n",
        "                ResConvBlock(cs[1]*2, cs[1], cs[1]),\n",
        "                ResConvBlock(cs[1], cs[1], cs[1]),\n",
        "                upsample(cs[1],cs[0]),\n",
        "            ]),\n",
        "            ResConvBlock(cs[0]*2, cs[0], cs[0]),\n",
        "            ResConvBlock(cs[0], cs[0], 3, dropout=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, t[:, None]), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(t)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "pixelartv4_model = PixelArtV4()\n",
        "pixelartv4_params = pixelartv4_model.init_weights(jax.random.PRNGKey(0))\n",
        "\n",
        "# There are many checkpoints supported with this model\n",
        "pixelartv4_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_34.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_63.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_150.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_50.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_65.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_97.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_173.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_344.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_432.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_600.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_700.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_800.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_1000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_2000.pt'\n",
        "    'https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy/pixelart-fgood_3000.pt',\n",
        "    key='params_ema'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e-9dz8u_3Ep"
      },
      "outputs": [],
      "source": [
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutoutsPixelated(object):\n",
        "    \"\"\"Used for pixel art model - nearest upscale by 4x before taking cutouts to present a more pixel-arty pred to CLIP.\"\"\"\n",
        "    def __init__(self, make_cutouts, factor=4):\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.factor = factor\n",
        "        self.cutn = make_cutouts.cutn\n",
        "    \n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        input = jax.image.resize(input, [n, c, h*self.factor, w * self.factor], method='nearest')\n",
        "        return self.make_cutouts(input, key)\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.make_cutouts], [self.factor])\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MakeCutoutsPixelated(*dynamic, *static)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M67CgKc1fDd"
      },
      "outputs": [],
      "source": [
        "# Kat models\n",
        "danbooru_128_model = v_diffusion.get_model('danbooru_128')\n",
        "danbooru_128_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy//danbooru_128.pkl')))\n",
        "\n",
        "wikiart_256_model = v_diffusion.get_model('wikiart_256')\n",
        "wikiart_256_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy//wikiart_256.pkl')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiorqmdkGQo1"
      },
      "source": [
        "Model Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ4ozP3aqhAV"
      },
      "outputs": [],
      "source": [
        "use_checkpoint = False # Set to True to save some memory\n",
        "\n",
        "model_urls = {\n",
        "    512: 'https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy/512x512_diffusion_uncond_finetune_008100.pt',\n",
        "    256: 'https://pollinations.ai/ipfs/QmS9wbZNq4bngZBCdZgyzdgqpvXp8LqjrBkbZy94LsAVcy/256x256_diffusion_uncond.pt'\n",
        "}\n",
        "\n",
        "# Load models, both 256 and 512\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '1000',\n",
        "    'image_size': 512,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "    'use_checkpoint': use_checkpoint \n",
        "})\n",
        "\n",
        "\n",
        "openai_512_model, _ = create_model_and_diffusion(**model_config)\n",
        "openai_512_params = openai_512_model.init_weights(jax.random.PRNGKey(0))\n",
        "openai_512_params = LazyParams.pt(model_urls[512])\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '1000',\n",
        "    'image_size': 256,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "    'use_checkpoint': use_checkpoint \n",
        "})\n",
        "\n",
        "openai_256_model, _ = create_model_and_diffusion(**model_config)\n",
        "openai_256_params = openai_256_model.init_weights(jax.random.PRNGKey(0))\n",
        "openai_256_params = LazyParams.pt(model_urls[256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXQ2Di_LRM46"
      },
      "outputs": [],
      "source": [
        "# Losses and cond fn.\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondCLIP(object):\n",
        "    # CLIP guidance loss. Pushes the image toward a text prompt.\n",
        "    def __init__(self, text_embed, clip_guidance_scale, perceptor, make_cutouts, cut_batches):\n",
        "        self.text_embed = text_embed\n",
        "        self.clip_guidance_scale = clip_guidance_scale\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.cut_batches = cut_batches\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts).reshape([self.make_cutouts.cutn, n, 512])\n",
        "            losses = spherical_dist_loss(image_embeds, self.text_embed).mean(0)\n",
        "            return losses.sum() * self.clip_guidance_scale\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.text_embed, self.clip_guidance_scale, self.perceptor, self.make_cutouts], [self.cut_batches]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [text_embed, clip_guidance_scale, perceptor, make_cutouts] = dynamic\n",
        "        [cut_batches] = static\n",
        "        return CondCLIP(text_embed, clip_guidance_scale, perceptor, make_cutouts, cut_batches)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class InfoLOOB(object):\n",
        "    # CLIP guidance loss. Pushes the image toward a text prompt.\n",
        "    def __init__(self, text_embed, clip_guidance_scale, perceptor, make_cutouts, lm, cut_batches):\n",
        "        self.text_embed = text_embed\n",
        "        self.clip_guidance_scale = clip_guidance_scale\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.lm = lm\n",
        "        self.cut_batches = cut_batches\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts).reshape([self.make_cutouts.cutn, n, 512])\n",
        "                  \n",
        "            all_image_embeds = norm1(image_embeds.mean(0))\n",
        "            all_text_embeds = norm1(self.text_embed)\n",
        "            sim_matrix = jnp.einsum('nc,mc->nm', all_image_embeds, all_text_embeds)\n",
        "\n",
        "            x = 1\n",
        "            xn = sim_matrix.shape[0]\n",
        "            def loob(sim_matrix):\n",
        "              diag = jnp.eye(xn) * sim_matrix\n",
        "              off_diag = (1 - jnp.eye(xn))*sim_matrix\n",
        "              return -diag.sum() + self.lm * off_diag.exp().sum(axis=-1).log().sum()\n",
        "            losses = (loob(sim_matrix) + loob(sim_matrix.transpose())) / x\n",
        "            return losses.sum() * self.clip_guidance_scale\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return [self.text_embed, self.clip_guidance_scale, self.perceptor, self.make_cutouts, self.lm], [self.cut_batches]\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        return cls(*dynamic, *static)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondTV(object):\n",
        "    # Multiscale Total Variation loss. Tries to smooth out the image.\n",
        "    def __init__(self, tv_scale):\n",
        "        self.tv_scale = tv_scale\n",
        "    def __call__(self, x_in, key):\n",
        "        def sum_tv_loss(x_in, f=None):\n",
        "            if f is not None:\n",
        "                x_in = downscale2d(x_in, f)\n",
        "            return tv_loss(x_in).sum() * self.tv_scale\n",
        "        tv_grad_512 = jax.grad(sum_tv_loss)(x_in)\n",
        "        tv_grad_256 = jax.grad(partial(sum_tv_loss,f=2))(x_in)\n",
        "        tv_grad_128 = jax.grad(partial(sum_tv_loss,f=4))(x_in)\n",
        "        return tv_grad_512 + tv_grad_256 + tv_grad_128        \n",
        "    def tree_flatten(self):\n",
        "        return [self.tv_scale], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return CondTV(*dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondSat(object):\n",
        "    # Saturation loss. Tries to prevent the image from going out of range.\n",
        "    def __init__(self, sat_scale):\n",
        "        self.sat_scale = sat_scale\n",
        "    def __call__(self, x_in, key):\n",
        "        def saturation_loss(x_in):\n",
        "            return jnp.abs(x_in - x_in.clamp(minval=-1,maxval=1)).mean()\n",
        "        return self.sat_scale * jax.grad(saturation_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.sat_scale], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return CondSat(*dynamic)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondMSE(object):\n",
        "    # MSE loss. Targets the output towards an image.\n",
        "    def __init__(self, target, mse_scale):\n",
        "        self.target = target\n",
        "        self.mse_scale = mse_scale\n",
        "    def __call__(self, x_in, key):\n",
        "        def mse_loss(x_in):\n",
        "            return (x_in - self.target).square().mean()\n",
        "        return self.mse_scale * jax.grad(mse_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.target, self.mse_scale], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return CondMSE(*dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MaskedMSE(object):\n",
        "    # MSE loss. Targets the output towards an image.\n",
        "    def __init__(self, target, mse_scale, mask, grey=False):\n",
        "        self.target = target\n",
        "        self.mse_scale = mse_scale\n",
        "        self.mask = mask\n",
        "        self.grey = grey\n",
        "    def __call__(self, x_in, key):\n",
        "        def mse_loss(x_in):\n",
        "            if self.grey:\n",
        "              return (self.mask * grey(x_in - self.target).square()).mean()\n",
        "            else:\n",
        "              return (self.mask * (x_in - self.target).square()).mean()\n",
        "        return self.mse_scale * jax.grad(mse_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.target, self.mse_scale, self.mask], [self.grey]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MaskedMSE(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MainCondFn(object):\n",
        "    # Used to construct the main cond_fn. Accepts a diffusion model which will \n",
        "    # be used for denoising, plus a list of 'conditions' which will\n",
        "    # generate gradient of a loss wrt the denoised, to be summed together.\n",
        "    def __init__(self, diffusion, conditions, use='pred'):\n",
        "        self.diffusion = diffusion\n",
        "        self.conditions = [c for c in conditions if c is not None]\n",
        "        self.use = use\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, key, x, t):\n",
        "        rng = PRNG(key)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        alphas, sigmas = get_ddpm_alphas_sigmas(t)\n",
        "\n",
        "        def denoise(key, x):\n",
        "            pred = self.diffusion(x, t, key).pred\n",
        "            if self.use == 'pred':\n",
        "                return pred\n",
        "            elif self.use == 'x_in':\n",
        "                return pred * sigmas + x * alphas\n",
        "        (x_in, backward) = jax.vjp(partial(denoise, rng.split()), x)\n",
        "\n",
        "        total = jnp.zeros_like(x_in)\n",
        "        for cond in self.conditions:\n",
        "            total += cond(x_in, rng.split())\n",
        "        final_grad = -backward(total)[0]\n",
        "\n",
        "        # clamp gradients to a max of 0.2\n",
        "        magnitude = final_grad.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "        final_grad = final_grad * jnp.where(magnitude > clamp_max, clamp_max / magnitude, 1.0)\n",
        "        return final_grad  \n",
        "    def tree_flatten(self):\n",
        "        return [self.diffusion, self.conditions], [self.use]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MainCondFn(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class ClassifierFn(object):\n",
        "    def __init__(self, model, params, guidance_scale, **kwargs):\n",
        "       self.model = model\n",
        "       self.params = params\n",
        "       self.guidance_scale = guidance_scale\n",
        "       self.kwargs = kwargs\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, key, x, t):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cosine_t = alpha_sigma_to_t(alpha, sigma).broadcast_to([n])\n",
        "        def fwd(x):\n",
        "          cx = Context(self.params, key).eval_mode_()\n",
        "          return self.guidance_scale * self.model.score(cx, x, cosine_t, **self.kwargs)\n",
        "        return -jax.grad(fwd)(x)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params, self.guidance_scale, self.kwargs], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params, guidance_scale, kwargs] = dynamic\n",
        "        [model] = static\n",
        "        return ClassifierFn(model, params, guidance_scale, **kwargs)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondFns(object):\n",
        "    def __init__(self, *conditions):\n",
        "        self.conditions = conditions\n",
        "    def __call__(self, key, x, t):\n",
        "        rng = PRNG(key)\n",
        "        total = jnp.zeros_like(x)\n",
        "        for cond in self.conditions:\n",
        "          total += cond(rng.split(), x, t)\n",
        "        return total\n",
        "    def tree_flatten(self):\n",
        "        return [self.conditions], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [conditions] = dynamic\n",
        "        return MixCondFn(*conditions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBlHR4k2lfiB"
      },
      "outputs": [],
      "source": [
        "def sample_step(key, x, t1, t2, diffusion, cond_fn, eta):\n",
        "    rng = PRNG(key)\n",
        "\n",
        "    n = x.shape[0]\n",
        "    alpha1, sigma1 = get_ddpm_alphas_sigmas(t1)\n",
        "    alpha2, sigma2 = get_ddpm_alphas_sigmas(t2)\n",
        "\n",
        "    # Run the model\n",
        "    out = diffusion(x, t1, rng.split())\n",
        "    eps = out.eps\n",
        "    pred0 = out.pred\n",
        "\n",
        "    # # Predict the denoised image\n",
        "    # pred0 = (x - eps * sigma1) / alpha1\n",
        "\n",
        "    # Adjust eps with conditioning gradient\n",
        "    cond_score = cond_fn(rng.split(), x, t1)\n",
        "    eps = eps - sigma1 * cond_score\n",
        "\n",
        "    # Predict the denoised image with conditioning\n",
        "    pred = (x - eps * sigma1) / alpha1\n",
        "\n",
        "    # Negative eta allows more extreme levels of noise.\n",
        "    ddpm_sigma = (sigma2**2 / sigma1**2).sqrt() * (1 - alpha1**2 / alpha2**2).sqrt()\n",
        "    ddim_sigma = jnp.where(eta >= 0.0, \n",
        "                           eta * ddpm_sigma, # Normal: eta interpolates between ddim and ddpm\n",
        "                           -eta * sigma2)    # Extreme: eta interpolates between ddim and q_sample(pred)\n",
        "    adjusted_sigma = (sigma2**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "    # Recombine the predicted noise and predicted denoised image in the\n",
        "    # correct proportions for the next step\n",
        "    x = pred * alpha2 + eps * adjusted_sigma\n",
        "\n",
        "    # Add the correct amount of fresh noise\n",
        "    x += jax.random.normal(rng.split(), x.shape) * ddim_sigma\n",
        "    return x, pred0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz4DN5r-B5ek"
      },
      "source": [
        "Load CLIP Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjE7Z7i3Mkza"
      },
      "outputs": [],
      "source": [
        "clip_size = 224\n",
        "normalize = Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                      std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "image_fn, text_fn, clip_params, _ = clip_jax.load('ViT-B/32')\n",
        "vit32 = Perceptor(image_fn, text_fn, clip_params)\n",
        "image_fn, text_fn, clip_params, _ = clip_jax.load('ViT-B/16')\n",
        "vit16 = Perceptor(image_fn, text_fn, clip_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_LhsWCFfJJ5"
      },
      "source": [
        "QoL Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-pAeASbfhvc"
      },
      "outputs": [],
      "source": [
        "def setres():\n",
        "    if resSize == \"SmallLandscape\":\n",
        "        side_y = 512\n",
        "        side_x = 896\n",
        "    elif resSize == \"LargeLandscape\":\n",
        "        side_y = 704\n",
        "        side_x = 1280\n",
        "    elif resSize == \"Square\":\n",
        "        side_x = side_y = 512\n",
        "    elif resSize == \"LargeSquare\":\n",
        "        side_x = side_y = 768\n",
        "    return side_x, side_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg6qjgQUflBu"
      },
      "outputs": [],
      "source": [
        "# Change outputFolder depending on diffusion model\n",
        "def get_output_folder(outputFolder):\n",
        "    os.makedirs(outputFolder, exist_ok=True)\n",
        "    videoOutputFolder = outputFolder\n",
        "    return outputFolder, videoOutputFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfLAIIJmfi7X"
      },
      "outputs": [],
      "source": [
        "# Make a video after diffusion is complete.\n",
        "\n",
        "secondsOfVideo = 16\n",
        "\n",
        "def make_video(batchnum):\n",
        "    print(\"disabled make_video function since we are doing it manually later\")\n",
        "    return\n",
        "    os.makedirs(videoOutputFolder, exist_ok=True)\n",
        "    timestring = time.strftime('%H-%M')\n",
        "    totalFrames = steps\n",
        "    totalFrames -= 1\n",
        "    videoName = f'{batchnum}_{all_title}_{timestring}'\n",
        "    frames = []\n",
        "    fps = steps//secondsOfVideo\n",
        "\n",
        "    tqdm.write(f'Generating video for batch {batchnum}...')\n",
        "    for i in range(totalFrames): \n",
        "        frames.append(Image.open(f\"/content/frames/\"+str(i)+'.jpgg'))\n",
        "    p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'medium', f'video_{videoName}.mp4'], stdin=PIPE)\n",
        "    for im in tqdm(frames):\n",
        "        im.save(p.stdin, 'PNG')\n",
        "    p.stdin.close()\n",
        "    p.wait()\n",
        "\n",
        "    if googleDrive:\n",
        "        shutil.copy(f\"/content/video_{videoName}.mp4\", f\"{videoOutputFolder}video_{videoName}.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62nhDcr-elV2"
      },
      "source": [
        "# Diffuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOynujCUCB4M"
      },
      "source": [
        "Configuration for the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxGgJmRzq3Cs"
      },
      "outputs": [],
      "source": [
        "choose_diffusion_model, sep, tail = choose_diffusion_model.partition(' |')\n",
        "outputFolder, videoOutputFolder = get_output_folder(outputFolderStatic)\n",
        "if choose_diffusion_model == \"PixelArtv4\":\n",
        "    all_title += \" #pixelart\"\n",
        "    \n",
        "for k in range(batch_size):\n",
        "        imagestepsFolder = '/content/output/'\n",
        "        os.makedirs(imagestepsFolder, exist_ok=True)\n",
        "\n",
        "# The number of prompts depends on the batch_size\n",
        "title = [all_title] * batch_size\n",
        "# If using a batch_size of two, can use the same prompt or two completety different ones \n",
        "# title = ['A spiraling portal with a spiritual and mystical sound','Vestiges of a hard landing by James Gurney']\n",
        "\n",
        "resSize, sep, tail = resSize.partition(' |')\n",
        "if resSize is not \"Custom\":\n",
        "    side_x, side_y = setres()\n",
        "    if choose_diffusion_model == \"Danbooru\":\n",
        "        side_x = side_x//2\n",
        "        side_y = side_y//2\n",
        "else:\n",
        "    side_x = imageWidth//64*64\n",
        "    side_y = imageHeight//64*64\n",
        "image_size = (side_x, side_y)\n",
        "\n",
        "make_cutouts = MakeCutouts(clip_size, cutn, cut_pow=cut_pow, p_mixgrey=0.0)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## Advanced settings:\n",
        "#@markdown clip_guidance_scale (default: 2500) - Controls how much the image should look like the prompt.\n",
        "#@markdown Higher resolutions generally tolerate higher values better, ie ~5000 is good for larger resolutions.\n",
        "#@markdown ##### **Note:** *With two perceptors, combined guidance scale is 2x because they are added together.*\n",
        "# Good values for cgs with various models - PixelArt: 8000, Danbooru: 3000.\n",
        "clip_guidance_scale =  2500#@param {type:\"integer\"}\n",
        "#@markdown tv_scale; Total Variation loss scale (default: 1000) - Higher values produce smoother outputs; similar to denoising.\n",
        "tv_scale = 1000 #@param {type:\"integer\"}\n",
        "#@markdown sat_scale (default: 500) - Prevents image from over/under-saturating.\n",
        "sat_scale = 500 #@param {type:\"integer\"}\n",
        "#@markdown clamp_max - (default: 0.2) Clamp gradients to a max value. \n",
        "clamp_max = 0.2 #@param {type:\"raw\"}\n",
        "#@markdown if 0, seed uses the current Unix timestamp (in seconds).\n",
        "seed = 0 #@param {type:\"integer\"}\n",
        "#@markdown InfoLOOB is an alternate way of using loss to guide the image to the text prompt.\n",
        "use_InfoLOOB = True #@param {type:\"boolean\"}\n",
        "#@markdown lm (default: 0.9) - Used if InfoLOOB is enabled, higher lm tries to make each image not look like the other prompts.\n",
        "lm = 0.9 #@param {type:\"raw\"}\n",
        "\n",
        "eta = 1.0   # 0.0: DDIM | 1.0: DDPM | -1.0: Extreme noise (q_sample)\n",
        "            # If a non-integer is used this will act as ratio and will modulate accordingly. \n",
        "\n",
        "\n",
        "if init_image == '':\n",
        "    init_image = None\n",
        "if seed == 0:\n",
        "    seed = None\n",
        "\n",
        "# OpenAI used T=1000 to 0. We've just rescaled to between 1 and 0.\n",
        "schedule = jnp.linspace(starting_noise, 0, steps+1)\n",
        "\n",
        "if init_image is not None:\n",
        "    init_array = Image.open(fetch(init_image)).convert('RGB')\n",
        "    init_array = init_array.resize(image_size, Image.LANCZOS)\n",
        "    init_array = jnp.array(TF.to_tensor(init_array)).unsqueeze(0).mul(2).sub(1)\n",
        "else:\n",
        "    init_array = None\n",
        "\n",
        "def config():\n",
        "    # Configure models and load parameters onto gpu.\n",
        "    # We do this in a function to avoid leaking gpu memory.\n",
        "\n",
        "    if choose_diffusion_model == \"OpenAI\":\n",
        "        # -- Openai with anti-jpeg --\n",
        "        openai = OpenaiModel(openai_512_model, openai_512_params())\n",
        "        secondary2 = CosineModel(secondary2_model, secondary2_params())\n",
        "        jpeg_0 = CosineModel(jpeg_model, jpeg_params(), cond=jnp.array([0]*batch_size)) # Clean class\n",
        "        jpeg_1 = CosineModel(jpeg_model, jpeg_params(), cond=jnp.array([2]*batch_size)) # Noisy class\n",
        "\n",
        "        jpeg_classifier_fn = ClassifierFn(classifier_model, classifier_params(), \n",
        "                                          guidance_scale=10000.0, # will generally depend on image size\n",
        "                                          cond=jnp.array([0]*batch_size), # Clean class\n",
        "                                          flood_level=0.7, # Prevent over-optimization\n",
        "                                          blur_size=3.0)\n",
        "\n",
        "        diffusion = LerpModels([(openai, 1.0),\n",
        "                                (jpeg_0, 1.0),\n",
        "                                (jpeg_1, -1.0)])\n",
        "        cond_model = secondary2\n",
        "\n",
        "        if use_InfoLOOB:\n",
        "            print(f\"Using {choose_diffusion_model} with InfoLOOB.\")\n",
        "            cond_fn = CondFns(MainCondFn(cond_model, [\n",
        "                                InfoLOOB(vit32.embed_texts(title), clip_guidance_scale, vit32, make_cutouts, lm, cut_batches),\n",
        "                                InfoLOOB(vit16.embed_texts(title), clip_guidance_scale, vit16, make_cutouts, lm, cut_batches),\n",
        "                                CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                                CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 and has_init_image else None,\n",
        "                                CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "                                ], use='pred'),\n",
        "                              jpeg_classifier_fn,\n",
        "                              )\n",
        "        else:\n",
        "            print(f\"Using {choose_diffusion_model} with CondCLIP.\")\n",
        "            cond_fn = CondFns(MainCondFn(cond_model, [\n",
        "                        CondCLIP(vit32.embed_texts(title), clip_guidance_scale, vit32, make_cutouts, cut_batches),\n",
        "                        CondCLIP(vit16.embed_texts(title), clip_guidance_scale, vit16, make_cutouts, cut_batches),\n",
        "                        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                        CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 and has_init_image else None,\n",
        "                        CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "                        ], use='pred'),\n",
        "                      jpeg_classifier_fn,\n",
        "                      )\n",
        "    \n",
        "    elif choose_diffusion_model == \"PixelArtv4\":\n",
        "        # -- pixel art model --\n",
        "        diffusion = CosineModel(pixelartv4_model, pixelartv4_params())\n",
        "        cond_model = diffusion\n",
        "        if use_InfoLOOB:\n",
        "            print(f\"Using {choose_diffusion_model} with InfoLOOB.\")\n",
        "            cond_fn = MainCondFn(cond_model, [\n",
        "                        InfoLOOB(vit32.embed_texts(title), clip_guidance_scale, vit32, MakeCutoutsPixelated(make_cutouts), lm, cut_batches),\n",
        "                        InfoLOOB(vit16.embed_texts(title), clip_guidance_scale, vit16, MakeCutoutsPixelated(make_cutouts), lm, cut_batches),\n",
        "                        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                        CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 and has_init_image else None,\n",
        "                        CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "                        ], use='pred')\n",
        "        else:\n",
        "            print(f\"Using {choose_diffusion_model} with CondCLIP.\")\n",
        "            cond_fn = MainCondFn(cond_model, [\n",
        "                        CondCLIP(vit32.embed_texts(title), clip_guidance_scale, vit32, MakeCutoutsPixelated(make_cutouts), cut_batches),\n",
        "                        CondCLIP(vit16.embed_texts(title), clip_guidance_scale, vit16, MakeCutoutsPixelated(make_cutouts), cut_batches),\n",
        "                        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                        CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 and has_init_image else None,\n",
        "                        CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "                        ], use='pred')\n",
        "\n",
        "    elif choose_diffusion_model == \"WikiArt\" or \"Danbooru\":\n",
        "        # -- v diffusion models --\n",
        "        if choose_diffusion_model == \"WikiArt\":\n",
        "            diffusion = KatModel(wikiart_256_model, wikiart_256_params())\n",
        "        elif choose_diffusion_model == \"Danbooru\":\n",
        "            diffusion = KatModel(danbooru_128_model, danbooru_128_params())\n",
        "        cond_model = diffusion\n",
        "\n",
        "        if use_InfoLOOB:\n",
        "            print(f\"Using {choose_diffusion_model} with InfoLOOB.\")\n",
        "            cond_fn = MainCondFn(cond_model, [\n",
        "                        InfoLOOB(vit32.embed_texts(title), clip_guidance_scale, vit32, make_cutouts, lm, cut_batches),\n",
        "                        InfoLOOB(vit16.embed_texts(title), clip_guidance_scale, vit16, make_cutouts, lm, cut_batches),\n",
        "                        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                        CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 and has_init_image else None,\n",
        "                        CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "                        ], use='pred')\n",
        "        else:\n",
        "            print(f\"Using {choose_diffusion_model} with CondCLIP.\")\n",
        "            cond_fn = MainCondFn(cond_model, [\n",
        "                        CondCLIP(vit32.embed_texts(title), clip_guidance_scale, vit32, make_cutouts, cut_batches),\n",
        "                        CondCLIP(vit16.embed_texts(title), clip_guidance_scale, vit16, make_cutouts, cut_batches),\n",
        "                        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                        CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 and has_init_image else None,\n",
        "                        CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "                        ], use='pred') \n",
        "\n",
        "    return diffusion, cond_fn\n",
        "\n",
        "diffusion, cond_fn = config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkTQD8Y5b0Sc"
      },
      "source": [
        "Start the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoIL7ayzq7kC"
      },
      "outputs": [],
      "source": [
        "#@title Diffuse!\n",
        "displayRate =  10#@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "saveVideo = True #@param {type:\"boolean\"}\n",
        "\n",
        "!mkdir -p /content/frames\n",
        "\n",
        "def sanitize(title):\n",
        "  return title[:100].replace('/', '_').replace('\\\\', '_').replace('\"', 'q')\n",
        "\n",
        "@torch.no_grad()\n",
        "def run():\n",
        "    if seed is None:\n",
        "        local_seed = int(time.time())\n",
        "    else:\n",
        "        local_seed = seed    \n",
        "    rng = PRNG(jax.random.PRNGKey(local_seed))\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "        print(f'Starting run of ({all_title}) with seed {local_seed}...')\n",
        "        ts = schedule\n",
        "        alphas, sigmas = get_ddpm_alphas_sigmas(ts)\n",
        "        cosine_ts = alpha_sigma_to_t(alphas, sigmas)\n",
        "\n",
        "        x = sigmas[0] * jax.random.normal(rng.split(), [batch_size, 3, image_size[1], image_size[0]])\n",
        "\n",
        "        if init_array is not None:\n",
        "            x = x + alphas[0] * init_array\n",
        "\n",
        "        # Main loop\n",
        "        local_steps = schedule.shape[0] - 1\n",
        "        for j in tqdm(range(local_steps)):\n",
        "            if ts[j] != ts[j+1]:\n",
        "                # Skip steps where the ts are the same, to make it easier to\n",
        "                # make complicated schedules out of cat'ing linspaces.\n",
        "                x, pred = sample_step(rng.split(), x, ts[j], ts[j+1], diffusion, cond_fn, eta)\n",
        "            if j % displayRate == 0 or j == local_steps - 1:\n",
        "                images = pred.add(1).div(2).clamp(0, 1)\n",
        "                images = torch.tensor(np.array(images))\n",
        "                #display.display(TF.to_pil_image(utils.make_grid(images, 4).cpu()))\n",
        "                for k in range(batch_size):\n",
        "                    pil_image = TF.to_pil_image(images[k])\n",
        "                    pil_image.save(f\"{output_path}/{j:04d}_{k}.jpg\")\n",
        "\n",
        "            if j == j and saveVideo:\n",
        "                for k in range(batch_size):\n",
        "                    images = pred.add(1).div(2).clamp(0, 1)\n",
        "                    images = torch.tensor(np.array(images))\n",
        "                    stepnum = f'{j}.jpg' \n",
        "                    pil_image = TF.to_pil_image(images[k])\n",
        "                    pil_image.save(f\"/content/frames/{j:04d}_{k}.jpg\")\n",
        "\n",
        "        # Save samples\n",
        "        if batch_size >= 2:\n",
        "            os.makedirs('samples/grid', exist_ok=True)\n",
        "            os.makedirs(f'{outputFolder}grid', exist_ok=True)\n",
        "            TF.to_pil_image(utils.make_grid(images, 4).cpu()).save(f'samples/grid/{timestring}_{sanitize(all_title)}.png')\n",
        "            TF.to_pil_image(utils.make_grid(images, 4).cpu()).save(f'{outputFolder}grid/{timestring}_{sanitize(all_title)}.png')\n",
        "\n",
        "        os.makedirs('samples/images', exist_ok=True)\n",
        "        for k in range(batch_size):\n",
        "            this_title = sanitize(title[k])\n",
        "            dname = f'samples/images/{timestring}_{k}_{this_title}.png'\n",
        "            pil_image = TF.to_pil_image(images[k])\n",
        "            pil_image.save(dname)\n",
        "            if googleDrive:\n",
        "                pil_image.save(f'{outputFolder}{k}_{this_title}_{timestring}.png')\n",
        "            if saveVideo:\n",
        "                make_video(k)\n",
        "        if i == n_batches-1:\n",
        "            print(f'\\nFinished run of ({all_title}) with seed {local_seed}...')\n",
        "        \n",
        "\n",
        "\n",
        "try:\n",
        "  run()\n",
        "  success = True\n",
        "except:\n",
        "  import traceback\n",
        "  traceback.print_exc()\n",
        "  success = False\n",
        "assert success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2TJgolDy9Az"
      },
      "outputs": [],
      "source": [
        "out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "!mkdir -p /tmp/ffmpeg\n",
        "!cp /content/frames/*.jpg /tmp/ffmpeg\n",
        "last_frame=!ls -t /tmp/ffmpeg/*.jpg | head -1\n",
        "last_frame = last_frame[0]\n",
        "\n",
        "# Copy last frame to start and duplicate at end so it sticks around longer\n",
        "end_still_seconds = 4\n",
        "!cp -v $last_frame /tmp/ffmpeg/0000.jpg\n",
        "for i in range(end_still_seconds * 10):\n",
        "  pad_file = f\"/tmp/ffmpeg/zzzz_pad_{i:05}.jpg\"\n",
        "  !cp -v $last_frame $pad_file\n",
        "\n",
        "!ffmpeg  -r 10 -i /tmp/ffmpeg/%*.jpg -y -c:v libx264 /tmp/vid_no_audio.mp4\n",
        "!ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n",
        "\n",
        "print(\"Written\", out_file)\n",
        "!sleep 2\n",
        "!rm -r /tmp/ffmpeg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NYiq47LEeboR"
      ],
      "name": "QoL tweaks for nshepperd's JAX CLIP Guided Diffusion v2.4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
