{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0PbNr1Ncxmw"
      },
      "source": [
        "**By [Ajay Jain](https://ajayj.com/), [Ben Mildenhall](https://bmild.github.io/), [Jonathan T. Barron](https://jonbarron.info/), [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/), and [Ben Poole](https://cs.stanford.edu/~poole/).**\n",
        "\n",
        "\n",
        "Website: https://ajayj.com/dreamfields\n",
        "\n",
        "This notebook demonstrates a scaled down version of Dream Fields, a method for synthesizing 3D objects from natural language descriptions. Dream Fields train a 3D Neural Radiance Field (NeRF) so 2D renderings from any perspective are semantically consistent with a given description. Our loss is based on the OpenAI CLIP text-image model.\n",
        "\n",
        "If you find this code relevant to your work, please cite our [paper](https://arxiv.org/abs/2112.01455):\n",
        "```\n",
        "@article{jain2021dreamfields,\n",
        "  author = {Jain, Ajay and Mildenhall, Ben and Barron, Jonathan T. and Abbeel, Pieter and Poole, Ben},\n",
        "  title = {Zero-Shot Text-Guided Object Generation with Dream Fields},\n",
        "  joural = {arXiv},\n",
        "  month = {December},\n",
        "  year = {2021},\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Nz2GWyagQoY5"
      },
      "outputs": [],
      "source": [
        "#@markdown **Natural language query and seed.**\n",
        "query = 'a robotic dog. a mechanical dog. robot dog'  #@param {type:'string'}\n",
        "\n",
        "iters =   10000 #@param {type:'integer'}\n",
        "\n",
        "output_path = \"/content\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHocyi1uB3Yz"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "#!pip install --upgrade jax[cuda] jaxlib -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "!python -c \"print(__import__('jax').local_devices())\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-66Qid-BCkpe"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/google-research/google-research\n",
        "%cd google-research/dreamfields\n",
        "#!pip install -r requirements.txt\n",
        "!pip install clu\n",
        "!pip install git+git://github.com/voodoohop/scenic.git\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install dm_pix mediapy\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.config.experimental.set_visible_devices([], 'GPU')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgSvskIfJn9I"
      },
      "source": [
        "# Install dependencies.\n",
        "After running, you may need to restart your runtime (Runtime > Restart Runtime)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgxgrMLeeH7"
      },
      "outputs": [],
      "source": [
        "# Uninstall and reinstall packages to deal with version mismatch\n",
        "!pip uninstall --yes numpy matplotlib\n",
        "!pip install --upgrade numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGNOiEPNDXET"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "%cd /content/google-research/dreamfields/\n",
        "\n",
        "#import os \n",
        "#os.environ['LD_LIBRARY_PATH']='/usr/local/cuda-11.1/lib64:/usr/lib64-nvidia'\n",
        "#!python run.py --config=dreamfields/config/config_lq.py --query=\"sleepy bonsai cat\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEj5tFx4Jmbh"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ataxIm_pF2R"
      },
      "outputs": [],
      "source": [
        "# Prevent tensorflow from hogging memory\n",
        "import tensorflow as tf\n",
        "tf.config.experimental.set_visible_devices([], \"GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uQJRMw-NLKQ"
      },
      "outputs": [],
      "source": [
        "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "\n",
        "import jax\n",
        "try:\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "  default_precision = 'bfloat16'\n",
        "  use_gpu = False\n",
        "except KeyError as e:\n",
        "  print('No TPUs available')\n",
        "  default_precision = 'tensorfloat32'\n",
        "  use_gpu = True\n",
        "\n",
        "print(jax.devices())\n",
        "\n",
        "if use_gpu:\n",
        "  !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VekUreA5mmu"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "import os\n",
        "import random as py_random\n",
        "import time\n",
        "from typing import Optional, Sequence\n",
        "\n",
        "from absl import logging\n",
        "from clu import metric_writers\n",
        "import flax \n",
        "import flax.linen as nn\n",
        "from flax.training import checkpoints\n",
        "from IPython.display import clear_output, display, Image\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax import random\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import ml_collections\n",
        "import numpy as onp\n",
        "from scipy import stats\n",
        "import tensorflow.io.gfile as gfile\n",
        "import tqdm\n",
        "\n",
        "nn.enable_named_call()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyOz2vnWj7WI"
      },
      "outputs": [],
      "source": [
        "from dreamfields import augment\n",
        "from dreamfields import helpers\n",
        "from dreamfields import log\n",
        "from dreamfields import mipnerf\n",
        "from dreamfields import scene\n",
        "from dreamfields import schedule\n",
        "from dreamfields.config.config_mq import get_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrpIOPoUGCXS"
      },
      "source": [
        "# Configure run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WR_QthuULpbi"
      },
      "outputs": [],
      "source": [
        "#@title Training hyperparameters\n",
        "#@markdown More parameters are documented [on GitHub](https://github.com/google-research/google-research/blob/master/dreamfields/dreamfields/config/config_base.py).\n",
        "\n",
        "#@markdown **Natural language query and seed.**\n",
        "seed = 0  #@param {type:'integer'}\n",
        "\n",
        "#@markdown **CLIP guidance model.** `clip_vit_b16` is suggested.\n",
        "loss_model = \"clip_vit_b16\" #@param [\"clip_vit_b32\", \"clip_vit_b16\", \"clip_resnet_50\", \"clip_resnet_101\", \"clip_resnet_50x4\"]\n",
        "\n",
        "\n",
        "#@markdown **Quality settings.** Increase `iters`, `render_width`, `num_samples` or `n_local_aug` for higher quality results, or lower to run faster and reduce memory usage.\n",
        "config = get_config(iters=iters)\n",
        "config.render_width = 88  #@param {type:'integer'}\n",
        "config.crop_width = 80  #@param {type:'integer'}\n",
        "config.num_samples =   64#@param {type:'integer'}\n",
        "config.n_local_aug = 7 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "assert config.crop_width <= config.render_width\n",
        "\n",
        "config.query = query\n",
        "config.seed = seed\n",
        "config.loss_model = loss_model\n",
        "\n",
        "#@markdown **Save memory by recomputation**. These options reduce memory usage but slow down optimization.\n",
        "config.remat_image_encoder = False #@param {type:\"boolean\"}\n",
        "config.remat_mlp = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **Camera sampling.** These define 3D data augs for viewing the scene.\n",
        "min_zoom = 1.2 #@param {type:\"slider\", min:1, max:2, step:0.1}\n",
        "max_zoom = 1.2 #@param {type:\"slider\", min:1, max:2, step:0.1}\n",
        "min_elevation = 30 #@param {type:\"slider\", min:0, max:90, step:1}\n",
        "max_elevation = 30 #@param {type:\"slider\", min:0, max:90, step:1}\n",
        "min_azimuth = 0 #@param {type:\"slider\", min:0, max:90, step:1}\n",
        "max_azimuth = 360 #@param {type:\"slider\", min:0, max:360, step:1}\n",
        "\n",
        "config.focal_mult_range = [min_zoom, max_zoom]  # Zoom augmentations.\n",
        "config.th_range = [0, 360]  # Optimize in 360 degrees around object.\n",
        "config.phi_range = [-max_elevation, -min_elevation]  # Camera elevation, neg degrees above equator.\n",
        "config.ema_scene_origin = True  # Set to False if object clipped too much.\n",
        "# Freeze tracked origin initially\n",
        "config.fix_origin_iters = 200  #@param {type:\"integer\"}\n",
        "config.origin_decay = 0.9995  #@param {type:\"number\"}\n",
        "\n",
        "#@markdown **Learning rate schedule**. Linear warmup from lr0 to lr1 then linear decay to lr2. Try a higher lr1 if impatient.\n",
        "lr_warmup_iters = 100  #@param {type:'number'}\n",
        "config.lr_i_split = lr_warmup_iters\n",
        "config.lr0 = 1e-5  #@param {type:'number'}\n",
        "config.lr1 = 5e-4  #@param {type:'number'}\n",
        "config.lr2 = 1e-4  #@param {type:'number'}\n",
        "\n",
        "# Reduce size of scene to get away with fewer samples per ray\n",
        "config.mr0 = 1.\n",
        "config.mr1 = 1.\n",
        "# Duration to anneal in transmittance loss.\n",
        "# Increase if transmittance goes to 1 (empty scene).\n",
        "config.acc_target_i_split = 100  #@param {type:\"integer\"}\n",
        "config.acc_target0 = 0.5\n",
        "config.acc_target0 = 0.1\n",
        "# Weight on transmittance loss\n",
        "config.acc_lam = 0.25\n",
        "\n",
        "#@markdown **MLP architecture.** Shallower and narrower than in the paper.\n",
        "residual_blocks = 2 #@param {type:\"slider\", min:1, max:6, step:1}\n",
        "min_mlp_width = 128  #@param {type:'number'}\n",
        "max_mlp_width = 192  #@param {type:'number'}\n",
        "config.mipnerf.features_early = [min_mlp_width]\n",
        "config.mipnerf.features_residual = [(max_mlp_width, min_mlp_width)] * residual_blocks\n",
        "config.mipnerf.features_late = [4]\n",
        "\n",
        "#@markdown **Coarse to fine**: Integrate positional encoding over larger regions at the start of training. log scale 1 is used in Dream Fields (i.e. no coarse to fine, as in mip-NeRF).\n",
        "initial_log_posenc_scale = 1 #@param {type:\"slider\", min:1, max:8, step:0.1}\n",
        "coarse_to_fine_iters = 1000 #@param {type:\"number\"}\n",
        "config.decay_start = initial_log_posenc_scale\n",
        "config.decay_iters = coarse_to_fine_iters\n",
        "\n",
        "#@markdown **Logging parameters.** Increase the final render quality here.\n",
        "\n",
        "# Save model checkpoints\n",
        "config.checkpoint_every = iters\n",
        "# Enable low-quality video.\n",
        "config.render_lq_video = True\n",
        "config.video_every = iters\n",
        "config.lq_video_n_frames =   192#@param {type:'integer'}\n",
        "config.lq_video_width = 512  #@param {type:'integer'}\n",
        "config.test.num_samples = 128  # Increase if desired to reduce aliasing.\n",
        "# Disable HQ video.\n",
        "config.render_hq_video = False\n",
        "config.hq_video_every = 100000\n",
        "# Disable validation view rendering.\n",
        "config.render_every = 100000\n",
        "\n",
        "timestr = time.strftime('%Y%m%d-%H%M%S')\n",
        "experiment_dir = f'/content/results/{timestr} {config.query}'\n",
        "work_unit_dir = f'{experiment_dir}/1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLjBuwrkGFkQ"
      },
      "source": [
        "# Set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB4YHW5UF-0e"
      },
      "outputs": [],
      "source": [
        "py_random.seed(config.seed * jax.process_count() + jax.process_index())\n",
        "onp.random.seed(config.seed * jax.process_count() + jax.process_index())\n",
        "rng = helpers.RngGen(\n",
        "    jax.random.fold_in(jax.random.PRNGKey(config.seed), jax.process_index()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Gy03snwOmU"
      },
      "source": [
        "# Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSEhye7OwP4l"
      },
      "outputs": [],
      "source": [
        "class MipMLPLate(nn.Module):\n",
        "  \"\"\"MLP architecture.\"\"\"\n",
        "  activation: str\n",
        "  features_early: Sequence[int]\n",
        "  features_residual: Sequence[Sequence[int]]\n",
        "  features_late: Sequence[int]\n",
        "  fourfeat: bool\n",
        "  max_deg: int\n",
        "  use_cov: bool\n",
        "  dropout_rate: float\n",
        "  remat: bool\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, mean, cov=None, x_late=None, decayscale=1.,\n",
        "               *, deterministic):\n",
        "    \"\"\"Run MLP.\"\"\"\n",
        "    # Integrate the positional encoding over a region centered at mean.\n",
        "    if not self.fourfeat:\n",
        "      # Axis-aligned positional encoding.\n",
        "      feat = 2**np.arange(self.max_deg)[:, None, None] * np.eye(3)\n",
        "      feat = feat.reshape(-1, 3)\n",
        "    else:\n",
        "      # Random Fourier Feature positional encoding. Fix the PRNGKey used for the\n",
        "      # fourier feature basis so the encoding does not change over iterations.\n",
        "      fourfeat_key = random.PRNGKey(124124)\n",
        "      dirs = random.normal(fourfeat_key, (3, 128))\n",
        "      dirs = dirs / np.linalg.norm(dirs, axis=-1, keepdims=True)\n",
        "      rads = 2 ** (self.max_deg * random.uniform(fourfeat_key, (128,)))\n",
        "      feats = (rads * dirs).astype(np.int32)\n",
        "      feats = np.concatenate([np.eye(3), feats], 1).astype(np.float32)\n",
        "      feat = feats.T\n",
        "\n",
        "    mean_proj = (mean[Ellipsis, None] * feat.T).sum(-2)\n",
        "    if self.use_cov:\n",
        "      cov_diag_proj = ((cov[Ellipsis, None] * feat.T).sum(-2) * feat.T).sum(-2)\n",
        "      decay = np.exp(-.5 * cov_diag_proj * decayscale**2)\n",
        "    else:\n",
        "      # Disable IPE\n",
        "      decay = 1.\n",
        "    x = np.concatenate([decay * np.cos(mean_proj),\n",
        "                        decay * np.sin(mean_proj)], -1)\n",
        "\n",
        "    # Network\n",
        "    activation = nn.__getattribute__(self.activation)\n",
        "    if self.remat:\n",
        "      activation = jax.remat(activation)\n",
        "    for feat in self.features_early:\n",
        "      x = activation(nn.Dense(feat)(x))\n",
        "      x = nn.Dropout(self.dropout_rate)(\n",
        "          x, deterministic=deterministic)\n",
        "\n",
        "    for feat_block in self.features_residual:\n",
        "      norm = nn.LayerNorm()\n",
        "      if self.remat:\n",
        "        norm = jax.remat(norm)\n",
        "      h = norm(x)\n",
        "      for l, feat in enumerate(feat_block):\n",
        "        h = nn.Dense(feat)(h)\n",
        "        h = nn.Dropout(self.dropout_rate)(\n",
        "            h, deterministic=deterministic)\n",
        "        if l < len(feat_block) - 1:  # don't activate right before the residual\n",
        "          h = activation(h)\n",
        "      x = x + h\n",
        "\n",
        "    if x_late is not None:\n",
        "      x = np.concatenate([x, x_late], axis=-1)\n",
        "    for feat in self.features_late[:-1]:\n",
        "      x = activation(nn.Dense(feat)(x))\n",
        "      x = nn.Dropout(self.dropout_rate)(\n",
        "          x, deterministic=deterministic)\n",
        "    x = nn.Dense(self.features_late[-1])(x)  # don't activate output\n",
        "    return x\n",
        "\n",
        "\n",
        "def init_nerf_model(key, config):\n",
        "  \"\"\"Initialize NeRF MLP.\"\"\"\n",
        "  if config.parameterization == 'mipnerf':\n",
        "    model = MipMLPLate(\n",
        "        activation=config.mlp_activation,\n",
        "        features_early=config.features_early,\n",
        "        features_residual=config.features_residual,\n",
        "        features_late=config.features_late,\n",
        "        max_deg=config.posenc_deg,\n",
        "        fourfeat=config.fourfeat,\n",
        "        use_cov=config.mipnerf.get('use_cov', True),\n",
        "        dropout_rate=config.mipnerf.get('dropout_rate', 0.),\n",
        "        remat=config.get('remat_mlp', False))\n",
        "    if config.viewdirs:\n",
        "      x_late = scene.posenc(np.zeros([1, 3]), config.posenc_dirs_deg)\n",
        "    else:\n",
        "      x_late = None\n",
        "    variables = model.init(\n",
        "        key, np.zeros([1, 3]), np.zeros([1, 3, 3]), x_late, deterministic=True)\n",
        "\n",
        "    render_rays = functools.partial(mipnerf.render_rays_mip, model=model)\n",
        "  else:\n",
        "    raise ValueError\n",
        "\n",
        "  return variables, render_rays\n",
        "\n",
        "\n",
        "class DreamField:\n",
        "  \"\"\"Trainable Dream Field model.\"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    self.config = config\n",
        "\n",
        "  def run_train(self,\n",
        "                experiment_dir,\n",
        "                work_unit_dir,\n",
        "                rng: helpers.RngGen,\n",
        "                yield_results=False):\n",
        "    \"\"\"Train a Dream Field and save results to work_unit_dir.\"\"\"\n",
        "    t_start = time.time()\n",
        "    config = self.config\n",
        "\n",
        "    logging.info('Local devices: %s', jax.local_devices())\n",
        "    logging.info('All devices: %s', jax.devices())\n",
        "\n",
        "    ## Load CLIP\n",
        "    encode_image, encode_text, preprocess_image, tokenize_fn = (\n",
        "        helpers.load_image_text_model(config.loss_model))\n",
        "\n",
        "    ## Pick a prompt\n",
        "    template = config.get('query_template', '{query}')\n",
        "    query = template.format(query=config.query)\n",
        "    z_clip = encode_text(tokenize_fn(query))\n",
        "    del encode_text, tokenize_fn  # Clean up text encoder.\n",
        "\n",
        "    ## Scene origin manually tracked\n",
        "    scene_origin = scene.EMA(\n",
        "        np.zeros(3, dtype=np.float64), decay=config.get('origin_decay', 0.999))\n",
        "\n",
        "    def train_step(state, rays, key, *multistep_constants):\n",
        "      \"\"\"Perform a training iteration, optionally composed of multiple substeps.\n",
        "\n",
        "      Using multiple substeps slightly reduces training time, but only one\n",
        "      substep per training iteration is used in experiments.\n",
        "\n",
        "      Args:\n",
        "        state: Optimizer state.\n",
        "        rays: Camera rays for rendering, shared across all substeps.\n",
        "        key: PRNGKey for random number generation (e.g. for augmentations).\n",
        "        *multistep_constants: Training constants that can vary across substeps.\n",
        "          6 arrays of constants of length config.substeps are expected:\n",
        "            (1) lrs: learning rates\n",
        "            (2) scs: scale factor for integrated positional encoding. Larger\n",
        "                scales lead to a blurrier appearance. A constant sc=1 is the\n",
        "                standard mip-NeRF IPE, and used by Dream Fields.\n",
        "            (3) sns: standard deviation of pre-activation noise for NeRF\n",
        "                density. Dream Fields use sn=0.\n",
        "                  density(x) = softplus(s(x) + eps), eps ~ N(0, sn^2)\n",
        "            (4) mrs: norm of radiance mask, defining scene bounds.\n",
        "            (5) acct: transmittance loss hyperparameter, defining the target\n",
        "                average opacity. This is 1 - tau (target transmittance).\n",
        "            (6) acclam: weight of transmittance loss.\n",
        "\n",
        "      Returns:\n",
        "        state: Updated optimizer state.\n",
        "        last_augs: Augmented views of renderings from the last substep.\n",
        "        mean_losses: Dictionary of losses averaged over replicas and substeps.\n",
        "        scene_origin: Updated origin of the scene, based on the center of mass.\n",
        "      \"\"\"\n",
        "      # NOTE(jainajay): rays are shared across all substeps\n",
        "      pmean = functools.partial(jax.lax.pmean, axis_name='batch')\n",
        "      psum = functools.partial(jax.lax.psum, axis_name='batch')\n",
        "\n",
        "      def loss_fn(params, key, sc, sn, mr, acct, acclam):\n",
        "        render_key, aug_key, key = random.split(key, 3)\n",
        "\n",
        "        # Render from nerf\n",
        "        (rgb_est_flat, _, acc_est_flat), aux = render_rays(\n",
        "            rays=rays,\n",
        "            variables=params,\n",
        "            rng=render_key,\n",
        "            config=config,\n",
        "            sc=sc,\n",
        "            sigma_noise_std=sn,\n",
        "            mask_rad=mr,\n",
        "            origin=scene_origin.value,\n",
        "            train=True)\n",
        "        rgb_est = scene.gather_and_reshape(rgb_est_flat, config.render_width, 3)\n",
        "        acc_est = scene.gather_and_reshape(acc_est_flat, config.render_width, 1)\n",
        "        # Make augmentations process specific\n",
        "        aug_key = random.fold_in(aug_key, pid)\n",
        "        # Perform augmentations and resize to clip_width\n",
        "        augs = augment.augment_rendering(config, rgb_est, acc_est, aug_key)\n",
        "\n",
        "        # Run through CLIP\n",
        "        augs_pre = preprocess_image(augs)\n",
        "        with jax.default_matmul_precision(default_precision):\n",
        "          if config.get('remat_image_encoder', False):\n",
        "            z_est = jax.remat(encode_image)(augs_pre)\n",
        "          else:\n",
        "            z_est = encode_image(augs_pre)\n",
        "        clip_loss = -(z_est * z_clip).sum(-1).mean()\n",
        "        total_loss = clip_loss\n",
        "\n",
        "        transparency_loss = config.get('transparency_loss', None)\n",
        "        acc_mean = np.mean(acc_est)\n",
        "        aux['losses']['acc_mean'] = acc_mean\n",
        "        if transparency_loss == 'neg_lam_transmittance_clipped':\n",
        "          # Compute the Dream Fields transmittance loss for scene sparsity.\n",
        "          trans_mean = 1 - acc_mean\n",
        "          trans_mean_clipped = np.minimum(1 - acct, trans_mean)\n",
        "          reg = acclam * trans_mean_clipped\n",
        "          total_loss -= reg\n",
        "\n",
        "          aux['losses']['trans_mean_clipped'] = trans_mean_clipped\n",
        "          aux['losses']['acc_reg_additive'] = reg\n",
        "        else:\n",
        "          assert transparency_loss is None\n",
        "\n",
        "        # Compute a weighted mean of each replica's estimated scene origin,\n",
        "        # since replicas get a different subset of rays\n",
        "        total_sigma = psum(aux['scene_origin_sigma'])\n",
        "        aux['scene_origin'] = psum(aux['scene_origin'] *\n",
        "                                   aux['scene_origin_sigma'] / total_sigma)\n",
        "\n",
        "        aux['losses'].update({\n",
        "            'clip_loss': clip_loss,\n",
        "            'loss': total_loss,\n",
        "        })\n",
        "        aux['augs'] = augs\n",
        "        return total_loss, aux\n",
        "\n",
        "      grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "\n",
        "      # Scan over substeps\n",
        "      def body_fn(state, step_constants):\n",
        "        lr, step_constants = step_constants[0], step_constants[1:]\n",
        "        grad_fn_key, _ = random.split(key, 2)\n",
        "        (_, aux), grad = grad_fn(state.target, grad_fn_key, *step_constants)\n",
        "        grad = pmean(grad)  # all-reduce grad\n",
        "        aux['losses'] = pmean(aux['losses'])\n",
        "        aux['losses']['grad_norm'] = helpers.tree_norm(grad)\n",
        "        state = state.apply_gradient(grad, learning_rate=lr)\n",
        "        return state, aux\n",
        "\n",
        "      assert len(multistep_constants) == 6\n",
        "      multistep_constants = np.array(multistep_constants).T\n",
        "\n",
        "      if config.substeps == 1:\n",
        "        state, aux = body_fn(state, np.squeeze(multistep_constants))\n",
        "        last_augs = aux['augs']\n",
        "      else:\n",
        "        state, aux = jax.lax.scan(body_fn, state, multistep_constants)\n",
        "        # Augmentations from last substep.\n",
        "        # Shape: [n_local_aug, clip_width, clip_width, 3]\n",
        "        last_augs = aux['augs'][-1]\n",
        "\n",
        "      # Average each type of loss over substeps\n",
        "      mean_losses = jax.tree_map(np.mean, aux['losses'])\n",
        "      return state, last_augs, mean_losses, aux['scene_origin']\n",
        "\n",
        "    train_pstep = jax.pmap(\n",
        "        train_step,\n",
        "        axis_name='batch',\n",
        "        in_axes=(0, 0, 0, None, None, None, None, None, None))\n",
        "\n",
        "    onp.random.seed(config.seed)\n",
        "\n",
        "    n_device = jax.local_device_count()\n",
        "    pid = jax.process_index()\n",
        "    logging.info('n_device %d', n_device)\n",
        "    ## Modified NeRF architecture, with swish, softplus, skips.\n",
        "    variables, render_rays = init_nerf_model(rng.advance(1), config)\n",
        "    state = flax.optim.Adam(config.lr0, eps=config.adam_eps).create(variables)\n",
        "\n",
        "    ## Try to restore a checkpoint.\n",
        "    restore_dir = config.get('restore_dir', experiment_dir)\n",
        "    restore_dir = os.path.join(restore_dir, os.path.basename(work_unit_dir))\n",
        "    if checkpoints.latest_checkpoint(restore_dir):\n",
        "      restored = checkpoints.restore_checkpoint(\n",
        "          restore_dir,\n",
        "          target={\n",
        "              'origin': np.zeros(3),\n",
        "              'state': state,\n",
        "              'vars': variables\n",
        "          })\n",
        "      scene_origin.value = onp.array(restored['origin'])\n",
        "      state = restored['state']\n",
        "      variables = restored['vars']\n",
        "      logging.info('restored checkpoint from step %d', state.state.step)\n",
        "    else:\n",
        "      logging.info('did not find checkpoint in %s', restore_dir)\n",
        "\n",
        "    ## Replicate state.\n",
        "    step_init = state.state.step\n",
        "    helpers.defragment()\n",
        "    state = flax.jax_utils.replicate(state, jax.devices())\n",
        "    helpers.defragment()\n",
        "\n",
        "    ## pmap'd rendering for test time evaluation.\n",
        "    kwargs_test = dict(rng=None, sigma_noise_std=0.)\n",
        "    config_test = ml_collections.ConfigDict(config)\n",
        "    config_test.update(config.test)\n",
        "\n",
        "    @functools.partial(jax.pmap, in_axes=(0, None, None, None))\n",
        "    def render_test_p(rays, variables, sc=1., mr=1.):\n",
        "      return render_rays(\n",
        "          rays=rays,\n",
        "          variables=variables,\n",
        "          sc=sc,\n",
        "          mask_rad=mr,\n",
        "          origin=scene_origin.value,\n",
        "          config=config_test,\n",
        "          **kwargs_test)[0]\n",
        "\n",
        "    def render_test(rays, variables, sc=1., mr=1.):\n",
        "      sh = rays[0].shape\n",
        "      rays = [x.reshape((jax.device_count(), -1) + x.shape[1:]) for x in rays]\n",
        "      out = render_test_p(rays, variables, sc, mr)\n",
        "      out = [x.reshape(sh[:-1] + (-1,)) for x in out]\n",
        "      return out\n",
        "\n",
        "    def render_loop(rays, variables, sc=1., mr=1., chunk=2**13):\n",
        "      sh = list(rays[0].shape[:-1])\n",
        "      rays = [x.reshape((-1,) + x.shape[-1:]) for x in rays]\n",
        "      outs = [\n",
        "          render_test([x[i:i + chunk]\n",
        "                       for x in rays], variables, sc, mr)\n",
        "          for i in range(0, rays[0].shape[0], chunk)\n",
        "      ]\n",
        "      outs = [\n",
        "          np.reshape(np.concatenate([z[i]\n",
        "                                     for z in outs]), sh + [-1])\n",
        "          for i in range(3)\n",
        "      ]\n",
        "      return outs\n",
        "\n",
        "    ## Training loop\n",
        "    t_total = 0.\n",
        "    logging.info('Experiment dir %s', experiment_dir)\n",
        "    logging.info('Work unit dir %s', work_unit_dir)\n",
        "    gfile.makedirs(work_unit_dir)\n",
        "\n",
        "    if jax.process_index() == 0:\n",
        "      train_config = config.copy_and_resolve_references()\n",
        "      log.write_config_json(train_config, work_unit_dir)\n",
        "\n",
        "    # Scale instrinsics to different resolutions.\n",
        "    hwf_clip_r = scene.scale_intrinsics(config.retrieve_widths[0])\n",
        "    hwf_base = scene.scale_intrinsics(config.render_width)\n",
        "    hwf_video = scene.scale_intrinsics(config.get('lq_video_width', 300.))\n",
        "    hwf_video_hq = scene.scale_intrinsics(config.get('hq_video_width', 400.))\n",
        "\n",
        "    # JIT compile ray generation\n",
        "    @jax.jit\n",
        "    def camera_ray_batch_base(p, focal_mult):\n",
        "      return scene.camera_ray_batch(p, *hwf_base[:2], hwf_base[2] * focal_mult)\n",
        "\n",
        "    @jax.jit\n",
        "    def sample_pose_focal(key):\n",
        "      return scene.sample_camera(key, config.th_range, config.phi_range,\n",
        "                                 config.rad_range, config.focal_mult_range)\n",
        "\n",
        "    shard_rays_jit = jax.jit(functools.partial(scene.shard_rays))\n",
        "\n",
        "    def sample_iter_data(key, step):\n",
        "      # Sample pose, focal length multiplier.\n",
        "      pose, rad, focal_mult = sample_pose_focal(key)\n",
        "\n",
        "      # Generate rays, shaped for pmap over devices.\n",
        "      rays = camera_ray_batch_base(pose, focal_mult)\n",
        "      rays_in = shard_rays_jit(rays)\n",
        "      # Select rays for this process\n",
        "      rays_in = jax.tree_map(lambda x: x[pid], rays_in)\n",
        "\n",
        "      substeps = np.arange(start=step, stop=step + config.substeps, step=1)\n",
        "\n",
        "      # mip-NeRF scale annealing.\n",
        "      decays = config.mipnerf.decay_start * (\n",
        "          1 - substeps / config.mipnerf.decay_iters)\n",
        "      scs = np.maximum(1., 2**decays)\n",
        "\n",
        "      # Sigma noise annealing.\n",
        "      sns = schedule.sigma_noise_std_fn(\n",
        "          substeps, i_split=config.sn_i_split, sn0=config.sn0, sn1=config.sn1)\n",
        "\n",
        "      # Scene bounds annealing.\n",
        "      mrs = schedule.mask_rad_fn(\n",
        "          substeps, i_split=config.mr_i_split, mr0=config.mr0, mr1=config.mr1)\n",
        "\n",
        "      # Anneal target opacity (1 - transmittance).\n",
        "      accts = schedule.anneal_exponentially(substeps, config.acc_target_i_split,\n",
        "                                            config.acc_target0,\n",
        "                                            config.acc_target1)\n",
        "      # The area of an object on the image plane grows with the focal length\n",
        "      # and shrinks with increasing camera radius. Scale target opacity\n",
        "      # proportionally with the squared focal multiplier and inversely\n",
        "      # proportionally with the squared camera radius. For consistency with\n",
        "      # early experiments that did not use this scaling, we also scale by a\n",
        "      # constant, 1 / (4^2 * 1.2).\n",
        "      acct_scaling = focal_mult**2 / ((rad / 4.)**2) / 1.2\n",
        "      accts = np.minimum(1., acct_scaling * accts)\n",
        "      acclams = np.where(substeps < config.acc_lam_after, 0., config.acc_lam)\n",
        "\n",
        "      # Learning rate schedule.\n",
        "      # NOTE: vectorized calculation of lrs doesn't work with multiple substeps\n",
        "      lrs = schedule.lr_fn(\n",
        "          substeps,\n",
        "          i_split=config.lr_i_split,\n",
        "          i_end=config.iters,\n",
        "          lr0=config.lr0,\n",
        "          lr1=config.lr1,\n",
        "          lr2=config.lr2,\n",
        "          cosine_decay=config.lr_cosine_decay)\n",
        "\n",
        "      return substeps, rays_in, lrs, scs, sns, mrs, accts, acclams\n",
        "\n",
        "    pbar = tqdm.trange(\n",
        "        step_init,\n",
        "        config.iters + config.substeps,\n",
        "        config.substeps,\n",
        "        desc='training')\n",
        "    for i in pbar:\n",
        "      substeps, rays_in, lrs, scs, sns, mrs, accts, acclams = (\n",
        "          sample_iter_data(rng.advance(1), i))\n",
        "      l = substeps[-1]\n",
        "\n",
        "      keys_pstep = rng.split(n_device)\n",
        "      # NOTE: loss is averaged across substeps.\n",
        "      new_state, augs, mean_losses, new_scene_origin = train_pstep(\n",
        "          state, rays_in, keys_pstep, lrs, scs, sns, mrs, accts, acclams)\n",
        "\n",
        "      # Reduce across devices\n",
        "      mean_losses = jax.tree_map(np.mean, mean_losses)\n",
        "\n",
        "      # Gradient skipping if nan.\n",
        "      if (helpers.all_finite_tree(mean_losses) and\n",
        "          helpers.all_finite_tree(new_state)):\n",
        "        state = new_state\n",
        "      else:\n",
        "        logging.warn('Skipping update on step %d. non-finite loss or state', i)\n",
        "        continue\n",
        "\n",
        "      # Update scene origin.\n",
        "      if (config.get('ema_scene_origin', False) and\n",
        "          l > config.get('fix_origin_iters', -1)):\n",
        "        if helpers.all_finite(new_scene_origin):\n",
        "          scene_origin.update(new_scene_origin[0])\n",
        "        else:\n",
        "          logging.warn(\n",
        "              'Skipping origin update on step %d. '\n",
        "              'non-finite origin. old: %s skipped update: %s', i,\n",
        "              scene_origin.value, new_scene_origin)\n",
        "\n",
        "      ## Yield results, for display in colab.\n",
        "      augs = augs.reshape(-1, *augs.shape[2:])  # devices, n_localaug, HWC->BHWC\n",
        "      if yield_results:\n",
        "        yield mean_losses, augs, scene_origin.value, lrs[-1]\n",
        "      else:\n",
        "        yield None\n",
        "      pbar.set_description(f'CLIP loss: {mean_losses[\"clip_loss\"]:.4f}')\n",
        "\n",
        "      ## Logging.\n",
        "      if i == 0:\n",
        "        continue\n",
        "\n",
        "      if i % config.render_every == 0:\n",
        "        variables = helpers.state_to_variables(state)\n",
        "        cam2world = scene.pose_spherical(30., -45., 4.)\n",
        "        rays = scene.camera_ray_batch(cam2world, *hwf_clip_r)\n",
        "\n",
        "        # Render with no scale manipulation.\n",
        "        outs = render_loop(rays, variables, sc=1., mr=mrs[-1], hq=True)\n",
        "        outs = [np.squeeze(x) for x in outs]\n",
        "        step_images = {\n",
        "            'render/rgb': outs[0][None],\n",
        "            'render/depth': outs[1][None, ..., None],\n",
        "            'render/acc': outs[2][None, ..., None],\n",
        "            'render/augmentations': log.make_image_grid(augs[:8])\n",
        "        }\n",
        "\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(1. / np.maximum(config.near, outs[1]))\n",
        "        plt.colorbar()\n",
        "        plt.title('disparity')\n",
        "        disparity = log.plot_to_image(fig)\n",
        "        step_images['render/disparity'] = disparity\n",
        "\n",
        "      if config.render_lq_video and (i == config.iters or\n",
        "          config.video_every and i % config.video_every == 0):\n",
        "        def rays_theta(th):\n",
        "          cam2world = scene.pose_spherical(th, -30., 4.)\n",
        "          return scene.camera_ray_batch(cam2world, *hwf_video)\n",
        "\n",
        "        th_range = np.linspace(\n",
        "            0, 360, config.get('lq_video_n_frames', 60), endpoint=False)\n",
        "        variables = helpers.state_to_variables(state)\n",
        "        frames_all = [\n",
        "            render_loop(rays_theta(th), variables, scs[-1], mrs[-1])\n",
        "            for th in tqdm.tqdm(th_range, desc='render video')\n",
        "        ]\n",
        "\n",
        "        videos = [[np.squeeze(f[i]) for f in frames_all] for i in range(3)]\n",
        "        for video, label in zip(videos, 'rgb depth acc'.split()):\n",
        "          scale = (label == 'depth')\n",
        "          log.log_video(\n",
        "              None, video, 'frames', label, l, work_unit_dir, scale=scale)\n",
        "\n",
        "      defrag_every = config.get('defragment_every', default=0)\n",
        "      if defrag_every and i % defrag_every == 0:\n",
        "        helpers.defragment()\n",
        "\n",
        "      if config.get('checkpoint_every') and i % config.checkpoint_every == 0:\n",
        "        saved_path = checkpoints.save_checkpoint(\n",
        "            ckpt_dir=work_unit_dir,\n",
        "            target={\n",
        "                'state': flax.jax_utils.unreplicate(state),\n",
        "                'vars': helpers.state_to_variables(state),\n",
        "                'origin': np.array(scene_origin.value),\n",
        "            },\n",
        "            step=l,\n",
        "            keep=1,\n",
        "            overwrite=True,\n",
        "            keep_every_n_steps=config.get('keep_every_n_steps', None))\n",
        "        logging.info('saved checkpoint to %s', saved_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NN3LHPaYORI"
      },
      "source": [
        "# Visualization helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx7bGgCFwKgB"
      },
      "outputs": [],
      "source": [
        "def show_videos(work_unit_dir, title):\n",
        "  video_iters = config.iters\n",
        "  fps = config.lq_video_n_frames / 2\n",
        "  media.show_video(onp.concatenate([\n",
        "      media.read_video(os.path.join(work_unit_dir, f'frames_rgb_{video_iters:05d}.mp4')),\n",
        "      media.read_video(os.path.join(work_unit_dir, f'frames_acc_{video_iters:05d}.mp4')),\n",
        "      media.read_video(os.path.join(work_unit_dir, f'frames_depth_{video_iters:05d}.mp4')),\n",
        "  ], axis=2), title=title, fps=fps)\n",
        "\n",
        "\n",
        "def plot_losses(losses, log_interval=1):\n",
        "  steps = np.arange(len(losses['loss'])) * log_interval\n",
        "\n",
        "  keys = {\n",
        "    'loss': 'Total loss',\n",
        "    'clip_loss': 'CLIP negative cosine sim',\n",
        "    'grad_norm': 'Gradient norm',\n",
        "    'acc_mean': 'Avg transmittance',\n",
        "    'origin_norm': 'Origin drift',\n",
        "    'learning_rate': 'Learning rate',\n",
        "  }\n",
        "\n",
        "  _, axes = plt.subplots(1, len(keys), figsize=(len(losses) * 3, 3))\n",
        "  for i, (key, label) in enumerate(keys.items()):\n",
        "    axes[i].set_title(label)\n",
        "    values = onp.array(losses[key])\n",
        "    if key == 'acc_mean':\n",
        "      values = 1 - values  # Opacity to transmittance\n",
        "    axes[i].plot(steps, values)\n",
        "    if key in ('grad_norm', 'learning_rate'):\n",
        "      axes[i].set_yscale('log')\n",
        "  plt.tight_layout()\n",
        "  #plt.show()\n",
        "  #plt.savefig(f\"{output_path}/loss_{step:05}.jpg\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "def show_image_row(augs, step):\n",
        "  A, H, W, C = augs.shape\n",
        "  augs = augs.transpose(1, 0, 2, 3)\n",
        "  augs = augs.reshape((H, A*W, C))\n",
        "\n",
        "  plt.figure(figsize=(8*A, 8))\n",
        "  plt.imshow(augs)\n",
        "  #plt.show()\n",
        "  plt.savefig(f\"{output_path}/frame_{step:05}.jpg\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "def show_image_grid(images, ncol=8):\n",
        "  \"\"\"Plot a grid of images.\"\"\"\n",
        "  images_stacked = onp.array(images)\n",
        "  T = len(images)\n",
        "  nrow = int(onp.ceil(T / ncol))\n",
        "  H, W, C = images[0].shape\n",
        "  dtype = images[0].dtype\n",
        "\n",
        "  canvas = onp.zeros((H*nrow, W*ncol, C), dtype=dtype)\n",
        "  for i, image in enumerate(images):\n",
        "    row, col = i // ncol, i % ncol\n",
        "    canvas[row*H:row*H+H, col*W:col*W+W, :] = image\n",
        "\n",
        "  plt.figure(figsize=(2*ncol, 2*nrow))\n",
        "  plt.imshow(canvas)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhqN8qhSGCFE"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y matplotlib\n",
        "!pip install matplotlib==3.1.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf3MJgJkGHjS"
      },
      "source": [
        "# Run training\n",
        "100-200 iterations may be required before the scene begins to fill with visible content.\n",
        "\n",
        "The above configuration should work with 15 GB of GPU memory (e.g. Tesla T4). **If you run out of memory, tweak the configuration options above.** For example, try:\n",
        "*   num_samples = 48\n",
        "*   n_local_aug = 3\n",
        "\n",
        "You can also try scaling up parameters like resolution and n_local_aug for improved quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrCmSqTIAor-"
      },
      "outputs": [],
      "source": [
        "if use_gpu:\n",
        "  # Monitor memory usage\n",
        "  !nvidia-smi\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.config.experimental.set_visible_devices([], 'GPU')\n",
        "\n",
        "model = DreamField(config)\n",
        "losses = defaultdict(list)\n",
        "images = []\n",
        "log_interval = 2\n",
        "for step, (loss, image, origin, lr) in enumerate(model.run_train(\n",
        "    experiment_dir=experiment_dir, work_unit_dir=work_unit_dir, rng=rng,\n",
        "    yield_results=True)):\n",
        "  \n",
        "  if step % log_interval == 0:\n",
        "    losses['origin_norm'].append(np.sqrt(np.sum(np.square(origin))))\n",
        "    for key, value in loss.items():\n",
        "      losses[key].append(value)\n",
        "    losses['learning_rate'].append(lr)\n",
        "\n",
        "  if step % 25 == 0:\n",
        "    clear_output(wait=True)\n",
        "    show_image_row(image[:2], step)  # display up to 4 current augs\n",
        "    plot_losses(losses, log_interval)\n",
        "\n",
        "    # Plot past renders\n",
        "    images.append(image[0])  # limit to 1 aug for history\n",
        "    # show_image_grid(images)\n",
        "\n",
        "    if use_gpu:\n",
        "      # Monitor memory usage\n",
        "      !nvidia-smi\n",
        "\n",
        "    if step % 100 != 0:\n",
        "      # Remove last iter's render so we don't plot too much\n",
        "      images.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1m7YNq4eu73"
      },
      "outputs": [],
      "source": [
        "#show_videos(work_unit_dir, title=config.query)\n",
        "!cp -v /content/*.mp4 {output_path}/video.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDbXnfniDw_k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GgSvskIfJn9I",
        "AEj5tFx4Jmbh",
        "ZLjBuwrkGFkQ",
        "f3Gy03snwOmU",
        "5NN3LHPaYORI"
      ],
      "machine_shape": "hm",
      "name": "Zero Shot Text Guided Object Generation with Dream Fields, lower memory.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
