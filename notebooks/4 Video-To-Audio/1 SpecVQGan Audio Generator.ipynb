{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gsX4IjZFQOO"
      },
      "source": [
        "<img src=\"https://pollinations.ai/ipfs/QmRfRzENotGosgNiaaXs6LMW4ieSGiMV1ZkdsvPHToVGqG\" />\n",
        "\n",
        "Taming Visually Guided Sound Generation üñºÔ∏è üëâ üîâ\n",
        "\n",
        "This notebook will guide you through the visually-guided sound generation. We will condition the generation on a set of visual frames extracted from an arbitrary video.\n",
        "\n",
        "[Project Page](https://v-iashin.github.io/SpecVQGAN) ‚Ä¢ [Code](https://github.com/v-iashin/SpecVQGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input video file\n",
        "video_file = '' #@param {type: \"string\"}\n",
        "\n",
        "output_path = \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTmo9M6gFQOP"
      },
      "source": [
        "## Imports and Device Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl6u14lNFQOQ",
        "outputId": "94c8dc17-b110-41e9-8b8e-c3d4b0e38166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/SpecVQGAN\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import os\n",
        "    import time\n",
        "    from pathlib import Path\n",
        "    \n",
        "    import IPython.display as display_audio\n",
        "    import soundfile\n",
        "    import torch\n",
        "    from IPython import display\n",
        "    from matplotlib import pyplot as plt\n",
        "    from torch.utils.data.dataloader import default_collate\n",
        "    from torchvision.utils import make_grid\n",
        "    from tqdm import tqdm\n",
        "    import streamlit\n",
        "    \n",
        "    %cd SpecVQGAN\n",
        "    \n",
        "    from feature_extraction.demo_utils import (ExtractResNet50,\n",
        "                                               extract_melspectrogram, load_model,\n",
        "                                               show_grid, trim_video)\n",
        "    from sample_visualization import (all_attention_to_st, get_class_preditions,\n",
        "                                      last_attention_to_st, spec_to_audio_to_st,\n",
        "                                      tensor_to_plt)\n",
        "    from specvqgan.data.vggsound import CropImage\n",
        "\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "except ModuleNotFoundError:\n",
        "    # Cloning the repo from GitHub\n",
        "    !git clone https://github.com/v-iashin/SpecVQGAN\n",
        "    print('Some packages are not installed. Installing...')\n",
        "    # Installing the environment\n",
        "    !pip uninstall torchtext -y # otherwise fails on PytorchLightning import\n",
        "    !pip install pytorch-lightning==1.2.10 omegaconf==2.0.6 streamlit==0.80 matplotlib==3.4.1 albumentations==0.5.2\n",
        "    # We need to restart Colab Runtime because we installed new packages\n",
        "    !for i in {1..20}; do echo \"Packages have been installed. Please rerun the cell.\"; done\n",
        "    raise Exception('Some packages are not installed. Please rerun.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXWNFxmdFQOS"
      },
      "source": [
        "## Select a Model\n",
        "The model will be automatically downloaded given the `model_name`. \n",
        "Use the following reference to select one:\n",
        "\n",
        "|                               Model Name |                  Info | FID ‚Üì | Avg. MKL ‚Üì | Sample TimeÔ∏è ‚Üì |\n",
        "| ---------------------------------------: | --------------------: | ----: | ---------: | ------------: |\n",
        "| 2021-06-20T16-35-20_vggsound_transformer |              No Feats |  13.5 |        9.7 |           7.7 |\n",
        "| 2021-07-30T21-03-22_vggsound_transformer |    1 ResNet50 Feature |  11.5 |        7.3 |           7.7 |\n",
        "| 2021-07-30T21-34-25_vggsound_transformer |   5 ResNet50 Features |  11.3 |        7.0 |           7.9 |\n",
        "| 2021-07-30T21-34-41_vggsound_transformer | 212 ResNet50 Features |  10.5 |        6.9 |          11.8 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeJarWQuFQOT",
        "outputId": "3c801835-5bdb-4e1d-99f3-a18d5619070e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: 2021-07-30T21-34-25_vggsound_transformer (5 ResNet50 Features)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3.68GB [03:29, 17.5MB/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unpacking ./logs/2021-07-30T21-34-25_vggsound_transformer.tar.gz to ./logs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-10-19 09:13:25.481 WARNING root: \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "2021-10-19 09:13:30.487 INFO    specvqgan.modules.transformer.mingpt: number of parameters: 3.046851e+08\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading melception model from https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/specvqgan_public/melception-21-05-10T09-28-40.pt to evaluation/logs/21-05-10T09-28-40/melception-21-05-10T09-28-40.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "281MB [00:16, 16.7MB/s]                           \n"
          ]
        }
      ],
      "source": [
        "model_name = '2021-07-30T21-34-25_vggsound_transformer'\n",
        "log_dir = './logs'\n",
        "# loading the models might take a few minutes\n",
        "config, sampler, melgan, melception = load_model(model_name, log_dir, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAvWGNB7FQOV"
      },
      "source": [
        "## Select a Video\n",
        "We extract visual features and display the corresponding frames.\n",
        "\n",
        "**Note**: the selected video is trimmed to 10 seconds.\n",
        "By default, we use the first 10 seconds: adjust `start_sec` if you'd like to \n",
        "start from another time.\n",
        "If the video is shorter than 10 sec it will be tiled until 10 sec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def get_length(filename):\n",
        "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
        "                             \"format=duration\", \"-of\",\n",
        "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT)\n",
        "    return float(result.stdout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62-T8P-SFQOV"
      },
      "outputs": [],
      "source": [
        "from math import floor\n",
        "\n",
        "# Select a video\n",
        "video_path = video_file\n",
        "\n",
        "# Calculate the length of the video\n",
        "video_sec = get_length(video_path)\n",
        "\n",
        "print(\"Video has\", video_sec, \"seconds.\")\n",
        "\n",
        "# Trim the video\n",
        "start_sec = max(video_sec - 10, 0)  # to start with 01:35 use 95 seconds\n",
        "video_path = trim_video(video_path, start_sec, trim_duration=10)\n",
        "\n",
        "# Extract Features\n",
        "extraction_fps = 21.5\n",
        "feature_extractor = ExtractResNet50(extraction_fps, config.data.params, device)\n",
        "visual_features, resampled_frames = feature_extractor(video_path)\n",
        "\n",
        "\n",
        "\n",
        "# Prepare Input\n",
        "batch = default_collate([visual_features])\n",
        "batch['feature'] = batch['feature'].to(device)\n",
        "c = sampler.get_input(sampler.cond_stage_key, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy-Ej3ZNFQOW"
      },
      "source": [
        "## Codebook Reconstruction of the Input Spectrogram\n",
        "\n",
        "If the video has an audio, it will extract mel-spectrogram from the \n",
        "audio track using the same pre-processing pipeline as in our experiments.\n",
        "This is also useful if you plan to prime the sampling with a half of \n",
        "the ground truth codes.\n",
        "\n",
        "This cell can be ignored if the video doesn't have any audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml7oKwY7FQOW"
      },
      "outputs": [],
      "source": [
        "# Extract Spectrogram\n",
        "audio_fps = 22050\n",
        "\n",
        "spectrogram = {\"input\":torch.zeros((config.data.params.mel_num,config.data.params.spec_crop_len))}\n",
        "\n",
        "# Prepare input\n",
        "batch = default_collate([spectrogram])\n",
        "batch['image'] = batch['input'].to(device)\n",
        "x = sampler.get_input(sampler.first_stage_key, batch)\n",
        "\n",
        "# Encode and Decode the Spectrogram\n",
        "with torch.no_grad():\n",
        "    quant_z, z_indices = sampler.encode_to_z(x)\n",
        "    xrec = sampler.first_stage_model.decode(quant_z)\n",
        "\n",
        "print('Original Spectrogram:')\n",
        "display.display(tensor_to_plt(x, flip_dims=(2,)))\n",
        "print('Reconstructed Spectrogram:')\n",
        "display.display(tensor_to_plt(xrec, flip_dims=(2,)))\n",
        "plt.close()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpNj-iPeFQOX"
      },
      "source": [
        "## Sample a New Sound Given a Condition\n",
        "\n",
        "Feel free to adjust the parameters:\n",
        "- `W_scale (int > 0)`: the output sound duration will be `W_scale * 9.8 seconds`\n",
        "- `mode (str ['half', 'full'])`: the sampling mode:\n",
        "    - `full`: the sampling starts with conditioning-only\n",
        "    - `half`: the sampling will be primed with the first half of the quantized spectrogram representation \n",
        "- `temperature (float != 0)`: softmax temperature. $T$ in $\\frac{\\exp(x_i/T)} {\\Sigma_j \\exp(x_j/T)}$\n",
        "- `top_x (int [0, codebook_size])`: the distribution of the next token is cliped to Top-`top_x` codes of the codebook\n",
        "- `update_every (int >= 0)`: how often to print progress (slows down the sampling speed)\n",
        "- `full_att_mat (bool)`: \n",
        "    - `True`: shows attention at each step at once\n",
        "    - `False`: shows attention at the current step only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYIQR3hfFQOY"
      },
      "outputs": [],
      "source": [
        "# Define Sampling Parameters\n",
        "from math import ceil\n",
        "W_scale = 1.0 #ceil(video_sec / 9.8)\n",
        "mode = 'full'\n",
        "temperature = 1.0\n",
        "top_x = sampler.first_stage_model.quantize.n_e // 2\n",
        "update_every = 0  # use > 0 value, e.g. 15, to see the progress of generation (slows down the sampling speed)\n",
        "full_att_mat = True\n",
        "\n",
        "# Start sampling\n",
        "with torch.no_grad():\n",
        "    start_t = time.time()\n",
        "\n",
        "    quant_c, c_indices = sampler.encode_to_c(c)\n",
        "    # crec = sampler.cond_stage_model.decode(quant_c)\n",
        "\n",
        "    patch_size_i = 5\n",
        "    patch_size_j = 53\n",
        "\n",
        "    B, D, hr_h, hr_w = sampling_shape = (1, 256, 5, floor(53*W_scale))\n",
        "\n",
        "    if mode == 'full':\n",
        "        start_step = 0\n",
        "    else:\n",
        "        start_step = (patch_size_j // 2) * patch_size_i\n",
        "\n",
        "    z_pred_indices = torch.zeros((B, hr_h*hr_w)).long().to(device)\n",
        "    z_pred_indices[:, :start_step] = z_indices[:, :start_step]\n",
        "\n",
        "    pbar = tqdm(range(start_step, hr_w * hr_h), desc='Sampling Codebook Indices')\n",
        "    for step in pbar:\n",
        "        i = step % hr_h\n",
        "        j = step // hr_h\n",
        "\n",
        "        i_start = min(max(0, i - (patch_size_i // 2)), hr_h - patch_size_i)\n",
        "        j_start = min(max(0, j - (patch_size_j // 2)), hr_w - patch_size_j)\n",
        "        i_end = i_start + patch_size_i\n",
        "        j_end = j_start + patch_size_j\n",
        "\n",
        "        local_i = i - i_start\n",
        "        local_j = j - j_start\n",
        "\n",
        "        patch_2d_shape = (B, D, patch_size_i, patch_size_j)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            Step=f'({i},{j}) | Local: ({local_i},{local_j}) | Crop: ({i_start}:{i_end},{j_start}:{j_end})'\n",
        "        )\n",
        "\n",
        "        patch = z_pred_indices \\\n",
        "            .reshape(B, hr_w, hr_h) \\\n",
        "            .permute(0, 2, 1)[:, i_start:i_end, j_start:j_end].permute(0, 2, 1) \\\n",
        "            .reshape(B, patch_size_i * patch_size_j)\n",
        "\n",
        "        # assuming we don't crop the conditioning and just use the whole c, if not desired uncomment the above\n",
        "        cpatch = c_indices\n",
        "        logits, _, attention = sampler.transformer(patch[:, :-1], cpatch)\n",
        "        # remove conditioning\n",
        "        logits = logits[:, -patch_size_j*patch_size_i:, :]\n",
        "\n",
        "        local_pos_in_flat = local_j * patch_size_i + local_i\n",
        "        logits = logits[:, local_pos_in_flat, :]\n",
        "\n",
        "        logits = logits / temperature\n",
        "        logits = sampler.top_k_logits(logits, top_x)\n",
        "\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # sample from the distribution\n",
        "        ix = torch.multinomial(probs, num_samples=1)\n",
        "        z_pred_indices[:, j * hr_h + i] = ix\n",
        "\n",
        "        if update_every > 0 and step % update_every == 0:\n",
        "            z_pred_img = sampler.decode_to_img(z_pred_indices, sampling_shape)\n",
        "            # fliping the spectrogram just for illustration purposes (low freqs to bottom, high - top)\n",
        "            # z_pred_img_st = tensor_to_plt(z_pred_img, flip_dims=(2,))\n",
        "            display.clear_output(wait=True)\n",
        "            # display.display(z_pred_img_st)\n",
        "\n",
        "            quant_z_shape = sampling_shape\n",
        "            c_length = cpatch.shape[-1]\n",
        "            quant_c_shape = quant_c.shape\n",
        "            c_att_plot, z_att_plot = last_attention_to_st(\n",
        "                attention, local_pos_in_flat, c_length, sampler.first_stage_permuter,\n",
        "                sampler.cond_stage_permuter, quant_c_shape, patch_2d_shape,\n",
        "                placeholders=None, flip_c_dims=None, flip_z_dims=(2,))\n",
        "\n",
        "\n",
        "    # quant_z_shape = sampling_shape\n",
        "    z_pred_img = sampler.decode_to_img(z_pred_indices, sampling_shape)\n",
        "\n",
        "    # showing the final image\n",
        "    # z_pred_img_st = tensor_to_plt(z_pred_img, flip_dims=(2,))\n",
        "    display.clear_output(wait=True)\n",
        "    # display.display(z_pred_img_st)\n",
        "\n",
        "\n",
        "    quant_z_shape = sampling_shape\n",
        "    c_length = cpatch.shape[-1]\n",
        "    quant_c_shape = quant_c.shape\n",
        "    c_att_plot, z_att_plot = last_attention_to_st(\n",
        "        attention, local_pos_in_flat, c_length, sampler.first_stage_permuter,\n",
        "        sampler.cond_stage_permuter, quant_c_shape, patch_2d_shape,\n",
        "        placeholders=None, flip_c_dims=None, flip_z_dims=(2,)\n",
        "    )\n",
        "    # display.display(c_att_plot)\n",
        "    # display.display(z_att_plot)\n",
        "    # plt.close()\n",
        "    # plt.close()\n",
        "    # plt.close()\n",
        "\n",
        "    print(f'Sampling Time: {time.time() - start_t:3.2f} seconds')\n",
        "    waves = spec_to_audio_to_st(z_pred_img, config.data.params.spec_dir_path,\n",
        "                                config.data.params.sample_rate, show_griffin_lim=False,\n",
        "                                vocoder=melgan, show_in_st=False)\n",
        "    print(f'Sampling Time (with vocoder): {time.time() - start_t:3.2f} seconds')\n",
        "    print(f'Generated: {len(waves[\"vocoder\"]) / config.data.params.sample_rate:.2f} seconds')\n",
        "\n",
        "    # Melception opinion on the class distribution of the generated sample\n",
        "    topk_preds = get_class_preditions(z_pred_img, melception)\n",
        "    print(topk_preds)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "audio = np.array(waves[\"vocoder\"])\n",
        "normalized_audio = audio / np.max(np.abs(audio))\n",
        "\n",
        "save_path = os.path.join(output_path, 'audio.wav')\n",
        "\n",
        "soundfile.write(save_path, normalized_audio, config.data.params.sample_rate, 'PCM_24')\n",
        "print(f'The sample has been saved @ {save_path}')\n",
        "# display_audio.Audio(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add generated sound to the video\n",
        "!ffmpeg -i {video_path} -i {save_path} -c:v copy -c:a aac -strict experimental -map 0:v:0 -map 1:a:0 -shortest {output_path}/video.mp4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Video-To-Audio - SpecVQGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
