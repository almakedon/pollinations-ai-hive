{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP-Conditioned CLIP-Guided Diffusion (cc12m_1, 256x256)",
      "provenance": [],
      "collapsed_sections": [
        "tWgrsecxuFmz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/interesting_notebooks/CLIP_Conditioned_CLIP_Guided_Diffusion_(cc12m_1%2C_256x256).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [CLIP-Conditioned CLIP-Guided Diffusion (cc12m_1, 256x256)](https://github.com/crowsonkb/v-diffusion-pytorch)\n",
        "\n",
        "By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
        "\n",
        "and JD Pressman (https://twitter.com/jd_pressman).\n",
        "\n",
        "Notebook by BoneAmputee (https://twitter.com/BoneAmputee)."
      ],
      "metadata": {
        "id": "ISWlIVtq7Oui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "tWgrsecxuFmz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7c8Jo0Yqb_d"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy\n",
        "%cd /content/\n",
        "!git clone https://github.com/crowsonkb/v-diffusion-pytorch.git\n",
        "%cd v-diffusion-pytorch\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "%mkdir -p checkpoints\n",
        "%mkdir -p frames\n",
        "!curl -L \"https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1.pth\" > \"checkpoints/cc12m_1.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "a053QXsbuIcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "\"\"\"CLIP guided sampling from a diffusion model.\"\"\"\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import trange\n",
        "from IPython import display\n",
        "from shutil import rmtree\n",
        "import os\n",
        "\n",
        "from CLIP import clip\n",
        "from diffusion import get_model, get_models, utils\n",
        "\n",
        "MODULE_DIR = Path(\"/content/v-diffusion-pytorch/\").resolve()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, check_in, eta, extra_args):\n",
        "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    alphas, sigmas = utils.t_to_alpha_sigma(steps)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(len(steps)):\n",
        "\n",
        "        # Get the model output (v, the predicted velocity)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            v = model(x, ts * steps[i], **extra_args).float()\n",
        "\n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        if i % check_in == 0:\n",
        "          outfile = f'frames/{str(i).zfill(4)}.png'\n",
        "          utils.to_pil_image(pred).save(outfile)\n",
        "          display.display(display.Image(outfile))\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < len(steps) - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def cond_sample(model, x, steps, check_in, eta, extra_args, cond_fn):\n",
        "    \"\"\"Draws guided samples from a model given starting noise.\"\"\"\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    alphas, sigmas = utils.t_to_alpha_sigma(steps)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(len(steps)):\n",
        "\n",
        "        # Get the model output\n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                v = model(x, ts * steps[i], **extra_args)\n",
        "\n",
        "            if steps[i] < 1:\n",
        "                pred = x * alphas[i] - v * sigmas[i]\n",
        "                if i % check_in == 0:\n",
        "                  outfile = f'frames/{str(i).zfill(4)}.png'\n",
        "                  utils.to_pil_image(pred).save(outfile)\n",
        "                  display.display(display.Image(outfile))\n",
        "                cond_grad = cond_fn(x, ts * steps[i], pred, **extra_args).detach()\n",
        "                v = v.detach() - cond_grad * (sigmas[i] / alphas[i])\n",
        "            else:\n",
        "                v = v.detach()\n",
        "\n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < len(steps) - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutout = F.adaptive_avg_pool2d(cutout, self.cut_size)\n",
        "            cutouts.append(cutout)\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    #@markdown `prompts`: the text prompts to use. Relative weights for text prompts can be specified by putting the weight after a colon. The vertical bar character can be used to denote multiple prompts.\n",
        "    prompts = \"an armchair in the shape of an avocado|conceptual art\" #@param {type:\"string\"}\n",
        "    #@markdown `batch_size`: sample this many images at a time (default 1)\n",
        "    batch_size = 1 #@param {type:\"integer\"}\n",
        "    #@markdown `checkpoint`: manually specify the model checkpoint file\n",
        "    checkpoint = \"\" #@param {type:\"string\"}\n",
        "    #@markdown `clip_guidance_scale`: how strongly the result should match the text prompt (default 500). If set to 0, the cc12m_1 model will still be CLIP conditioned and sampling will go faster and use less memory.\n",
        "    clip_guidance_scale = 500 #@param {type:\"number\"}\n",
        "    #@markdown `device`: the PyTorch device name to use (default autodetects)\n",
        "    device = \"cuda:0\" #@param {type:\"string\"}\n",
        "    #@markdown `eta`: set to 0 for deterministic (DDIM) sampling, 1 (the default) for stochastic (DDPM) sampling, and in between to interpolate between the two. DDIM is preferred for low numbers of timesteps.\n",
        "    eta = 1.0 #@param {type:\"number\"}\n",
        "    #@markdown `images`: the image prompts to use (local files or HTTP(S) URLs). Relative weights for image prompts can be specified by putting the weight after a colon, for example: `\"image_1.png:0.5\"`.\n",
        "    images = \"\" #@param {type:\"string\"}\n",
        "    #@markdown `model`: specify the model to use (default cc12m_1)\n",
        "    model = \"cc12m_1\" #@param {type:\"string\"}\n",
        "    #@markdown `n`: sample until this many images are sampled (default 1)\n",
        "    n = 1 #@param {type:\"integer\"}\n",
        "    #@markdown `seed`: specify the random seed (default 0)\n",
        "    seed = 0 #@param {type:\"integer\"}\n",
        "    #@markdown `steps`: specify the number of diffusion timesteps (default is 1000, can lower for faster but lower quality sampling)\n",
        "    steps = 1000 #@param {type:\"integer\"}\n",
        "    #@markdown `check_in`: specify the number of steps between each image update\n",
        "    check_in = 100 #@param {type:\"integer\"}\n",
        "    #@markdown `cutn`: specify the number of cuts to observe when guiding\n",
        "    cutn = 16 #@param {type:\"integer\"}\n",
        "    #@markdown `cut_pow`: specify the cut power\n",
        "    cut_pow = 1.0 #@param {type:\"number\"}\n",
        "    #@markdown `width`: specify the width\n",
        "    width = 256 #@param {type:\"integer\"}\n",
        "    #@markdown `height`: specify the height\n",
        "    height = 256 #@param {type:\"integer\"}\n",
        "\n",
        "    prompts = [x.strip() for x in prompts.split('|')]\n",
        "    prompts = [x for x in prompts if x != '']\n",
        "    images = [x.strip() for x in images.split('|')]\n",
        "    images = [x for x in images if x != '']\n",
        "\n",
        "    args = argparse.Namespace(\n",
        "      prompts = prompts,\n",
        "      batch_size = batch_size,\n",
        "      checkpoint = checkpoint,\n",
        "      clip_guidance_scale = clip_guidance_scale,\n",
        "      device = device,\n",
        "      eta = eta,\n",
        "      images = images,\n",
        "      model = model,\n",
        "      n = n,\n",
        "      seed = seed,\n",
        "      steps = steps,\n",
        "      check_in = check_in,\n",
        "      cutn = cutn,\n",
        "      cut_pow = cut_pow,\n",
        "      width = width,\n",
        "      height = height\n",
        "    )\n",
        "\n",
        "    if args.device:\n",
        "        device = torch.device(args.device)\n",
        "    else:\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Using device:', device)\n",
        "\n",
        "    model = get_model(args.model)()\n",
        "    # _, side_y, side_x = model.shape\n",
        "    side_y, side_x = (args.height//64)*64, (args.width//64)*64\n",
        "    checkpoint = args.checkpoint\n",
        "    if not checkpoint:\n",
        "        checkpoint = MODULE_DIR / f'checkpoints/{args.model}.pth'\n",
        "    model.load_state_dict(torch.load(checkpoint, map_location='cpu'))\n",
        "    if device.type == 'cuda':\n",
        "        model = model.half()\n",
        "    model = model.to(device).eval().requires_grad_(False)\n",
        "    clip_model = clip.load(model.clip_model, jit=False, device=device)[0]\n",
        "    clip_model.eval().requires_grad_(False)\n",
        "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                     std=[0.26862954, 0.26130258, 0.27577711])\n",
        "    cutn = args.cutn\n",
        "    make_cutouts = MakeCutouts(clip_model.visual.input_resolution, cutn=cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "    target_embeds, weights = [], []\n",
        "\n",
        "    for prompt in args.prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in args.images:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(utils.fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size),\n",
        "                        transforms.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img)[None].to(device))\n",
        "        embeds = F.normalize(clip_model.encode_image(normalize(batch)).float(), dim=-1)\n",
        "        target_embeds.append(embeds)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "    if not target_embeds:\n",
        "        raise RuntimeError('At least one text or image prompt must be specified.')\n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    clip_embed = F.normalize(target_embeds.mul(weights[:, None]).sum(0, keepdim=True), dim=-1)\n",
        "    clip_embed = clip_embed.repeat([args.n, 1])\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    def cond_fn(x, t, pred, clip_embed):\n",
        "        clip_in = normalize(make_cutouts((pred + 1) / 2))\n",
        "        image_embeds = clip_model.encode_image(clip_in).view([cutn, x.shape[0], -1])\n",
        "        losses = spherical_dist_loss(image_embeds, clip_embed[None])\n",
        "        loss = losses.mean(0).sum() * args.clip_guidance_scale\n",
        "        grad = -torch.autograd.grad(loss, x)[0]\n",
        "        return grad\n",
        "\n",
        "    def run(x, clip_embed):\n",
        "        t = torch.linspace(1, 0, args.steps + 1, device=device)[:-1]\n",
        "        steps = utils.get_spliced_ddpm_cosine_schedule(t)\n",
        "        extra_args = {'clip_embed': clip_embed}\n",
        "        if not args.clip_guidance_scale:\n",
        "            return sample(model, x, steps, args.check_in, args.eta, extra_args)\n",
        "        return cond_sample(model, x, steps, args.check_in, args.eta, extra_args, cond_fn)\n",
        "\n",
        "    def run_all(n, batch_size):\n",
        "        x = torch.randn([args.n, 3, side_y, side_x], device=device)\n",
        "        for i in trange(0, n, batch_size):\n",
        "            cur_batch_size = min(n - i, batch_size)\n",
        "            outs = run(x[i:i+cur_batch_size], clip_embed[i:i+cur_batch_size])\n",
        "            for j, out in enumerate(outs):\n",
        "                outfile = f'out_{i + j:05}.png'\n",
        "                utils.to_pil_image(out).save(outfile)\n",
        "                display.display(display.Image(outfile))\n",
        "\n",
        "    try:\n",
        "        run_all(args.n, args.batch_size)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if os.path.exists(\"frames\"):\n",
        "      rmtree(\"frames\")\n",
        "    os.makedirs(\"frames\")\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "h-hZ1MbgvR0Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r0hhpsuv0NIz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}