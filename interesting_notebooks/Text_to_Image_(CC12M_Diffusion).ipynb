{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text to Image (CC12M Diffusion).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/Text_to_Image_(CC12M_Diffusion).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generates images from text prompts.\n",
        "\n",
        "By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses a 602M parameter diffusion model trained on Conceptual 12M. See the GitHub repo for more information: https://github.com/crowsonkb/v-diffusion-pytorch."
      ],
      "metadata": {
        "id": "sThFl4fJtEQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "cellView": "form",
        "id": "eb9FKu1rtH6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa4Er6nMgI1b"
      },
      "outputs": [],
      "source": [
        "# Check the GPU\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies (no need to rerun this section if you restart the notebook runtime)"
      ],
      "metadata": {
        "id": "0XknUUiEsPCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "\n",
        "!pip install ftfy regex requests tqdm\n",
        "!git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch"
      ],
      "metadata": {
        "id": "xQVAY5-4g8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the diffusion model\n",
        "# SHA-256: 4fc95ee1b3205a3f7422a07746383776e1dbc367eaf06a5b658ad351e77b7bda\n",
        "\n",
        "!mkdir v-diffusion-pytorch/checkpoints\n",
        "!curl -L https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth > v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth"
      ],
      "metadata": {
        "id": "9410kZ2aipVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import modules and load models"
      ],
      "metadata": {
        "id": "tbjtOS68sazS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import gc\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "import torch\n",
        "from torchvision import utils as tv_utils\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "sys.path.append('/content/v-diffusion-pytorch')\n",
        "\n",
        "from CLIP import clip\n",
        "from diffusion import get_model, sampling, utils"
      ],
      "metadata": {
        "id": "f_6OhF85i5vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the models\n",
        "\n",
        "model = get_model('cc12m_1_cfg')()\n",
        "_, side_y, side_x = model.shape\n",
        "model.load_state_dict(torch.load('v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth', map_location='cpu'))\n",
        "model = model.half().cuda().eval().requires_grad_(False)\n",
        "clip_model = clip.load(model.clip_model, jit=False, device='cpu')[0]"
      ],
      "metadata": {
        "id": "v7Zl0mg1jrBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "cellView": "form",
        "id": "kitsmKsekOJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actually do the run..."
      ],
      "metadata": {
        "id": "kuTFyUdbrlcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_embed = clip_model.encode_text(clip.tokenize(prompt)).float().cuda()\n",
        "\n",
        "\n",
        "def cfg_model_fn(x, t):\n",
        "    \"\"\"The CFG wrapper function.\"\"\"\n",
        "    n = x.shape[0]\n",
        "    x_in = x.repeat([2, 1, 1, 1])\n",
        "    t_in = t.repeat([2])\n",
        "    clip_embed_repeat = target_embed.repeat([n, 1])\n",
        "    clip_embed_in = torch.cat([torch.zeros_like(clip_embed_repeat), clip_embed_repeat])\n",
        "    v_uncond, v_cond = model(x_in, t_in, clip_embed_in).chunk(2, dim=0)\n",
        "    v = v_uncond + (v_cond - v_uncond) * weight\n",
        "    return v\n",
        "\n",
        "\n",
        "def display_callback(info):\n",
        "    if info['i'] % display_every == 0:\n",
        "        nrow = math.ceil(info['pred'].shape[0]**0.5)\n",
        "        grid = tv_utils.make_grid(info['pred'], nrow, padding=0)\n",
        "        tqdm.write(f'Step {info[\"i\"]} of {steps}:')\n",
        "        display.display(utils.to_pil_image(grid))\n",
        "        tqdm.write(f'')\n",
        "\n",
        "\n",
        "def run():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.manual_seed(seed)\n",
        "    x = torch.randn([n_images, 3, side_y, side_x], device='cuda')\n",
        "    t = torch.linspace(1, 0, steps + 1, device='cuda')[:-1]\n",
        "    step_list = utils.get_spliced_ddpm_cosine_schedule(t)\n",
        "    outs = sampling.plms_sample(cfg_model_fn, x, step_list, {}, callback=display_callback)\n",
        "    tqdm.write('Done!')\n",
        "    for i, out in enumerate(outs):\n",
        "        filename = f'out_{i}.png'\n",
        "        utils.to_pil_image(out).save(filename)\n",
        "        display.display(display.Image(filename))\n",
        "\n",
        "\n",
        "run()"
      ],
      "metadata": {
        "id": "MnjPM_PUkYui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}