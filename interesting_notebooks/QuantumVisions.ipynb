{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumVisions.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/interesting_notebooks/QuantumVisions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOd34BtkuK63"
      },
      "source": [
        "## QuantumVisions\n",
        "Derived from DirectVisions by Jens Goldberg / [Aransentisssdd(https://https://twitter.com/aransentin)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLzYneEMEdEp",
        "cellView": "form"
      },
      "source": [
        "#@title color quantization for init images\n",
        "class Color(object):\n",
        "    \"\"\"\n",
        "    Color class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, red=0, green=0, blue=0, alpha=None):\n",
        "        \"\"\"\n",
        "        Initialize color\n",
        "        \"\"\"\n",
        "        self.red = int(red)\n",
        "        self.green = int(green)\n",
        "        self.blue = int(blue)\n",
        "        self.alpha = int(alpha)if alpha is not None else None\n",
        "class OctreeNode(object):\n",
        "    \"\"\"\n",
        "    Octree Node class for color quantization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, level, parent):\n",
        "        \"\"\"\n",
        "        Init new Octree Node\n",
        "        \"\"\"\n",
        "        self.color = Color(0, 0, 0)\n",
        "        self.pixel_count = 0\n",
        "        self.palette_index = 0\n",
        "        self.children = [None for _ in range(8)]\n",
        "        # add node to current level\n",
        "        if level < OctreeQuantizer.MAX_DEPTH - 1:\n",
        "            parent.add_level_node(level, self)\n",
        "\n",
        "    def is_leaf(self):\n",
        "        \"\"\"\n",
        "        Check that node is leaf\n",
        "        \"\"\"\n",
        "        return self.pixel_count > 0\n",
        "\n",
        "    def get_leaf_nodes(self):\n",
        "        \"\"\"\n",
        "        Get all leaf nodes\n",
        "        \"\"\"\n",
        "        leaf_nodes = []\n",
        "        for i in range(8):\n",
        "            node = self.children[i]\n",
        "            if node:\n",
        "                if node.is_leaf():\n",
        "                    leaf_nodes.append(node)\n",
        "                else:\n",
        "                    leaf_nodes.extend(node.get_leaf_nodes())\n",
        "        return leaf_nodes\n",
        "\n",
        "    def get_nodes_pixel_count(self):\n",
        "        \"\"\"\n",
        "        Get a sum of pixel count for node and its children\n",
        "        \"\"\"\n",
        "        sum_count = self.pixel_count\n",
        "        for i in range(8):\n",
        "            node = self.children[i]\n",
        "            if node:\n",
        "                sum_count += node.pixel_count\n",
        "        return sum_count\n",
        "\n",
        "    def add_color(self, color, level, parent):\n",
        "        \"\"\"\n",
        "        Add `color` to the tree\n",
        "        \"\"\"\n",
        "        if level >= OctreeQuantizer.MAX_DEPTH:\n",
        "            self.color.red += color.red\n",
        "            self.color.green += color.green\n",
        "            self.color.blue += color.blue\n",
        "            self.pixel_count += 1\n",
        "            return\n",
        "        index = self.get_color_index_for_level(color, level)\n",
        "        if not self.children[index]:\n",
        "            self.children[index] = OctreeNode(level, parent)\n",
        "        self.children[index].add_color(color, level + 1, parent)\n",
        "\n",
        "    def get_palette_index(self, color, level):\n",
        "        \"\"\"\n",
        "        Get palette index for `color`\n",
        "        Uses `level` to go one level deeper if the node is not a leaf\n",
        "        \"\"\"\n",
        "        if self.is_leaf():\n",
        "            return self.palette_index\n",
        "        index = self.get_color_index_for_level(color, level)\n",
        "        if self.children[index]:\n",
        "            return self.children[index].get_palette_index(color, level + 1)\n",
        "        else:\n",
        "            # get palette index for a first found child node\n",
        "            for i in range(8):\n",
        "                if self.children[i]:\n",
        "                    return self.children[i].get_palette_index(color, level + 1)\n",
        "\n",
        "    def remove_leaves(self):\n",
        "        \"\"\"\n",
        "        Add all children pixels count and color channels to parent node \n",
        "        Return the number of removed leaves\n",
        "        \"\"\"\n",
        "        result = 0\n",
        "        for i in range(8):\n",
        "            node = self.children[i]\n",
        "            if node:\n",
        "                self.color.red += node.color.red\n",
        "                self.color.green += node.color.green\n",
        "                self.color.blue += node.color.blue\n",
        "                self.pixel_count += node.pixel_count\n",
        "                result += 1\n",
        "        return result - 1\n",
        "\n",
        "    def get_color_index_for_level(self, color, level):\n",
        "        \"\"\"\n",
        "        Get index of `color` for next `level`\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        mask = 0x80 >> level\n",
        "        if color.red & mask:\n",
        "            index |= 4\n",
        "        if color.green & mask:\n",
        "            index |= 2\n",
        "        if color.blue & mask:\n",
        "            index |= 1\n",
        "        return index\n",
        "\n",
        "    def get_color(self):\n",
        "        \"\"\"\n",
        "        Get average color\n",
        "        \"\"\"\n",
        "        return Color(\n",
        "            self.color.red / self.pixel_count,\n",
        "            self.color.green / self.pixel_count,\n",
        "            self.color.blue / self.pixel_count)\n",
        "\n",
        "\n",
        "class OctreeQuantizer(object):\n",
        "    \"\"\"\n",
        "    Octree Quantizer class for image color quantization\n",
        "    Use MAX_DEPTH to limit a number of levels\n",
        "    \"\"\"\n",
        "\n",
        "    MAX_DEPTH = 8\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Init Octree Quantizer\n",
        "        \"\"\"\n",
        "        self.levels = {i: [] for i in range(OctreeQuantizer.MAX_DEPTH)}\n",
        "        self.root = OctreeNode(0, self)\n",
        "\n",
        "    def get_leaves(self):\n",
        "        \"\"\"\n",
        "        Get all leaves\n",
        "        \"\"\"\n",
        "        return [node for node in self.root.get_leaf_nodes()]\n",
        "\n",
        "    def add_level_node(self, level, node):\n",
        "        \"\"\"\n",
        "        Add `node` to the nodes at `level`\n",
        "        \"\"\"\n",
        "        self.levels[level].append(node)\n",
        "\n",
        "    def add_color(self, color):\n",
        "        \"\"\"\n",
        "        Add `color` to the Octree\n",
        "        \"\"\"\n",
        "        # passes self value as `parent` to save nodes to levels dict\n",
        "        self.root.add_color(color, 0, self)\n",
        "\n",
        "    def make_palette(self, color_count):\n",
        "        \"\"\"\n",
        "        Make color palette with `color_count` colors maximum\n",
        "        \"\"\"\n",
        "        palette = []\n",
        "        palette_index = 0\n",
        "        leaf_count = len(self.get_leaves())\n",
        "        # reduce nodes\n",
        "        # up to 8 leaves can be reduced here and the palette will have\n",
        "        # only 248 colors (in worst case) instead of expected 256 colors\n",
        "        print(\"creating palette...\")\n",
        "        for level in range(OctreeQuantizer.MAX_DEPTH - 1, -1, -1):\n",
        "            if self.levels[level]:\n",
        "                for node in self.levels[level]:\n",
        "                    leaf_count -= node.remove_leaves()\n",
        "                    if leaf_count <= color_count:\n",
        "                        break\n",
        "                if leaf_count <= color_count:\n",
        "                    break\n",
        "                self.levels[level] = []\n",
        "        # build palette\n",
        "        for node in self.get_leaves():\n",
        "            if palette_index >= color_count:\n",
        "                break\n",
        "            if node.is_leaf():\n",
        "                palette.append(node.get_color())\n",
        "            node.palette_index = palette_index\n",
        "            palette_index += 1\n",
        "        return palette\n",
        "\n",
        "    def get_palette_index(self, color):\n",
        "        \"\"\"\n",
        "        Get palette index for `color`\n",
        "        \"\"\"\n",
        "        return self.root.get_palette_index(color, 0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sinkhorn distance for histogram loss\n",
        "! pip install Ninja\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import torch.utils.cpp_extension\n",
        "cuda_source = \"\"\"\n",
        "\n",
        "#include <torch/extension.h>\n",
        "#include <ATen/core/TensorAccessor.h>\n",
        "#include <ATen/cuda/CUDAContext.h>\n",
        "\n",
        "using at::RestrictPtrTraits;\n",
        "using at::PackedTensorAccessor;\n",
        "\n",
        "#if defined(__HIP_PLATFORM_HCC__)\n",
        "constexpr int WARP_SIZE = 64;\n",
        "#else\n",
        "constexpr int WARP_SIZE = 32;\n",
        "#endif\n",
        "\n",
        "// The maximum number of threads in a block\n",
        "#if defined(__HIP_PLATFORM_HCC__)\n",
        "constexpr int MAX_BLOCK_SIZE = 256;\n",
        "#else\n",
        "constexpr int MAX_BLOCK_SIZE = 512;\n",
        "#endif\n",
        "\n",
        "// Returns the index of the most significant 1 bit in `val`.\n",
        "__device__ __forceinline__ int getMSB(int val) {\n",
        "  return 31 - __clz(val);\n",
        "}\n",
        "\n",
        "// Number of threads in a block given an input size up to MAX_BLOCK_SIZE\n",
        "static int getNumThreads(int nElem) {\n",
        "#if defined(__HIP_PLATFORM_HCC__)\n",
        "  int threadSizes[5] = { 16, 32, 64, 128, MAX_BLOCK_SIZE };\n",
        "#else\n",
        "  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };\n",
        "#endif\n",
        "  for (int i = 0; i != 5; ++i) {\n",
        "    if (nElem <= threadSizes[i]) {\n",
        "      return threadSizes[i];\n",
        "    }\n",
        "  }\n",
        "  return MAX_BLOCK_SIZE;\n",
        "}\n",
        "\n",
        "\n",
        "template <typename T>\n",
        "__device__ __forceinline__ T WARP_SHFL_XOR(T value, int laneMask, int width = warpSize, unsigned int mask = 0xffffffff)\n",
        "{\n",
        "#if CUDA_VERSION >= 9000\n",
        "    return __shfl_xor_sync(mask, value, laneMask, width);\n",
        "#else\n",
        "    return __shfl_xor(value, laneMask, width);\n",
        "#endif\n",
        "}\n",
        "\n",
        "// While this might be the most efficient sinkhorn step / logsumexp-matmul implementation I have seen,\n",
        "// this is awfully inefficient compared to matrix multiplication and e.g. NVidia cutlass may provide\n",
        "// many great ideas for improvement\n",
        "template <typename scalar_t, typename index_t>\n",
        "__global__ void sinkstep_kernel(\n",
        "  // compute log v_bj = log nu_bj - logsumexp_i 1/lambda dist_ij - log u_bi\n",
        "  // for this compute maxdiff_bj = max_i(1/lambda dist_ij - log u_bi)\n",
        "  // i = reduction dim, using threadIdx.x\n",
        "  PackedTensorAccessor<scalar_t, 2, RestrictPtrTraits, index_t> log_v,\n",
        "  const PackedTensorAccessor<scalar_t, 2, RestrictPtrTraits, index_t> dist,\n",
        "  const PackedTensorAccessor<scalar_t, 2, RestrictPtrTraits, index_t> log_nu,\n",
        "  const PackedTensorAccessor<scalar_t, 2, RestrictPtrTraits, index_t> log_u,\n",
        "  const scalar_t lambda) {\n",
        "\n",
        "  using accscalar_t = scalar_t;\n",
        "\n",
        "  __shared__ accscalar_t shared_mem[2 * WARP_SIZE];\n",
        "\n",
        "  index_t b = blockIdx.y;\n",
        "  index_t j = blockIdx.x;\n",
        "  int tid = threadIdx.x;\n",
        "\n",
        "  if (b >= log_u.size(0) || j >= log_v.size(1)) {\n",
        "    return;\n",
        "  }\n",
        "  // reduce within thread\n",
        "  accscalar_t max = -std::numeric_limits<accscalar_t>::infinity();\n",
        "  accscalar_t sumexp = 0;\n",
        "  \n",
        "  if (log_nu[b][j] == -std::numeric_limits<accscalar_t>::infinity()) {\n",
        "    if (tid == 0) {\n",
        "      log_v[b][j] = -std::numeric_limits<accscalar_t>::infinity();\n",
        "    }\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  for (index_t i = threadIdx.x; i < log_u.size(1); i += blockDim.x) {\n",
        "    accscalar_t oldmax = max;\n",
        "    accscalar_t value = -dist[i][j]/lambda + log_u[b][i];\n",
        "    max = max > value ? max : value;\n",
        "    if (oldmax == -std::numeric_limits<accscalar_t>::infinity()) {\n",
        "      // sumexp used to be 0, so the new max is value and we can set 1 here,\n",
        "      // because we will come back here again\n",
        "      sumexp = 1;\n",
        "    } else {\n",
        "      sumexp *= exp(oldmax - max);\n",
        "      sumexp += exp(value - max); // if oldmax was not -infinity, max is not either...\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // now we have one value per thread. we'll make it into one value per warp\n",
        "  // first warpSum to get one value per thread to\n",
        "  // one value per warp\n",
        "  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n",
        "    accscalar_t o_max    = WARP_SHFL_XOR(max, 1 << i, WARP_SIZE);\n",
        "    accscalar_t o_sumexp = WARP_SHFL_XOR(sumexp, 1 << i, WARP_SIZE);\n",
        "    if (o_max > max) { // we're less concerned about divergence here\n",
        "      sumexp *= exp(max - o_max);\n",
        "      sumexp += o_sumexp;\n",
        "      max = o_max;\n",
        "    } else if (max != -std::numeric_limits<accscalar_t>::infinity()) {\n",
        "      sumexp += o_sumexp * exp(o_max - max);\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  __syncthreads();\n",
        "  // this writes each warps accumulation into shared memory\n",
        "  // there are at most WARP_SIZE items left because\n",
        "  // there are at most WARP_SIZE**2 threads at the beginning\n",
        "  if (tid % WARP_SIZE == 0) {\n",
        "    shared_mem[tid / WARP_SIZE * 2] = max;\n",
        "    shared_mem[tid / WARP_SIZE * 2 + 1] = sumexp;\n",
        "  }\n",
        "  __syncthreads();\n",
        "  if (tid < WARP_SIZE) {\n",
        "    max = (tid < blockDim.x / WARP_SIZE ? shared_mem[2 * tid] : -std::numeric_limits<accscalar_t>::infinity());\n",
        "    sumexp = (tid < blockDim.x / WARP_SIZE ? shared_mem[2 * tid + 1] : 0);\n",
        "  }\n",
        "  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n",
        "    accscalar_t o_max    = WARP_SHFL_XOR(max, 1 << i, WARP_SIZE);\n",
        "    accscalar_t o_sumexp = WARP_SHFL_XOR(sumexp, 1 << i, WARP_SIZE);\n",
        "    if (o_max > max) { // we're less concerned about divergence here\n",
        "      sumexp *= exp(max - o_max);\n",
        "      sumexp += o_sumexp;\n",
        "      max = o_max;\n",
        "    } else if (max != -std::numeric_limits<accscalar_t>::infinity()) {\n",
        "      sumexp += o_sumexp * exp(o_max - max);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (tid == 0) {\n",
        "    log_v[b][j] = (max > -std::numeric_limits<accscalar_t>::infinity() ?\n",
        "                   log_nu[b][j] - log(sumexp) - max : \n",
        "                   -std::numeric_limits<accscalar_t>::infinity());\n",
        "  }\n",
        "}\n",
        "\n",
        "template <typename scalar_t>\n",
        "torch::Tensor sinkstep_cuda_template(const torch::Tensor& dist, const torch::Tensor& log_nu, const torch::Tensor& log_u,\n",
        "                                     const double lambda) {\n",
        "  TORCH_CHECK(dist.is_cuda(), \"need cuda tensors\");\n",
        "  TORCH_CHECK(dist.device() == log_nu.device() && dist.device() == log_u.device(), \"need tensors on same GPU\");\n",
        "  TORCH_CHECK(dist.dim()==2 && log_nu.dim()==2 && log_u.dim()==2, \"invalid sizes\");\n",
        "  TORCH_CHECK(dist.size(0) == log_u.size(1) &&\n",
        "           dist.size(1) == log_nu.size(1) &&\n",
        "           log_u.size(0) == log_nu.size(0), \"invalid sizes\");\n",
        "  auto log_v = torch::empty_like(log_nu);\n",
        "  using index_t = int32_t;\n",
        "  \n",
        "  auto log_v_a = log_v.packed_accessor<scalar_t, 2, RestrictPtrTraits, index_t>();\n",
        "  auto dist_a = dist.packed_accessor<scalar_t, 2, RestrictPtrTraits, index_t>();\n",
        "  auto log_nu_a = log_nu.packed_accessor<scalar_t, 2, RestrictPtrTraits, index_t>();\n",
        "  auto log_u_a = log_u.packed_accessor<scalar_t, 2, RestrictPtrTraits, index_t>();\n",
        "  \n",
        "  auto stream = at::cuda::getCurrentCUDAStream();\n",
        "\n",
        "  int tf = getNumThreads(log_u.size(1));\n",
        "  dim3 blocks(log_v.size(1), log_u.size(0));\n",
        "  dim3 threads(tf);\n",
        "  \n",
        "  sinkstep_kernel<<<blocks, threads, 2*WARP_SIZE*sizeof(scalar_t), stream>>>(\n",
        "    log_v_a, dist_a, log_nu_a, log_u_a, static_cast<scalar_t>(lambda)\n",
        "    );\n",
        "\n",
        "  return log_v;\n",
        "}\n",
        "\n",
        "torch::Tensor sinkstep_cuda(const torch::Tensor& dist, const torch::Tensor& log_nu, const torch::Tensor& log_u,\n",
        "                            const double lambda) {\n",
        "    return AT_DISPATCH_FLOATING_TYPES(log_u.scalar_type(), \"sinkstep\", [&] {\n",
        "       return sinkstep_cuda_template<scalar_t>(dist, log_nu, log_u, lambda);\n",
        "    });\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"sinkstep\", &sinkstep_cuda, \"sinkhorn step\");\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "wasserstein_ext = torch.utils.cpp_extension.load_inline(\"wasserstein\", cpp_sources=\"\", cuda_sources=cuda_source,\n",
        "                                                    extra_cuda_cflags=[\"--expt-relaxed-constexpr\"], build_directory = \".\"   )\n",
        "\n",
        "def sinkstep(dist, log_nu, log_u, lam: float):\n",
        "    # dispatch to optimized GPU implementation for GPU tensors, slow fallback for CPU\n",
        "    if dist.is_cuda:\n",
        "        return wasserstein_ext.sinkstep(dist, log_nu, log_u, lam)\n",
        "    assert dist.dim() == 2 and log_nu.dim() == 2 and log_u.dim() == 2\n",
        "    assert dist.size(0) == log_u.size(1) and dist.size(1) == log_nu.size(1) and log_u.size(0) == log_nu.size(0)\n",
        "    log_v = log_nu.clone()\n",
        "    for b in range(log_u.size(0)):\n",
        "        log_v[b] -= torch.logsumexp(-dist/lam+log_u[b, :, None], 0)\n",
        "    return log_v\n",
        "\n",
        "class SinkhornOT(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, mu, nu, dist, lam=1e-3, N=100):\n",
        "        assert mu.dim() == 2 and nu.dim() == 2 and dist.dim() == 2\n",
        "        bs = mu.size(0)\n",
        "        d1, d2 = dist.size()\n",
        "        assert nu.size(0) == bs and mu.size(1) == d1 and nu.size(1) == d2\n",
        "        log_mu = mu.log()\n",
        "        log_nu = nu.log()\n",
        "        log_u = torch.full_like(mu, -math.log(d1))\n",
        "        log_v = torch.full_like(nu, -math.log(d2))\n",
        "        for i in range(N):\n",
        "            log_v = sinkstep(dist, log_nu, log_u, lam)\n",
        "            log_u = sinkstep(dist.t(), log_mu, log_v, lam)\n",
        "\n",
        "        # this is slight abuse of the function. it computes (diag(exp(log_u))*Mt*exp(-Mt/lam)*diag(exp(log_v))).sum()\n",
        "        # in an efficient (i.e. no bxnxm tensors) way in log space\n",
        "        distances = (-sinkstep(-dist.log()+dist/lam, -log_v, log_u, 1.0)).logsumexp(1).exp()\n",
        "        ctx.log_v = log_v\n",
        "        ctx.log_u = log_u\n",
        "        ctx.dist = dist\n",
        "        ctx.lam = lam\n",
        "        return distances\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):\n",
        "        return grad_out[:, None] * ctx.log_u * ctx.lam, grad_out[:, None] * ctx.log_v * ctx.lam, None, None, None\n",
        "\n",
        "def get_coupling(mu, nu, dist, lam=1e-3, N=1000):\n",
        "    assert mu.dim() == 2 and nu.dim() == 2 and dist.dim() == 2\n",
        "    bs = mu.size(0)\n",
        "    d1, d2 = dist.size()\n",
        "    assert nu.size(0) == bs and mu.size(1) == d1 and nu.size(1) == d2\n",
        "    log_mu = mu.log()\n",
        "    log_nu = nu.log()\n",
        "    log_u = torch.full_like(mu, -math.log(d1))\n",
        "    log_v = torch.full_like(nu, -math.log(d2))\n",
        "    for i in range(N):\n",
        "        log_v = sinkstep(dist, log_nu, log_u, lam)\n",
        "        log_u = sinkstep(dist.t(), log_mu, log_v, lam)\n",
        "    return (log_v[:, None, :]-dist/lam+log_u[:, :, None]).exp()"
      ],
      "metadata": {
        "id": "QvQkxTHXlQ2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEGzL9rYK_-I"
      },
      "source": [
        "\n",
        "from numpy.core.numeric import False_\n",
        "from torch._C import LongStorageBase\n",
        "! nvidia-smi -L\n",
        "\n",
        "! rm -rf images\n",
        "# ! rm *.png\n",
        "! mkdir images\n",
        "\n",
        "# Input prompts. Each prompt has \"text\" and a \"weight\"\n",
        "# Weights can be negatives, useful for discouraging specific artifacts\n",
        "texts = [\n",
        "    {\n",
        "        \"text\": \"A mermaid eating sushi\",\n",
        "        \"weight\": 1.0,\n",
        "    },{ # Set to 1 for very pixel-art style images, or -1 for smoother, more natural images when using scaling mode = nearest\n",
        "        \"text\": \"#pixelart\",\n",
        "        \"weight\": 1.0,\n",
        "    # },{\n",
        "    #     \"text\": \"vivid CryEngine watercolor and pencil sketch, 8k resolution\",\n",
        "    #     \"weight\": 0.5,\n",
        "    # # },{\n",
        "    # },{\n",
        "    #     \"text\": \"Beautiful and detailed fantasy painting.\",\n",
        "    #     \"weight\": 0.2,\n",
        "    # # },{\n",
        "    # # #     \"text\": \"Full body.\",\n",
        "    # # #     \"weight\": 0.1,\n",
        "    # },{ # Improves contrast, object coherence, and adds a nice depth of field effect\n",
        "    #     \"text\": \"Rendered in unreal engine, trending on artstation.\",\n",
        "    #     \"weight\": 0.2,\n",
        "    # },{\n",
        "    #     \"text\": \"speedpainting matte painting\",\n",
        "    #     \"weight\": 0.2,\n",
        "    # # },{\n",
        "    # #     \"text\": \"Vivid Colors\",\n",
        "    # #     \"weight\": 0.15,\n",
        "    # },{ # Doesn't seem to do much, but also doesn't seem to hurt. \n",
        "    #     \"text\": \"confusing, incoherent\",\n",
        "    #     \"weight\": -0.25,\n",
        "    },{ # Not really strong enough to remove all signatures... but I'm ok with small ones\n",
        "        \"text\":\"text, signature\",\n",
        "        \"weight\":-1\n",
        "    }\n",
        "]\n",
        "\n",
        "#Image prompts\n",
        "images = [\n",
        "          # {\n",
        "          #     \"fpath\": \"waste.png\",\n",
        "          #     \"weight\": 0.2,\n",
        "          #     \"cuts\": 16,\n",
        "          #     \"noise\": 0.0\n",
        "          # },{\n",
        "          #     \"fpath\": \"waste_2.png\",\n",
        "          #     \"weight\": 0.2,\n",
        "          #     \"cuts\": 16,\n",
        "          #     \"noise\": 0.0\n",
        "          # }\n",
        "          ]\n",
        "\n",
        "# random seed\n",
        "# Set to None for random seed\n",
        "seed = None\n",
        "\n",
        "#starting noise \n",
        "pix_noise_scale = 2.0\n",
        "pix_noise_persistence = 0.8\n",
        "pix_noise_clamp = 50.0\n",
        "\n",
        "palette_noise_scale = 1.0\n",
        "palette_noise_clamp = 6.0\n",
        "palette_brightness = 0\n",
        "init_rgb = True\n",
        "\n",
        "use_transparent = True\n",
        "\n",
        "# Number of times to run\n",
        "images_n = 1\n",
        "\n",
        "# Save rate for video. Does slow down training if you set it low.\n",
        "save_interval = 1000\n",
        "\n",
        "# Use \"nearest\" for pixel-art style images with very precise edges and sharp corners\n",
        "# Use Lanczos for images with smoother edges and more natural shapes\n",
        "# Bicubic and bilinear are intermediate between the two\n",
        "pyramid_scaling_mode = \"nearest\" # \"lanczos\" #'bicubic' \"nearest\" \"bilinear\" \n",
        "\n",
        "# AdamW is real basic and gets the job done\n",
        "# RAdam seems to work *extremely well* but seems to introduce some color instability?, use 0.5x lr\n",
        "# Yogi is just really blurry for some reason, use 5x + lr\n",
        "# Ranger works great. use 3-4x LR\n",
        "optimizer_type = \"Ranger\" # \"AdamW\", \"AccSGD\",\"Ranger\",\"RangerQH\",\"RangerVA\",\"AdaBound\",\"AdaMod\",\"Adafactor\",\"AdamP\",\"AggMo\",\"DiffGrad\",\"Lamb\",\"NovoGrad\",\"PID\",\"QHAdam\",\"QHM\",\"RAdam\",\"SGDP\",\"SGDW\",\"Shampoo\",\"SWATS\",\"Yogi\"\n",
        "\n",
        "# Image that sets pixel values and color palette can be set independently\n",
        "initial_image = None #\"skeleton handd.png\"\n",
        "palette_image = None\n",
        "\n",
        "# How strong is the influence of the initial image? \n",
        "initial_image_strength = 4\n",
        "\n",
        "# Aspect ratio of the output image\n",
        "# Also sets the size of the smallest pyramid step, so make this as small as possible for your target shape\n",
        "aspect_ratio = (4, 4) # (3, 4)\n",
        "\n",
        "# Max dim of the final output image.\n",
        "max_dim = 800\n",
        "\n",
        "# number of colors to use.\n",
        "# overridden by manual palette\n",
        "# Use 2 for duotone / 1-bit images. \n",
        "# Or try a very large number of colors for a more painterly look\n",
        "num_colors = 4\n",
        "\n",
        "# Use this to define a manual palette (in hex codes)\n",
        "# Set to None to use a randomly initialized palette\n",
        "palette = None\n",
        "# [\"FDFDFD\", \"222222\", \"444444\", \"666666\", \"888888\", \"AAAAAA\", \"CCCCCC\", \"020202\"] \n",
        "# [\n",
        "#             \"614ED9\",\n",
        "#             \"405B73\",\n",
        "#             \"F2BC57\",\n",
        "#             \"F29727\",\n",
        "#             \"D97F30\",\n",
        "#             \"4FE3E4\",\n",
        "#             \"E6E7B9\",\n",
        "# ]\n",
        "\n",
        "# How to scale the palette before applying sigmoid\n",
        "# Higher = more extremes, lower = more subtle\n",
        "# Recommended = 0.1, haven't experimented with this much\n",
        "palette_contrast = 0.1\n",
        "\n",
        "# How many levels of detail to use. You want to have at least log_2 of your max dimension/aspect ratio\n",
        "# So for a 1024 max_dim and (3,4) aspect ratio use at least 8 (you can use fewer but it'll be less detailed probably)\n",
        "# Higher isn't necessarily better? Hard to tell exactly what effect more steps has.\n",
        "pyramid_steps = 11\n",
        "\n",
        "# Add an extra step to the pyramid with size (1, 1)\n",
        "# The model will have an easier time setting an overall color and brightness for the image, but may have less large-scale contrast over the image.\n",
        "add_global_color = True\n",
        "\n",
        "# Optimizer settings for different training steps\n",
        "stages = (\n",
        "            { #First stage does rough detail.\n",
        "        \"cuts\": 1,\n",
        "        \"cycles\": 1000,\n",
        "        # Higher pixel LR seems to result in less blending\n",
        "        \"lr_pixel\": 4.0, \n",
        "        # Some decay on the pixels helps keep them from saturating and getting stuck\n",
        "        \"decay_pixel\": 1e-5,\n",
        "        # Learning rate of the colors-- set to 0 to use a fixed palette\n",
        "        \"lr_palette\": 4.0, \n",
        "        \"decay_palette\": 0.0,\n",
        "        # Higher noise helps increase contrast and edge sharpness\n",
        "        \"noise\": 0.2,\n",
        "        # TV loss weight. You probably won't need this but if your image is too noisy this will smooth it out\n",
        "        # May result in large flat areas of a single color if set too high\n",
        "        \"denoise\": 0.0,\n",
        "        # Use this to control how much blending is allowed. Positive = no blending. Negative = blending encouraged.\n",
        "        \"uncertanty_loss_scale\": -0.1,#0.05, \n",
        "        # Histogram loss: not used currently\n",
        "        \"hist_loss_scale\": 2,\n",
        "        \"checkin_interval\": 100,\n",
        "        # Three different ways to set the scale of LR at each resolution level:\n",
        "        # \"lr_persistence\": 0.95, # Depends on number of pyramid levels\n",
        "        \"pyramid_lr_min\" : 0.5, # Independent of number of pyramid levels. Defaults to 1\n",
        "        # \"lr_scales\": [0.25,0.15,0.15,0.15,0.15,0.05,0.05,0.01,0.01], # manually set lr at each level\n",
        "    }, { # 2nd stage does fine detail and polish\n",
        "        # (If you change settings in stage 1 make sure to also change things here!)\n",
        "        \"cuts\": 2,\n",
        "        \"cycles\": 1000,\n",
        "        \"lr_pixel\": 1.0,\n",
        "        \"decay_pixel\": 1e-8,\n",
        "        \"lr_palette\": 1.0,\n",
        "        \"decay_palette\": 0.0,\n",
        "        \"noise\": 0.2,\n",
        "        \"denoise\": 0.00,\n",
        "        \"uncertanty_loss_scale\": 0,# 0.1,#0.2,\n",
        "        \"hist_loss_scale\": 2,\n",
        "        \"checkin_interval\": 100,\n",
        "        # \"lr_persistence\": 1.0,\n",
        "        # \"pyramid_lr_min\" : 1\n",
        "        # \"lr_scales\": [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1],\n",
        "    },\n",
        "    # You can add new stages, or remove stage 2; but I find that the two stages is often just right\n",
        ")\n",
        "\n",
        "# Experimental color histogram matching\n",
        "# Doesn't really work as intended \n",
        "hist_image = None #\"waste_4.png\"\n",
        "hist_bins = 100\n",
        "\n",
        "##### END OF CONFIG #####\n",
        "\n",
        "\n",
        "#Calculate layer dims\n",
        "if pyramid_steps > 1:\n",
        "  pyramid_lacunarity = (max_dim / max(aspect_ratio))**(1.0/(pyramid_steps-1))\n",
        "else:\n",
        "  pyramid_lacunarity = 1\n",
        "scales = [pyramid_lacunarity**step for step in range(pyramid_steps)]\n",
        "dims = []\n",
        "if add_global_color:\n",
        "  dims.append([1,1])\n",
        "for step in range(pyramid_steps):\n",
        "  scale = pyramid_lacunarity**step\n",
        "  dim = [int(round(aspect_ratio[0] * scale)), int(round(aspect_ratio[1] * scale))]\n",
        "  # Ensure that no two levels have the same dims\n",
        "  if len(dims) > 0:\n",
        "    if dim[0] <= dims[-1][0]:\n",
        "      dim[0] = dims[-1][0]+1\n",
        "    if dim[1] <= dims[-1][1]:\n",
        "      dim[1] = dims[-1][1]+1\n",
        "  dims.append(dim)\n",
        "print(dims)\n",
        "display_size = [i * 160 for i in aspect_ratio]\n",
        "pyramid_steps = len(dims)\n",
        "for stage in stages:\n",
        "  if \"lr_scales\" not in stage:\n",
        "    if \"lr_persistence\" in stage:\n",
        "      persistence = stage[\"lr_persistence\"]\n",
        "    elif \"pyramid_lr_min\" in stage:\n",
        "      if pyramid_steps > 1:\n",
        "        persistence = stage[\"pyramid_lr_min\"]**(1.0/float(pyramid_steps-1))\n",
        "      else:\n",
        "        persistence = 1\n",
        "    else:\n",
        "      persistence = 1.0  \n",
        "    lrs = [persistence**i for i in range(pyramid_steps)]\n",
        "    sum_lrs = sum(lrs)\n",
        "    stage[\"lr_scales\"] = [rate / sum_lrs for rate in lrs]\n",
        "    print(persistence, stage[\"lr_scales\"])\n",
        "\n",
        "if palette is not None:\n",
        "  num_colors = len(palette)\n",
        "\n",
        "debug_clip_cuts = False\n",
        "\n",
        "import sys, os, random, shutil, math\n",
        "import torch, torchvision\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "bilinear = torchvision.transforms.functional.InterpolationMode.BILINEAR\n",
        "bicubic = torchvision.transforms.functional.InterpolationMode.BICUBIC\n",
        "\n",
        "torch.autograd.set_grad_enabled(False)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "if seed is not None:\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  # torch.use_deterministic_algorithms(mode=False)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "if not os.path.isdir(\"CLIP\"):\n",
        "  ! pip -q install ftfy\n",
        "  ! git clone https://github.com/openai/CLIP.git --depth 1\n",
        "  ! pip -q install torch_optimizer\n",
        "\n",
        "from CLIP import clip\n",
        "import torch_optimizer as optim\n",
        "\n",
        "\n",
        "def normalize_image(image):\n",
        "  R = (image[:,0:1] - 0.48145466) /  0.26862954\n",
        "  G = (image[:,1:2] - 0.4578275) / 0.26130258 \n",
        "  B = (image[:,2:3] - 0.40821073) / 0.27577711\n",
        "  return torch.cat((R, G, B), dim=1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def loadImage(filename):\n",
        "  data = open(filename, \"rb\").read()\n",
        "  image = torch.ops.image.decode_png(torch.as_tensor(bytearray(data)).cpu().to(torch.uint8), 3).cuda().to(torch.float32) / 255.0\n",
        "  # image = normalize_image(image)\n",
        "  return image.unsqueeze(0).cuda()\n",
        "\n",
        "def getClipTokens(image, cuts, noise, do_checkin, perceptor):\n",
        "    im = normalize_image(image)\n",
        "    cut_data = torch.zeros(cuts, 3, perceptor[\"size\"], perceptor[\"size\"])\n",
        "    for c in range(cuts):\n",
        "      angle = random.uniform(-20.0, 20.0)\n",
        "      img = torchvision.transforms.functional.rotate(im, angle=angle, expand=True, interpolation=bilinear)\n",
        "\n",
        "      # if random.random() > 0.5:\n",
        "      #   img = torchvision.transforms.functional.vflip(img)\n",
        "                                                  \n",
        "      padv = im.size()[2] // 8\n",
        "      img = torch.nn.functional.pad(img, pad=(padv, padv, padv, padv))\n",
        "\n",
        "      size = img.size()[2:4]\n",
        "      mindim = min(*size)\n",
        "\n",
        "      if mindim <= perceptor[\"size\"]-32:\n",
        "        width = mindim - 1\n",
        "      else:\n",
        "        width = random.randint( perceptor[\"size\"]-32, mindim-1 )\n",
        "\n",
        "      oy = random.randrange(0, size[0]-width)\n",
        "      ox = random.randrange(0, size[1]-width)\n",
        "      img = img[:,:,oy:oy+width,ox:ox+width]\n",
        "\n",
        "      img = torch.nn.functional.interpolate(img, size=(perceptor[\"size\"], perceptor[\"size\"]), mode='bilinear', align_corners=False)\n",
        "      cut_data[c] = img\n",
        "    cut_data += noise * torch.randn_like(cut_data, requires_grad=False)\n",
        "\n",
        "    if debug_clip_cuts and do_checkin:\n",
        "      displayImage(cut_data)\n",
        "\n",
        "    clip_tokens = perceptor['model'].encode_image(cut_data)\n",
        "    return clip_tokens\n",
        "\n",
        "\n",
        "def loadPerceptor(name):\n",
        "  model, preprocess = clip.load(name, device=\"cuda\")\n",
        "\n",
        "  tokens = []\n",
        "  imgs = []\n",
        "  for text in texts:\n",
        "    tok = model.encode_text(clip.tokenize(text[\"text\"]).cuda())\n",
        "    tokens.append( tok )\n",
        "\n",
        "  perceptor = {\"model\":model, \"size\": preprocess.transforms[0].size, \"tokens\": tokens, }\n",
        "  for img in images:\n",
        "    image = loadImage(img[\"fpath\"])\n",
        "    tokens = getClipTokens(image, img[\"cuts\"], img[\"noise\"], False, perceptor )\n",
        "    imgs.append(tokens)\n",
        "  perceptor[\"images\"] = imgs\n",
        "  return perceptor\n",
        "\n",
        "perceptors = (\n",
        "  loadPerceptor(\"ViT-B/32\"),\n",
        "  loadPerceptor(\"ViT-B/16\"),\n",
        "  # loadPerceptor(\"RN50x16\"),\n",
        ")\n",
        "\n",
        "@torch.no_grad()\n",
        "def saveImage(image, filename):\n",
        "  # R = image[:,0:1] * 0.26862954 + 0.48145466\n",
        "  # G = image[:,1:2] * 0.26130258 + 0.4578275\n",
        "  # B = image[:,2:3] * 0.27577711 + 0.40821073\n",
        "  # image = torch.cat((R, G, B), dim=1)\n",
        "  size = image.size()\n",
        "\n",
        "  if use_transparent:\n",
        "    im_np = image.cpu().numpy() * 255\n",
        "    image = Image.fromarray(im_np, \"RGBA\")#torchvision.transforms.functional.to_pil_image(image, 'RGBA')\n",
        "    image.save(filename)\n",
        "  else:\n",
        "    image = (image[0].clamp(0, 1) * 255).to(torch.uint8)\n",
        "    png_data = torch.ops.image.encode_png(image.cpu(), 6)\n",
        "    open(filename, \"wb\").write(bytes(png_data))\n",
        "\n",
        "# TODO: Use torchvision normalize / unnormalize\n",
        "def unnormalize_image(image):\n",
        "  \n",
        "  R = image[:,0:1] * 0.26862954 + 0.48145466\n",
        "  G = image[:,1:2] * 0.26130258 + 0.4578275\n",
        "  B = image[:,2:3] * 0.27577711 + 0.40821073\n",
        "  \n",
        "  return torch.cat((R, G, B), dim=1)\n",
        "\n",
        "\n",
        "def paramsToImage(params_pyramid, params_palette, quantize = False):\n",
        "  pixels = torch.zeros_like(params_pyramid[-1])\n",
        "  out_dim = params_pyramid[-1].shape[2:]\n",
        "  for i in range(len(params_pyramid)):\n",
        "    if pyramid_scaling_mode == \"lanczos\" and not (params_pyramid[i].shape[2] == 1 and params_pyramid[i].shape[3] == 1):\n",
        "      pixels += resample(params_pyramid[i], params_pyramid[-1].shape[2:])\n",
        "    else:\n",
        "      if pyramid_scaling_mode == \"nearest\" or (params_pyramid[i].shape[2] == 1 and params_pyramid[i].shape[3] == 1):\n",
        "        pixels += torch.nn.functional.interpolate(params_pyramid[i], size=out_dim, mode=\"nearest\")\n",
        "      else:\n",
        "        pixels += torch.nn.functional.interpolate(params_pyramid[i], size=out_dim, mode=pyramid_scaling_mode, align_corners=True)\n",
        "    # pixels += torch.nn.functional.softmax(p, dim=1)\n",
        "  pixels = torch.nn.functional.softmax(pixels, dim=1)\n",
        "  max_val, _ = pixels.max(dim=1)\n",
        "  uncertanty = (1.0 - max_val).mean()\n",
        "  if quantize:\n",
        "    a = torch.argmax(pixels, dim=1)\n",
        "    p = torch.zeros_like(pixels)\n",
        "    p = p.scatter(1, a.unsqueeze(1), 1.0)\n",
        "    pixels = p\n",
        "\n",
        "  palette = torch.sigmoid(params_palette * palette_contrast)\n",
        "  # palette = params_palette.clamp(0.0, 1.0)\n",
        "  if use_transparent:\n",
        "    pixels = pixels.view(num_colors, -1)\n",
        "    pixels = torch.matmul(palette.T, pixels)\n",
        "    pixels = pixels.view((1, 4, *out_dim))\n",
        "  else:\n",
        "    pixels = pixels.view(num_colors, -1)\n",
        "    pixels = torch.matmul(palette.T, pixels)\n",
        "    pixels = pixels.view((1, 3, *out_dim))\n",
        "\n",
        "  return pixels, uncertanty\n",
        "\n",
        "def paletteToImage(params_palette):\n",
        "  p = torch.sigmoid(params_palette.T.unsqueeze(0).unsqueeze(2) * palette_contrast)\n",
        "  p = torch.nn.functional.interpolate(p, size=(64, max_dim), mode=\"nearest\")\n",
        "  return p\n",
        "\n",
        "def imageToParams(image, palette_only = False):\n",
        "  octree = OctreeQuantizer()\n",
        "  height, width = image.shape[-2:]\n",
        "  pixels = torch.zeros(1, num_colors, height, width)\n",
        "  for j in range(height):\n",
        "        for i in range(width):\n",
        "            octree.add_color(Color(*image[0,:, j, i] * 255))\n",
        "\n",
        "  palette = octree.make_palette(num_colors)\n",
        "\n",
        "  if not palette_only:\n",
        "    for j in range(height):\n",
        "        for i in range(width):\n",
        "            index = octree.get_palette_index(Color(*image[0, :, j, i] * 255))\n",
        "            pixels[0, index, j, i] = 1.0\n",
        "\n",
        "  palette_colors = torch.zeros((num_colors, 3))\n",
        "  for i, color in enumerate(palette):\n",
        "    palette_colors[i][0] = color.red\n",
        "    palette_colors[i][1] = color.green\n",
        "    palette_colors[i][2] = color.blue\n",
        "  palette_colors = torch.logit(palette_colors.float() / 255.0, eps = 1e-6) / palette_contrast\n",
        "  return pixels, palette_colors\n",
        "\n",
        "@torch.no_grad()\n",
        "def displayImage(image):\n",
        "  size = image.size()\n",
        "\n",
        "  width = size[0] * size[3] + (size[0]-1) * 4\n",
        "  image_row = torch.randint(256, size=(3, size[2], width)).to(torch.float32)\n",
        "\n",
        "  nw = 0\n",
        "  for n in range(size[0]):\n",
        "    if image.shape[1] == 4:\n",
        "      alpha = image[n, 3, :,:]\n",
        "      image_row[:,:,nw:nw+size[3]] *= 1.0 - alpha\n",
        "      image_row[:,:,nw:nw+size[3]] += image[n,:3,:,:].clamp(0, 1) * 255 * alpha\n",
        "    else:\n",
        "      image_row[:,:,nw:nw+size[3]] = image[n,:3,:,:].clamp(0, 1) * 255\n",
        "    nw += size[3] + 4\n",
        "\n",
        "  png_data = torch.ops.image.encode_png(image_row.to(torch.uint8).cpu(), 6)\n",
        "  image = display.Image(bytes(png_data))\n",
        "  display.display( image )\n",
        "\n",
        "def lossClip(image, cuts, noise, do_checkin):\n",
        "  losses = []\n",
        "\n",
        "  max_loss = 0.0\n",
        "  for text in texts:\n",
        "    max_loss += abs(text[\"weight\"]) * len(perceptors)\n",
        "  for img in images:\n",
        "    max_loss += abs(img[\"weight\"]) * len(perceptors)\n",
        "\n",
        "  for perceptor in perceptors:\n",
        "    clip_tokens = getClipTokens(image, cuts, noise, do_checkin, perceptor)\n",
        "    for t, tokens in enumerate( perceptor[\"tokens\"] ):\n",
        "      similarity = torch.cosine_similarity(tokens, clip_tokens)\n",
        "      weight = texts[t][\"weight\"]\n",
        "      if weight > 0.0:\n",
        "        loss = (1.0 - similarity) * weight\n",
        "      else:\n",
        "        loss = similarity * (-weight)\n",
        "      losses.append(loss / max_loss)\n",
        "\n",
        "    for img in images:\n",
        "      for i, prompt_image in enumerate(perceptor[\"images\"]):\n",
        "        img_tokens = prompt_image\n",
        "        weight = images[i][\"weight\"] / float(images[i][\"cuts\"])\n",
        "        for token in img_tokens:\n",
        "          similarity = torch.cosine_similarity(token.unsqueeze(0), clip_tokens)\n",
        "          if weight > 0.0:\n",
        "            loss = (1.0 - similarity) * weight\n",
        "          else:\n",
        "            loss = similarity * (-weight)\n",
        "          losses.append(loss / max_loss)\n",
        "  return losses\n",
        "\n",
        "def lossTV(image, strength):\n",
        "  Y = (image[:,:,1:,:] - image[:,:,:-1,:]).abs().mean()\n",
        "  X = (image[:,:,:,1:] - image[:,:,:,:-1]).abs().mean()\n",
        "  loss = (X + Y) * 0.5 * strength\n",
        "  return loss\n",
        "\n",
        "def lossHist(image, target_hist):\n",
        "  if target_hist is None:\n",
        "    return 0\n",
        "  img_hist = torch.histc(image, bins=hist_bins, min=0, max = 1)\n",
        "  img_hist /= img_hist.sum()\n",
        "  cost = (img_hist[None, :] - target_hist[:, None])**2\n",
        "  cost /= cost.max()\n",
        "  # print(cost.shape, img_hist.shape, target_hist.shape)\n",
        "  return sinkstep(cost, img_hist.log().unsqueeze(0), target_hist.log().unsqueeze(0), 1e-3)\n",
        "\n",
        "\n",
        "def cycle(c, stage, optimizer, params_pyramid, params_palette):\n",
        "  do_checkin = (c+1) % stage[\"checkin_interval\"] == 0 or c == 0\n",
        "  with torch.enable_grad():\n",
        "    losses = []\n",
        "    image, uncertanty = paramsToImage(params_pyramid, params_palette)\n",
        "    \n",
        "    # image = torchvision.transforms.functional.gaussian_blur(image, 5)\n",
        "    losses += lossClip( image, stage[\"cuts\"], stage[\"noise\"], do_checkin )\n",
        "    losses += [lossTV( image, stage[\"denoise\"] )]\n",
        "    losses += [uncertanty * stage[\"uncertanty_loss_scale\"]]\n",
        "    # losses += [lossHist(image, stage[\"target_hist\"]) * stage[\"hist_loss_scale\"]]\n",
        "\n",
        "    loss_total = sum(losses).sum()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss_total.backward(retain_graph=False)\n",
        "    # if c <= warmup_its:\n",
        "    #   optimizer.param_groups[0][\"lr\"] = stage[\"lr_luma\"] * c / warmup_its\n",
        "    #   optimizer.param_groups[1][\"lr\"] = stage[\"lr_chroma\"] * c / warmup_its\n",
        "    optimizer.step()\n",
        "\n",
        "  if (c+1) % save_interval == 0 or c == 0:\n",
        "    nimg, _ = paramsToImage(params_pyramid, params_palette)\n",
        "    saveImage(nimg, f\"images/frame_{stage['n']:02}_{c:05}.png\")\n",
        "  if do_checkin:\n",
        "    TV = losses[1].sum().item()\n",
        "    nimg, uncertanty = paramsToImage(params_pyramid, params_palette, quantize=False)\n",
        "    print( \"Cycle:\", str(stage[\"n\"]) + \":\" + str(c), \"CLIP Loss:\", losses[0].sum().item(), \"TV loss:\", TV, \"Uncertanty:\", uncertanty.item())#, \"hist loss:\", losses[3].item())\n",
        "    displayImage(paletteToImage(params_palette))\n",
        "    displayImage(torch.nn.functional.interpolate(nimg, size=display_size, mode='nearest'))\n",
        "    saveImage(nimg, texts[0][\"text\"] + f\"_{stage['n']}\" + \".png\" )\n",
        "    nimg, _ = paramsToImage(params_pyramid, params_palette, quantize = True)\n",
        "    displayImage(torch.nn.functional.interpolate(nimg, size=display_size, mode='nearest'))\n",
        "    saveImage(nimg, texts[0][\"text\"] + f\"_quantized_{stage['n']}\" + \".png\" )\n",
        "\n",
        "    # for i in range(len(dims)):\n",
        "    #   print(i, \"luma\", params_luma[i].min().item(), params_luma[i].max().item())\n",
        "    #   print(i, \"chroma\", params_chroma[i].min().item(), params_chroma[i].max().item())\n",
        "      \n",
        "    # for i in range(len(dims)):\n",
        "    #   if pyramid_scaling_mode == \"lanczos\":\n",
        "    #     nimg = paramsToImage([resample(params_luma[i], display_size)], [resample(params_chroma[i], display_size)])\n",
        "    #   else:\n",
        "    #     nimg = paramsToImage([params_luma[:i+1]], [params_chroma[:i+1]])\n",
        "    #     nimg = torch.nn.functional.interpolate(nimg, size=display_size, mode=pyramid_scaling_mode)\n",
        "    #   displayImage(nimg)\n",
        "    \n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.reshape([n * c, 1, h, w])\n",
        "\n",
        "    # if dh < h:\n",
        "    kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "    pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "    input = torch.nn.functional.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "    input = torch.nn.functional.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    # if dw < w:\n",
        "    kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "    pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "    input = torch.nn.functional.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "    input = torch.nn.functional.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.reshape([n, c, h, w])\n",
        "    return torch.nn.functional.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "def init_optim(params_pyramid, params_palette, stage):\n",
        "  lr_scales = stage[\"lr_scales\"]\n",
        "  params = []\n",
        "  for i in range(len(lr_scales)):\n",
        "    params.append({\"params\": params_pyramid[i], \"lr\":stage[\"lr_pixel\"] * lr_scales[i], \"weight_decay\":stage[\"decay_pixel\"]})\n",
        "  params.append({\"params\": params_palette, \"lr\":stage[\"lr_palette\"], \"weight_decay\":stage[\"decay_palette\"]})\n",
        "  optimizer = getattr(optim, optimizer_type, None)(params)\n",
        "  return optimizer\n",
        "\n",
        "def main():\n",
        "  params_pyramid = []\n",
        "  if initial_image is not None:\n",
        "    for dim in dims:\n",
        "      pix = torch.zeros((1, num_colors, dim[0], dim[1])) + 1e-6\n",
        "      param_pix = torch.nn.parameter.Parameter( pix.cuda(), requires_grad=True)\n",
        "      params_pyramid.append(param_pix)\n",
        "    print(\"Loading image\")\n",
        "    image = loadImage(initial_image)\n",
        "    image = torch.nn.functional.interpolate(image, size=dims[-1], mode='bicubic', align_corners=False)\n",
        "    pix, palette_colors = imageToParams(image)\n",
        "    pix *= initial_image_strength + 1e-6\n",
        "    param_pix = torch.nn.parameter.Parameter( pix.cuda(), requires_grad=True)\n",
        "    params_pyramid[-1] = param_pix\n",
        "    print(\"loading complete\")\n",
        "  else:\n",
        "    for i, dim in enumerate(dims):\n",
        "        pix_c = (torch.randn(size = (1, num_colors, dim[0], dim[1])) * pix_noise_scale * pix_noise_persistence**i / len(dims)).clamp(-pix_noise_clamp / len(dims), pix_noise_clamp / len(dims)) \n",
        "        param_pix = torch.nn.parameter.Parameter( pix_c.cuda(), requires_grad=True)\n",
        "        params_pyramid.append(param_pix)\n",
        "    if palette is None:\n",
        "      if init_rgb:\n",
        "        palette_colors = torch.rand(size=(num_colors, 4 if use_transparent else 3))\n",
        "        palette_colors = torch.logit(palette_colors, eps=1e-6) / palette_contrast  + palette_brightness\n",
        "      else:\n",
        "        palette_colors = (torch.randn(size = (num_colors, 4 if use_transparent else 3)) * palette_noise_scale).clamp(-palette_noise_clamp, palette_noise_clamp) + paette_brightness\n",
        "    else:\n",
        "      palette_colors = torch.zeros((num_colors, 4 if use_transparent else 3))\n",
        "      for c, color in enumerate(palette):\n",
        "        palette_colors[c, :] = torch.tensor([int(color[i:i+2], 16) for i in (0, 2, 4)])\n",
        "      palette_colors = torch.logit(palette_colors.float() / 255.0, eps = 1e-6) / palette_contrast\n",
        "      print(\"Palette initialized:\")\n",
        "      displayImage(paletteToImage(palette_colors))\n",
        "  if palette_image is not None:\n",
        "    image = loadImage(palette_image)\n",
        "    _, palette_colors = imageToParams(image, True)\n",
        "  params_palette = torch.nn.parameter.Parameter( palette_colors.cuda(), requires_grad=True)\n",
        "  # if color_space == \"YCoCg\":\n",
        "  #   params_pyramid[0][:, 0, :, :] += luma_noise_mean\n",
        "  # elif color_space == \"RGB\":\n",
        "  #   params_pyramid[0] += luma_noise_mean\n",
        "\n",
        "  target_hist = None  \n",
        "  if hist_image is not None:\n",
        "    target_hist = torch.histc(loadImage(hist_image), bins=hist_bins, min=0, max = 1)\n",
        "    target_hist /= target_hist.sum\n",
        "\n",
        "  optimizer = init_optim(params_pyramid, params_palette, stages[0])\n",
        "\n",
        "  for n, stage in enumerate(stages):\n",
        "    stage[\"n\"] = n\n",
        "    stage[\"target_hist\"] = target_hist\n",
        "    if n > 0:\n",
        "      # if stage['dim'][0] != param_luma.shape[2]:\n",
        "      #   if upscaling_mode == \"lanczos\":\n",
        "      #     luma = resample(param_luma, ( stage['dim'][0], stage['dim'][1] ))\n",
        "      #     chroma = resample(param_chroma, ( stage['dim'][0], stage['dim'][1] )) \n",
        "      #     param_luma = torch.nn.parameter.Parameter( luma.cuda(), requires_grad=True )\n",
        "      #     param_chroma = torch.nn.parameter.Parameter( chroma.cuda(), requires_grad=True )\n",
        "      #   else:\n",
        "      #     param_luma = torch.nn.parameter.Parameter(torch.nn.functional.interpolate(param_luma.data, size=( stage['dim'][0], stage['dim'][1] ), mode=upscaling_mode, align_corners=False), requires_grad=True ).cuda()\n",
        "      #     param_chroma = torch.nn.parameter.Parameter(torch.nn.functional.interpolate(param_chroma.data, size=( stage['dim'][0]//chroma_fraction, stage['dim'][1]//chroma_fraction ), mode=upscaling_mode, align_corners=False), requires_grad=True ).cuda()\n",
        "      # if \"init_noise\" in stage:\n",
        "      #   param_luma += torch.randn_like(param_luma) * stage[\"init_noise\"]\n",
        "      #   param_chroma += torch.randn_like(param_chroma) * stage[\"init_noise\"]\n",
        "      # optimizer = init_optim(params_luma, params_chroma, stage)\n",
        "      for i in range(len(dims)):\n",
        "          optimizer.param_groups[i][\"lr\"] = stage[\"lr_pixel\"] * stage[\"lr_scales\"][i]\n",
        "      optimizer.param_groups[-1][\"lr\"] = stage[\"lr_palette\"]\n",
        "      #   optimizer.param_groups[0][\"lr\"] = stage[\"lr_luma\"] * c / warmup_its\n",
        "      #   optimizer.param_groups[1][\"lr\"] = stage[\"lr_chroma\"] * c / warmup_its\n",
        "        # params.append({\"params\":params_luma[i], \"lr\":stage[\"lr_luma\"] * lr_scales[i], \"weight_decay\":stage[\"decay_luma\"]})\n",
        "        # params.append({\"params\":params_chroma[i], \"lr\":stage[\"lr_chroma\"] * lr_scales[i], \"weight_decay\":stage[\"decay_chroma\"]})\n",
        "\n",
        "    for c in range(stage[\"cycles\"]):\n",
        "      cycle( c, stage, optimizer, params_pyramid, params_palette)\n",
        "    # for i in range(len(dims)):\n",
        "    #   if pyramid_scaling_mode == \"lanczos\":\n",
        "    #     nimg = paramsToImage(params_luma[:i+1], params_chroma[:i+1])\n",
        "    #     nimg = torch.nn.functional.interpolate(nimg, size=display_size, mode=\"bicubic\")\n",
        "    #   else:\n",
        "    #     nimg = paramsToImage(params_luma[:i+1], params_chroma[:i+1])\n",
        "    #     nimg = torch.nn.functional.interpolate(nimg, size=display_size, mode=pyramid_scaling_mode)\n",
        "    #   displayImage(nimg)\n",
        "\n",
        "for _ in range(images_n):\n",
        "  main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlyagDsqK2-p"
      },
      "source": [
        "auto_video = True\n",
        "if auto_video:\n",
        "    !rm /content/video.mp4\n",
        "    # Choose frame rate and final save path (in Colab's filesystem)\n",
        "    fps = 24\n",
        "    save_path = '/content/video.mp4'\n",
        "\n",
        "    # Create video\n",
        "    !ffmpeg -r {fps} -pattern_type glob -i '/content/images/*.png' -vcodec libx264 -crf 15 -pix_fmt yuv420p {save_path} -hide_banner -loglevel error"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}