{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP Guided Diffusion HQ 512x512 Uncond_Fast Dango233",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/interesting_notebooks/CLIP_Guided_Diffusion_HQ_512x512_Uncond_Fast_Dango233.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_5lo2iO-ZKs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwMUyt9LHG1"
      },
      "source": [
        "# Generates images from text prompts with CLIP guided diffusion.\n",
        "\n",
        "By Katherine Crowson (RHW) (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses a 512x512 unconditional ImageNet diffusion model fine-tuned from OpenAI's 512x512 class-conditional ImageNet diffusion model (https://github.com/openai/guided-diffusion) together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images.\n",
        "\n",
        "Modified by Dango233 (discord Dango233#7394) to include:\n",
        "\n",
        "\n",
        "*   Gradient clipping - Taken from nshepperd's simple diffusion https://colab.research.google.com/drive/1ivEu0d9TPQgI0tBYLCi1NyhvA2Wr1CjL#scrollTo=XIqUfrmvLIhg\n",
        "*   RGB range loss with configurable bonds - modified based on RHW's original notebook\n",
        "*   Better Cutouts with partial greyscale augmentation to capture structure, and partial rotation augmentation to capture whole frames\n",
        "*   Ability of using different clip models together (Warning: Vram intense, disabled by default on colab, you can turn it on if you run locally with more vram)\n",
        "*   Parameter tricks for getting better result with faster speed (Clipping, range loss, etc)\n",
        "*   Added some configurable parameters\n",
        "\n",
        "\n",
        "Enjoy!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XIqUfrmvLIhg"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivuJjs6p9ttA"
      },
      "source": [
        "Note: This notebook requires 16 GB of GPU memory to work, if you are unable to get a 16 GB GPU consistently, try the [256x256 version](https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ3rNuAWAewx"
      },
      "source": [
        "# Check the GPU\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_UVMZCIAq_r"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/crowsonkb/guided-diffusion\n",
        "!pip install -e ./CLIP\n",
        "!pip install -e ./guided-diffusion\n",
        "!pip install lpips"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zAqFEykBHDL"
      },
      "source": [
        "# Download the diffusion model\n",
        "\n",
        "!curl -OL --http1.1 'https://the-eye.eu/public/AI/models/512x512_diffusion_unconditional_ImageNet/512x512_diffusion_uncond_finetune_008100.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbrcrhpBPC6"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys,random\n",
        "\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./guided-diffusion')\n",
        "\n",
        "import clip\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOj78Yvx8jP"
      },
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1., cutn_whole_portion = 0.0, cutn_bw_portion = 0.2):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.cutn_whole_portion = cutn_whole_portion\n",
        "        self.cutn_bw_portion = cutn_bw_portion\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        if self.cutn==1:\n",
        "            cutouts.append(F.adaptive_avg_pool2d(input, self.cut_size))\n",
        "            return torch.cat(cutouts)\n",
        "        cut_1 = round(self.cutn*(1-self.cutn_whole_portion))\n",
        "        cut_2 = self.cutn-cut_1\n",
        "        gray = transforms.Grayscale(3)\n",
        "        if cut_1 >0:\n",
        "            for i in range(cut_1):\n",
        "                size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "                if i < int(self.cutn_bw_portion * cut_1):\n",
        "                    cutout = gray(cutout)\n",
        "                cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        if cut_2 >0:\n",
        "            for i in range(cut_2):\n",
        "                cutout = TF.rotate(input, angle=random.uniform(-10.0, 10.0), expand=True, fill=rotation_fill)\n",
        "                if i < int(self.cutn_bw_portion * cut_2):\n",
        "                    cutout =gray(cutout)\n",
        "                cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpbody2NCR7w"
      },
      "source": [
        "# Model settings\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32,16,8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing':\"24,48,64\",  # Modify this value to add the number of steps to each stages, will be slower but better quality                                 # timesteps.\n",
        "    'image_size': 512,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_fp16': True,\n",
        "    'use_scale_shift_norm': True\n",
        "    })\n",
        "\n",
        "# Use this with caution - only if you run this locally with rich vram\n",
        "model_list = [\n",
        "   #'RN50x16',\n",
        "   \"ViT-B/16\",\n",
        "  # \"ViT-B/32\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnQjGugaDZPJ"
      },
      "source": [
        "# Load models\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model.load_state_dict(torch.load('512x512_diffusion_uncond_finetune_008100.pt', map_location='cpu'))\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "        param.requires_grad_()\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()\n",
        "\n",
        "clip_model = {}\n",
        "clip_size = {}\n",
        "for i in model_list:\n",
        "    clip_model[i] = clip.load(i, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "    clip_size[i] = clip_model[i].visual.input_resolution\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zY-8I90LkC6"
      },
      "source": [
        "## Settings for this run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PwzFZbLfcy"
      },
      "source": [
        "prompts = ['princess in sanctuary, trending on artstation, photorealistic portrait of a young princess']\n",
        "image_prompts = []\n",
        "batch_size = 1\n",
        "clip_guidance_scale = 5000  # Controls how much the image should look like the prompt. Use high value when clamping activated\n",
        "tv_scale = 8000              # Controls the smoothness of the final output.\n",
        "range_scale = 150            # Controls how far out of range RGB values are allowed to be.\n",
        "clamp_max=0.5              # Controls how far gradient can go - try play with it, dramatic effect when clip guidance scale is high enough\n",
        "\n",
        "RGB_min, RGB_max = [-0.9,0.9]     # Play with it to get different styles\n",
        "cutn = 32\n",
        "cutn_batches = 4           # Turn this up for better result but slower speed\n",
        "cutn_whole_portion = 0.2       #The rotation augmentation, captures whole structure\n",
        "rotation_fill=[1,1,1]\n",
        "cutn_bw_portion = 0.2         #Greyscale augmentation, focus on structure rather than color info to give better structure\n",
        "cut_pow = 0.5\n",
        "n_batches = 1\n",
        "init_image = None   # This can be an URL or Colab local path and must be in quotes.\n",
        "skip_timesteps = 12  # Skip unstable steps                  # Higher values make the output look more like the init.\n",
        "init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
        "seed = 0\n",
        "clip_denoised = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9hTc8YLoLx"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5gODNAMEUCR"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "def do_run():\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    make_cutouts = {}\n",
        "    for i in model_list:\n",
        "        make_cutouts[i] = MakeCutouts(clip_size[i], cutn//len(model_list), cut_pow, cutn_whole_portion, cutn_bw_portion)\n",
        "\n",
        "    side_x = side_y = model_config['image_size']\n",
        "\n",
        "    target_embeds, weights = {}, []\n",
        "    for i in model_list:\n",
        "        target_embeds[i] = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        for i in model_list:\n",
        "            target_embeds[i].append(clip_model[i].encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "        for i in model_list:\n",
        "            batch = make_cutouts[i](TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "            embed = clip_model[i].encode_image(normalize(batch)).float()\n",
        "            target_embeds[i].append(embed)\n",
        "        weights.extend([weight / cutn*len(model_list)] * (cutn//len(model_list)))\n",
        "    for i in model_list:\n",
        "        target_embeds[i] = torch.cat(target_embeds[i])\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    def cond_fn(x, t, out, y=None):\n",
        "        clip_guidance_scale_2 = clip_guidance_scale\n",
        "        \n",
        "        n = x.shape[0]\n",
        "        cur_output = out['pred_xstart'].detach()\n",
        "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "        \n",
        "        my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
        "        loss = 0\n",
        "        x_in_grad = torch.zeros_like(x_in)\n",
        "        for k in range(cutn_batches):\n",
        "                losses=0\n",
        "                for i in model_list:\n",
        "                    if i == \"\":\n",
        "                        clip_in = normalize(make_cutouts[i](x_in.mean(dim=1).expand(3, -1, -1).unsqueeze(0).add(1).div(2)))\n",
        "                    else:\n",
        "                        clip_in = normalize(make_cutouts[i](x_in.add(1).div(2)))\n",
        "                    image_embeds = clip_model[i].encode_image(clip_in).float()\n",
        "                    image_embeds = image_embeds.unsqueeze(1)\n",
        "                    dists = spherical_dist_loss(image_embeds, target_embeds[i].unsqueeze(0))\n",
        "                    del image_embeds, clip_in\n",
        "                    dists = dists.view([cutn//len(model_list), n, -1])\n",
        "                    losses = dists.mul(weights).sum(2).mean(0)\n",
        "                    x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale_2, x_in)[0] / cutn_batches / len(model_list)          \n",
        "                    del dists,losses\n",
        "                gc.collect()\n",
        "        tv_losses = tv_loss(x_in)\n",
        "        range_losses = range_loss(out['pred_xstart'])\n",
        "        loss =  tv_losses.sum() * tv_scale + range_losses.sum() * range_scale \n",
        "        if init is not None and init_scale:\n",
        "            init_losses = lpips_model(x_in, init)\n",
        "            loss = loss + init_losses.sum() * init_scale\n",
        "        x_in_grad += torch.autograd.grad(loss, x_in, )[0]\n",
        "        grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
        "        magnitude = grad.square().mean().sqrt()\n",
        "        return grad * magnitude.clamp(max=clamp_max) / magnitude \n",
        "\n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        \n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            model,\n",
        "            (batch_size, 3, model_config['image_size'], model_config['image_size']),\n",
        "            clip_denoised=clip_denoised,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=True,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init,\n",
        "            cond_fn_with_grad=True,\n",
        "           # randomize_class=True,\n",
        "        )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            if j % 10 == 0 or cur_t == 0:\n",
        "                print()\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
        "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                    display.display(display.Image(filename))\n",
        "            cur_t -= 1\n",
        "\n",
        "gc.collect()\n",
        "do_run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}